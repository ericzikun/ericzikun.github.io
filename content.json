{"meta":{"title":"Eric’s blog","subtitle":"","description":"","author":"Eric kun","url":"https://ericzikun.github.io","root":"/"},"pages":[{"title":"欢迎骚扰！","date":"2020-04-08T01:39:55.000Z","updated":"2020-04-08T09:05:28.946Z","comments":true,"path":"about/index.html","permalink":"https://ericzikun.github.io/about/index.html","excerpt":"","text":"邮箱地址：847473488@qq.com知乎:ERICKCSDN: Popofzk 欢迎交流： 本人目前还是小白一个，希望能在未来三年有限的研究生时间内，对技术、知识做到多总结多复盘，总体上分为几条线展开：技术栈（开发篇）、算法理论兼实战（机器学习+深度学习）；还有就是一定得多健身，身体才是最重要的！ 喜欢交朋友，如果有愿意一起交流技术、模型、创业等的小伙伴可以联系我哦，我相信：一个人可以走的很快，但是一群人终将走的更远！ 这里附上Jobs及他的名言： Stay Hungry， Stay Foolish， Think Different!"}],"posts":[{"title":"Java核心技术读书笔记4&5章","slug":"Java核心技术读书笔记4-5章","date":"2020-08-15T14:37:38.000Z","updated":"2020-08-27T15:25:03.416Z","comments":true,"path":"2020/08/15/Java核心技术读书笔记4-5章/","link":"","permalink":"https://ericzikun.github.io/2020/08/15/Java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B04-5%E7%AB%A0/","excerpt":"","text":"Java核心技术 读书笔记：第四章 对象与类对象的理解：每个对象包含对用户公开的特定功能部分和隐藏的实现部分。从根本上来说，只要对象能够满足要求，就不必关心其功能到底如何实现！ 此外，每个对象都保存着描述当前状况的信息——对象的状态。对象状态的改变必须通过调用方法实现（如果不经过调用就可以改变对象状态，只能说明破坏了封装性！） 对象引用：对象的创建通过对象提前写好的构造函数（无参 有参），new+构造才能够创建出一个新的对象 12Date s = new Date()Date s = k 实际上，k和s引用的是同一对象（new的Date的对象），指向相同！ 所有的Java对象都存储在堆中，当一个对象包含另一个对象变量时，它只是包含着另一个堆对象的指针！ 类：三种关系： 依赖（uses-a）如Order类使用Account类，是因为Order对象需要访问Account对象查看信用状态！——我们应该尽可能减少相互依赖：减少类之间的耦合！ 聚合（has-a）包含关系 继承（is-a） 公共类和非公共类（是否带public） 源文件名必须与public类的名字相匹配，在一个源文件中，只能有一个公共类，但可以有任意数目的非公共类。 一般习惯将类单独命名为xxx.java 类中的public方法：public意味着任何类的任何方法都可以调用这些方法（一共有4个级别，后面介绍） 构造器： 与类同名 可以有一个以上的构造器 构造器可以有任意数目的参数 构造器没有返回值 总是伴随new一起调用！ var声明局部变量：Java10中，如果可以从变量的初始值推导出它们的类型，可用var关键字声明局部变量，无须指定类型。 12Employee harry = new Employee(\"Harry\",5000,1989,10,1);var harry = new Employee(\"Harry\",5000,1989,10,1); 两者等效 隐式参数和显式参数： 如： 12345public viod raiseSalary(double byPercent) &#123; double raise = salary * byPercent / 100; salary += raise;&#125;number007.raiseSalary(5) 其结果是将number007.salary字段新增5%（设置为了一个新值） raiseSalary有两个参数，其一是前面的Employee类的对象，第二则是括号中的参数 关键词this指示隐式参数，可以改写：（强烈推荐） 1234public viod raiseSalary(double byPercent) &#123; double raise = this.salary * byPercent / 100; this.salary += raise;&#125; 可以将实例字段和局部变量明显区分出来 警告！（初探对象封装性）不要编写返回可变对象引用的访问器方法。 例如： 123456789class Employee&#123; private Date hireDay; ... public Date getHireDay() &#123; return hireDay //Bad &#125;&#125; 其中的Date类有更改器方法setTime，也就是说Date对象是可变的，这就破坏了封装性！ 1234Employee harry = ...;Date d = harry.getHireDay();double tenYearsInNilliseconds = 10 * 365.25 * 24 * 60 * 60 * 1000;d.setTime(d.getTime() - (long)tenYearMilliseconds); d和harry.hareDay引用的是同一个对象，对d调用更改器方法就可以自动地改变这个Employee对象的私有状态！ 如果要返回一个可变对象的引用，首先应该对它进行克隆！对象克隆指放在另一个新位置上的对象副本。 123456789class Employee&#123; private Date hireDay; ... public Date getHireDay() &#123; return (Date)hireDay.clone(0) //Bad &#125;&#125; 谈谈私有方法和公共方法由于公共数据非常危险，应该将数据字段设置为私有的字段（很好理解），对于方法来说，尽管大部分都是公共的，但有些情况下用私有会更好：如，数据的表示发生了变化，这个方法可能会变得难以实现，或者不再需要，这并不重要，重要的是，只要它是私有方法，类的设计者就可以确信它不会在别处使用没所以可以将其删去，如果一个方法是公共的，就不能简单的删除，因为有可能在别处依赖！ final实例字段 一旦设置，以后就不再修改这个字段，如Employee类中的name字段设置为final，因为在对象构造后，值不会改变，即没有setName方法。 对于基本类型或者不可变类的字段尤其有用： 对于可变的类，可能混乱： 1private final StringBuilder evaluations; 它在Employee中初始化为： 1evaluations = new StringBuilder(); final关键字只是表示存储在evaluations变量中的对象引用不会再指向另一个不同的StringBuilder对象。不过这个对象依旧是可以更改的！！也就是地址不变而已 静态字段与静态方法静态字段：属于类，不属于对象！例如，要给每个员工一个唯一的标识码，这里给Employee类添加一个实例字段id和一个静态字段nextId； 1234567891011class Employee&#123; private static int nextId = 1; private int id; ...&#125;public void setId()&#123; id = nextId; nextId++;&#125; 当新增员工时，其id是在整个员工nextId基础之上的！也就是说，这个nextId是公共调用的！ 静态常量用的更多，比如Math类下的PI值 以下两种情况下可以使用静态方法： 方法不需要访问对象的状态，因为它需要的所有参数都通过显式参数提供，如Math.pow 方法只需要访问类的静态字，如Employee.getNextId 静态工厂方法类似LocalDate和NumberFormat的类使用静态工厂方法来构造对象。 12345NumberFormat currencyFormatter = NumberFormat.getCurrencyInstance();NumberFormat percentFormatter = NumberFormat.gatPercentInstance();double x = 0.1;System.out.println(currencyFormatter.format(x)); //0.1System.out.println(percentFormatter.format(x)); //10% 这里的NumberFormat类不使用构造器来完成，有两个原因： 无法命名构造器。构造器名字必须与类相同，这里希望有两个不同名字，分别得到货币实例和百分比实例。 使用构造器时，无法改变所构造对象的类型，而工厂方法实际上将返回DecimalFormat类的对象，是NumberFormat的子类 Main方法main方法也是一种静态方法。main方法不对任何对象进行操作，事实上，启动程序时还没有任何对象。静态的main方法将执行并构造程序所需要的对象。 方法参数按值调用——表示方法接受的是调用者提供的值； 按引用调用——表示方法接收的是调用者提供的变量地址。 Java总是按值调用的。方法得到的是所有参数值的一个副本。也就是说，方法不能修改传递给它的任何参数变量的内容。 12double percent = 10;harry.raiseSalary(percent); 无论方法如何实现，在这个方法调用后，percent值还是10。 但是对于对象引用则不同！ 1234public static void tripleSalary(Employee x)&#123; x.raiseSalary(200);&#125; 当调用 123harry = new Employee(...);tripleSalary(harry); 具体为： x初始化为harry值的一个副本，这里就是一个对象引用。 raiseSalary方法应用于这个对象引用。x和salary同时引用的那个Employee对象的工资提高了200%。 方法结束后x不再使用，对象变量harry继续引用那个工资增至3倍的员工对象 总结Java方法参数 方法不能修改基本数据类型的参数 方法可以改变对象参数的状态 方法参数不能让一个对象参数引用一个新的对象 对象构造重载——同方法、不同参数 默认字段初始化：如果构造器中没有显示地为字段设置初值，则会被自动的赋为默认值！数值为0、布尔值为false、对象引用为null 无参构造器：如果编写一个类没有无参构造，就会为你提供一个无参数的构造器，如果已经只定义了有参，再调无参则不合法。 参数名的定义：习惯将参数名和实例字段保持一致，通过this来区分： 12345public Employee(String name,double salary)&#123; this.name = name; this.salary = salary&#125; this的另一用法：this除了可以指示一个方法的隐式参数外，还可以调用同一个类的另一个构造器 12345public Employee(double s)&#123; this(\"Employee #\" + nextId,s); nextId ++;&#125; 当调用new Employee(6000)，Employee(double)构造器会调用Employee(String,double)构造器。 初始化块：12345678910111213141516171819202122232425class Employee &#123; private static int nextId; private int id; private String name; private double salary; //初始化块 &#123; id = nextId; nextId ++; &#125; public Employee(String n,double s) &#123; name = n; salary = s; &#125; public Employee(String n,double s) &#123; name = \"\"; salary = 0; &#125; ...&#125; 之前有两种初始化数据字段的方法： 构造器中赋值 声明中赋值 另一个则是设置一个初始化块，只要构造这个类的对象，初始化块就会被执行——首先运行初始化快，然后才运行构造器的主体部分。 但是这不是必需的，通常将初始化代码放在构造器中 区分于静态字段对应的静态代码块：如果类的静态字段需要很复杂的初始化代码，那么可以使用静态的初始化块 区分初始化块和静态初始化块： 静态初始化块:使用static定义,当类装载到系统时执行一次.若在静态初始化块中想初始化变量,那仅能初始化类变量,即static修饰的数据成员. 非静态初始化块:在每个对象生成时都会被执行一次,可以初始化类的实例变量. 类设计技巧 保证数据私有 一定要对数据进行初始化 不要在类中使用过多的基本类型 不是所有字段都需要单独的字段访问器和字段更改器 分解有过多职责的类 类名和方法名要足够体现它们的职责 优先使用不可变的类 第五章 继承继承的基本思想：基于已有的类创建新的类。就是复用已有类的方法，并且可以增加一些新的方法和字段 类、超类和子类已存在的类——超类、基类、父类；新类——子类、派生类、孩子类 如Employee中的经理和和员工在薪资待遇上面存在一些差异，但也存在很多相同的地方。他们之间存在一个明显的“is-a”关系，每一个经理都是一个员工：“is-a”关系是继承的明显特征 12345678910public class Manager extends Employee&#123; //added methods and fields private double bonus; ... public void setBonus(double bonus) &#123; this.bonus = bonus; &#125;&#125; setBonus不是在Employee中定义的，所以Employee不能使用它。经理继承了name、salary、hireDay三个字段，并且新增了bonus字段。 覆盖方法： 如果要返回经理的奖金 1234public double getSalary() &#123; return salary + bonus //不成功&#125; 因为salary是父类的私有字段，子类Manager的getSalary方法不能直接访问到！ 如果我们想调用父类Employee的getSalary方法，而不是当前类的这个方法，可以用super.getSalary() 12345public double getSalary()&#123; double baseSalary = super.getSalary(); return baseSalary + bonus;&#125; 这里的super和this不能等同于一类，因为super不是一个对象的引用，例如，不能将值super赋给另一个对象变量，它只是一个指示编译器调用超类方法的特殊关键字。 注意： 子类可以增加字段、增加方法或覆盖超类的方法，继承绝不会删除任何字段或方法 深入理解父子类继承（子类构造器）有关子类是否继承了父类的私有字段（再理解） 如，Student类继承了Person类 Student对象里，本身就装着一个Person对象。Student对象没有继承Person对象的name字段，所以Student对象没有一个叫name的字段。但Student内部封装的Person对象还是有name字段的。 12345public class Person &#123; private String name; public Person(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125;&#125; 1234567public class Student extends Person &#123; private int id; public Student(String name, int id) &#123; super(name); this.id = id; &#125;&#125; Student没有name字段，但它内部的Person对象有，而且还可以打出来看。 12345public static void main(String[] args) &#123; Student s = new Student(\"bitch\",99); System.out.println(s.getName()); // BITCH System.out.println(s.name); // ERROR: name has private access in Person &#125; 而且注意，我要直接打印Student的name字段 “s.name” ，报错说的是：Person类的name字段为私有，你不可以访问。而不是没有name字段。 大胆一点的话，我们还可以给Student类再加一个name字段。这时候的Student对象本身有一个name字段，内部的基类Person对象还有一个name对象。 123456789public class Student extends Person &#123; private int id; private String name; public Student(String personName, String studentName, int id) &#123; super(personName); this.name = studentName; this.id = id; &#125;&#125; 输出： 12345public static void main(String[] args) &#123; Student s = new Student(\"bitch\",\"whore\",99); System.out.println(s.getName()); // BITCH System.out.println(s.name); // WHORE &#125; 注意： 使用super调用构造器，必须是子类构造器的第一条语句 子类构造器如果没有显式地调用超类的构造器，将自动地调用超类的无参数构造器，所以必须要求父类有无参构造，否则报错 多态1234567891011Manager boss = new Manager(\"Carl Cracker\",8000,1987,12,15);boss.setBonus(5000);var staff = new Employee[3];staff[0] = boss;staff[1] = new Employee(\"Harry\",5000,1989,10,1);staff[2] = new Employee(\"Tony\",5000,1989,10,1);for(Employee e:staff) System.out.println(e.getName() + \" \" + e.getSalary()); 对于e来说，既可以是Manager也可以是Employee，像这种的，一个对象变量可以指示多种实际类型的现象称为多态，在运行时可以自动地选择适当的方法，称为动态绑定 例子： 123Manager boss = new Manager(...);Employee[] staff = new Employee[3];staff[0] = boss; 这里面采用了多态，虽然staff[0]和boss引用同一个对象，但是编译器只将staff[0]看成是一个Employee对象，这意味着，可以这么调用： 1boss.setBonus(5000); //OK 但不能这么调用： 1staff[0].setBonus(5000); //Error 这是因为staff[0]的声明类型是Employee，而setBonus不是Employee的方法。 多态——当声明变量为某一种形态的变量时，编译器就将它看成某种形态。 注意 不能将超类的引用赋值给子类变量，如下非法： 1Manager m = staff[i]; //Error 原因很清楚：不是所有的员工都是经理，如果赋值成功，m有可能引用了一个不是经理的Employee对象，而在后面有可能会调用m.setBonus，这就会发生错误。 警告：12Manager[] managers = new Manager[10];Employee[] staff = managers; //OK 这样是没有问题的，因为manger[i]是一个Manager就一定是一个Employee！一定要切记：这里的staff和mangers引用的是同一个数组，就是一开始new的长度为10的数组！ 1staff[0] = new Employee(\"Harry\"); 如果这么去赋值，编译器是可以接受的！但是！！staff[0]和managers[0]是相同的引用，我们把一个普通的员工Harry擅自归入到经理行列（数组）里面去了！！后面如果调用manager[0].setBonus(1000)的时候，将会试图调用一个根本不存在的实例字段，进而搅乱相邻存储空间的内容 牢记：所有数组要牢记创建时候的元素类型，并负责监督仅将类型兼容的引用存储到数组中！例如，使用new managers[10]创建数组是一个经理数组如果试图存储一个Employee类型的引用就会引发ArrayStoreException异常 方法调用 编译器查看对象的声明类型和方法名。 确定方法调用中提供的参数类型。 如果是private、static、final或者构造器，那么编译器将可以准确地知道应该调用哪个方法。——静态绑定； 动态绑定——如果调用的方法依赖于隐式参数的实际类型，则必须在运行的时候使用动态绑定。 强制类型转换对于对象： 由于在员工列表中，一部分是纯员工，有一部分是经理（子类），在创建数组的时候申明的是Employee对象，而Employee对象无法读取到其Manager字段或方法等属性（多态），那么在实际用Manager这个对象的时候，要先强制转换成Manager类型： 1Manager boss = (Manager)staff[0]; 将其复原为Manager对象，以便于访问其额外的字段，如bonus奖金。当然，前提是0号确实是Manager，如果“谎报”，则会报错ClassCastException，为了确保不会谎报，可以先判断一下： 12345if (staff[0] instanceof Manager)&#123; boss = (Manager)staff[1]; ...&#125; 受保护字段protected一般来说，声明为private私有，对其他类都是不可见的，即，子类不能访问超类的私有字段。不过有时候希望限制超类中的某个方法只允许子类访问，或者希望子类的方法访问超类的某个字段。 例如，将Employee中的hireDay字段设为protected，而不是private，则Manager方法就可以访问到这个字段。 注意： 要谨慎使用，如果你的代码被别的程序员访问了受保护字段，那么后期维护时候，修改自身类则会影响到别人！ 受保护的方法更具有实际意义，表明子类得到了信任，可以正确的使用这个方法，而其他类则不行 泛型类数组列表ArrayList是一个有类型参数的泛型类。尖括号里面填写保存的元素对象类型，如ArrayList&lt;Employee&gt; 声明一个保存Employee对象的数组列表： 123ArrayList&lt;Employee&gt; staff = new ArrayList&lt;Employee&gt;();//或者var staff = new ArrayList&lt;Employee&gt;(); 也可以省略右边括号里面的类型参数 1ArrayList&lt;Employee&gt; staff = new ArrayList&lt;&gt;(); 对象包装器与自动装箱每个基本类型都有与之对应的类Integer、Long、Float、Double、Short、Byte、Character、Boolean； &lt;&gt;尖括号中的类型参数不允许是基本类型 由于每个值分别包装在对象中，所以ArrayList&lt;Integer&gt;效率远远低于int[] 自动装箱： 12var list = new ArrayList&lt;Integer&gt;()list.add(3) 此时，进行了自动装箱过程： 1list.add(Integer.valueOf(3)) 自动拆箱：此时拿到的n应该是&lt;Integer&gt;类型 1int n = list.get(i) 转换成： 1int n = list.get(i).intValue();","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"Java","slug":"技术栈/Java","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://ericzikun.github.io/tags/Java/"},{"name":"对象","slug":"对象","permalink":"https://ericzikun.github.io/tags/%E5%AF%B9%E8%B1%A1/"},{"name":"类","slug":"类","permalink":"https://ericzikun.github.io/tags/%E7%B1%BB/"}]},{"title":"数据结构Java描述整理","slug":"数据结构Java描述整理","date":"2020-07-20T14:36:48.000Z","updated":"2020-08-27T15:17:00.362Z","comments":true,"path":"2020/07/20/数据结构Java描述整理/","link":"","permalink":"https://ericzikun.github.io/2020/07/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84Java%E6%8F%8F%E8%BF%B0%E6%95%B4%E7%90%86/","excerpt":"","text":"1. 绪论2. 线性表​ 线性表是一种最常用、最简单，也是一种最基本的数据结构，它是学习其他数据结构的基础。 线性表在计算机中可以用$\\begin{cases}顺序存储 \\\\ 链式存储\\end{cases}$两种存储结构来表示，其中，顺序存储的线性表成为顺序表，链式存储的线性表成为链表，链表又分为：$\\begin{cases} 单链表 \\\\ 双向链表 \\\\ 循环链表\\end{cases}$。 特点： 对于同一个线性表，其每一个数据元素的值虽然不同，但必须具有相同的数据类型 数据元素之间具有一种线性的或“一对一”的逻辑关系：开始结点没有前驱，末尾结点没有后继，除开始和末尾结点外，其余数据元素有且仅有一个前驱和一个后继 几个基本操作： 12345678910111213141516```clear()```:将已存在的线性表置为空表```isEmpty()```:判空```length()```:求线性表长度，即，元素个数```get(i)```:读取线性表中第i个数据元素的值。$0 \\leqslant i \\leqslant length-1$```insert(i,x)```:在线性表的第i个数据元素之前插入一个值为x的数据元素。```remove(i)```:删除并返回线性表中第i个数据元素 ```indexOf(x)```:返回线性表中首次出现指定数据元素的位序号，若不包含次数据元素，则返回-1 ```desplay(0)```:输出线性表中的各个数据元素的值 2.1 顺序表定义： 顺序表是用一组地址连续的存储单元依次存放线性表中各个数据元素的存储结构。 特点： 在线性表中逻辑相邻的数据元素，在物理存储元素上也是相邻的 存储密度高，但需要预先分配“足够应用的存储空间，这可能将会造成存储空间的浪费，其中，$存储密度=\\frac{数据元素本身值所需的存储空间}{该数据元素实际所占用的空间}$ 便于随机存取 不便于插入和删除操作没这事因为在顺序表上进行插入和删除操作会引起大量数据元素的移动 顺序表的局限性： 若要为顺序表扩充存储空间，则需要重新创建一个地址连续的更大存储空间，并把原有的数据元素都复制到新的存储空间中 因为顺序表存储要求逻辑上相邻的数据元素，在物理存储位置上也是相邻的，这就使得增删数据元素则会引起平均约一半的数据元素的移动 总结——查询快、增删慢！ 顺序表代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.dataStructure.SqList;public class SqList &#123; private Object[] listElem; private int curlen; //构造函数，构造一个存储空间容量为maxSize的线性表 public SqList(int maxSize)&#123; curlen = 0; //置顺序表的当前长度为0 listElem = new Object[maxSize];//给顺序表分配maxSize个存储单元 &#125; //置空表 public void clear()&#123; curlen = 0 ; //置顺序表的当前长度为0 &#125; //判空 public boolean isEmpty()&#123; return curlen == 0; &#125; //求线性表中数据元素个数，返回值 public int length()&#123; return curlen; &#125; //读取到线性表第i个元素并返回其值 public Object get(int i) throws Exception&#123; if(i&lt;0 || i&gt; curlen-1) throw new Exception(\"第\" + i + \"个元素不存在\"); return listElem[i]; &#125; //在第i个元素之前插入一个值为x的数据元素 public void insert(int i,Object x) throws Exception&#123; if(curlen == listElem.length) //判断顺序表是否已经满 throw new Exception(\"顺序表已满\");//抛出异常 if(i&lt;0 || i&gt;curlen) throw new Exception(\"插入位置不合法\"); for (int j = curlen; j &lt; i ; j--) //插入后的元素向后移一个存储单位 listElem[j] = listElem[j-1]; listElem[i] = x; curlen++; &#125; //删除并返回线性表中第i个数据元素 public void remove(int i) throws Exception&#123; if(i&lt;0 || i&gt;curlen -1) throw new Exception(\"删除位置不合法\"); for (int j = i; j &lt; curlen -1 ; j++) listElem[j] = listElem[j+1];//删除元素后的元素向前移一个单位 curlen--; &#125; //返回线性表中首次出现指定的数据元素的位序号，若线性表中不包含此数据元素，则返回-1 public int indexOf(Object x)&#123; int j =0; while(j&lt;curlen &amp;&amp; !listElem[j].equals(x)) j++; if(j&lt;curlen) return j; else return -1; &#125; //输出线性表中的数据元素 public void display()&#123; for (int j = 0; j &lt; curlen; j++) &#123; System.out.println(listElem[j] + \"\"); &#125; &#125;&#125; 1234567891011121314151617181920package cn.dataStructure.SqList;public class SqListTest &#123; public static void main(String[] args) throws Exception&#123; SqList L = new SqList(10); L.insert(0,\"a\"); L.insert(1,\"z\"); L.insert(2,\"d\"); L.insert(3,\"z\"); System.out.println(\"此顺序表为：\"); L.display(); int order = L.indexOf(\"z\"); if(order !=-1) System.out.println(\"顺序表中第一次出现的值为'z'的数据元素位置为：\"+order); else System.out.println(\"此顺序表中不包含值为z的属于元素\"); &#125;&#125; 2.2 链表定义： 顺序表适合表示静态线性表，一旦形成以后，就很少进行插入和删除操作，对于需要频繁插入和删除的动态线性表，通常采用链式存储结构。 特点： 链式结构不要求逻辑上相邻的数据元素在物理上也相邻，它是用一组地址任意的存储单元来存放数据元素的值，故它没有顺序结构某些操作上的局限性，但却失去了随机存取的特点，在链式结构上只能进行顺序存取 单链表代码实现：Node类 1234567891011121314151617181920212223package cn.dataStructure.LinkList;public class Node&#123; //存储元素 public Object data; //存放结点值 public Node next; //后继结点的引用 //无参数时的构造函数 public Node() &#123; this(null,null); &#125; //有一个参数时的构造函数 public Node(Object data) &#123; this.data = data; &#125; //有两个参数时的构造函数 public Node(Object data, Node next) &#123; this.data = data; this.next = next; &#125;&#125; LinkList类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package cn.dataStructure.LinkList;import java.util.Scanner;public class LinkList &#123; public Node head; //单链表的头指针 //无参构造 public LinkList() &#123; head = new Node();//初始化头结点 &#125; //构造一个长度为n的单链表 public LinkList(int n, boolean Order)throws Exception&#123; this(); //初始化头结点，相当于无参构造 if(Order) create1(n); //用尾插法顺序建立单链表 else create2(n); //用头插法逆位序建立单链表 &#125; //用尾插法顺序建立单链表，其中n为单链表的结点个数 先要编写insert public void create1(int n) throws Exception&#123; Scanner sc = new Scanner(System.in);//构造输入对象 for(int j = 0; j&lt;n; j++) insert(length(),sc.next()); &#125; //用头插法逆位序建立单链表，其中n为单链表的结点个数 public void create2(int n) throws Exception&#123; Scanner sc = new Scanner(System.in);//构造输入对象 for(int j = 0; j&lt;n; j++) insert(0,sc.next()); &#125; //将一个已经存在的带头结点单链表置成空表 public void clear()&#123; head.data = null; head.next = null; &#125; //判断带头结点的单链表是否为空 public boolean isEmpty()&#123; return head.next == null; &#125; //求带头结点的单链表的长度 public int length()&#123; //新建一个p指针指向head Node p = head; int length = 0; while(p != null)&#123; p = p.next; ++ length; &#125; return length-1; &#125; //求取带头结点的单链表中的第i个结点 public Object get(int i) throws Exception&#123; Node p = head; //定义一个p指针指向head，利用其遍历 int j = -1; while(p!=null &amp;&amp; j&lt;i)&#123; p = p.next; ++j; &#125; //判断是否合法 if(i&lt;0 || p == null)&#123; throw new Exception(\"get的第\" + i + \"个元素不存在\"); &#125; return p.data; &#125; //在带头结点的单链表中的第i个结点之前插入一个值为x的新结点 public void insert(int i, Object x) throws Exception&#123; Node p = head; //定义一个p指针先指向head，利用它遍历到i-1处 int j = -1; while(p != null &amp;&amp; j&lt; i-1)&#123; p = p.next; ++j; &#125;// 判断i是否合法 if(j&gt;i-1 || p == null) throw new Exception(\"插入位置不合法hh\"); Node s = new Node(x); s.next = p.next; p.next = s; &#125; //删除带头结点的单链表中的第i个结点 public void remove(int i) throws Exception&#123; Node p = head; int j = -1; while(p != null &amp;&amp; j &lt;i)&#123; p = p.next; ++j; &#125; //判断i是否合法 if(i&lt;0 || p ==null)&#123; throw new Exception(\"删除的第\" + i + \"个元素不存在\"); &#125; &#125; //在带头结点的单链表中查找值为x的结点，返回位置 public int indexOf(Object x) throws Exception&#123; Node p = head; int j = -1; while(p!=null &amp;&amp; !p.data.equals(x))&#123; p = p.next; ++j; &#125; //判断是否含有x这个值 if(p==null)&#123; throw new Exception(\"不存在值为\" + x + \"的结点\"); &#125; else return j; &#125; //输出单链表中的所有结点 public void display()&#123; Node node = head.next; //取出带头结点的单链表中的首结点 node作为指针来遍历 while(node!=null)&#123; System.out.println(node.data + \"\"); node = node.next; &#125; System.out.println(); //换行 &#125;&#125; 123456789101112131415161718package cn.dataStructure.LinkList;public class LinkListTest &#123; public static void main(String[] args) throws Exception&#123; LinkList L = new LinkList(); L.display(); L.insert(0,0); L.insert(1,1); L.display(); System.out.println(\"链表长度为\"+ L.length()); LinkList L2 = new LinkList(3,false);//头插法 L2.display(); System.out.println(\"链表长度为\"+ L2.length()); LinkList L3 = new LinkList(3,true);//尾插法 L3.display(); System.out.println(\"链表长度为\"+ L3.length()); &#125;&#125; 输出结果： 2.3 链表VS顺序表 与顺序表相比较,链表比灵活，它既不要求在预先分配的一块连续的存储空间中存储线性表的所有数据元素,也不要求按其逻辑顺序来分配存储单元,可根据需要进行存储空间的动态分配!因此,当线性表的长度变化较大或长度难以估计时,用链表。但在线性表的长度基本可预计且变化较小的情况下,宜用顺序表,因为链表的存储密度较顺序表的低,且顺序表具有随机存取的优势! 在顺序表中按序号访问第i个数据元素时的时间复杂度为O(1),而在链表中做同样操作的时间复杂度为O(n)所以若要经常对线性表按序号访问数据元素时,顺序表要优先链表;但在顺序表上做插入和删除操作时,需要平均移动一半的数据元素,而在链表上做插入和删除操作,不需要移动任何数据元素,虽然也要查找插入或删除数据元素的位置,但由于主要是比较操作,所以从这个角度考虑,链表要优先于顺序表 总之,链表比较灵活,**插入和删除操作的效率较高,但链表的空间利用率较低,适合于实现动态的线性表;顺序表实现比较简单,因为任何高级程序语言中都有数组类型,并且空间利用率也较高,可高效地进行随机存取,但顺序表不易扩充,插入和删除操作的效率较低,适合于实现相对“稳定”的静态线性表。两种存储结构各有所长,各种实现方法也不是一成不变的。在实际应用时,必须以这些基本方法和思想为基础,抓住两者各自的特点并结合具体情况,加以创造性地灵活应用和改造,用最合适的方法来解决问题。 3.栈与队列定义： 栈和队列可被看成是两种操作受限的特殊线性表，其特殊性体现在它们的插入和删除操作都是控制在线性表的一端或两端进行。 3.1 栈 栈是一种特殊的线性表，栈中的元素以及数据元素间的逻辑关系和线性表相同，区别在于： 线性表的插入和删除操作可以在表的任意位置进行，而栈只允许在表的尾端进行 特点： 先进后出(First In Last Out) 几个基本操作： 123456789101112```clear()```：置空```isEmpty()```：判空```length()```：返回栈中元素个数```peek()```：读取栈项元素并返回其值，若栈为空，则返回null```push()```：入栈——将数据元素x压入栈顶```pop()```：出栈——删除并返回栈顶元素 3.1.1 顺序栈类的定义： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package cn.dataStructure.SqStack;public class SqStack &#123; private Object[] stackElem; private int top;//在非空栈中，top始终指向栈顶元素的下一个存储位置，栈为空时，top=0 //构造函数，构造一个存储空间容量为maxSize的空站 public SqStack(int maxSize) &#123; top = 0; stackElem = new Object[maxSize];//为栈分配maxSize个存储单元 &#125; //栈置空 public void clear()&#123; top = 0; &#125; //判空 public boolean isEmpty()&#123; return top == 0; &#125; //求栈中数据元素个数 public int length()&#123; return top; &#125; //取栈顶元素 public Object peek()&#123; if(!isEmpty()) return stackElem[top-1]; else return null; &#125; //入栈 public void push(Object x) throws Exception&#123; if(top == stackElem.length)//这里判断一下Object数组的长度和top是否相等，与sqStack的length()区别开 throw new Exception(\"栈已满\");//跑出异常 else stackElem[top++] = x; &#125; //出栈 public Object pop() &#123; if (isEmpty()) return null; else return stackElem[--top]; &#125; //输出栈中所有数据元素，从栈顶元素到栈底 public void display() &#123; for (int i = top-1; i &gt;=0 ; i--) &#123; System.out.print(stackElem[i].toString() + \" \"); &#125; &#125;&#125; 测试类： 123456789101112package cn.dataStructure.SqStack;public class sqStackTest &#123; public static void main(String[] args) throws Exception&#123; SqStack sqStack = new SqStack(10); //循环压入栈 for (int i = 0; i &lt; 10; i++) &#123; sqStack.push(i); &#125; sqStack.display(); &#125;&#125; 3.1.2 链栈定义结点类，与之前一致： 1234567891011121314151617181920212223package cn.dataStructure.LinkStack;public class Node&#123; //存储元素 public Object data; //存放结点值 public Node next; //后继结点的引用 //无参数时的构造函数 public Node() &#123; this(null,null); &#125; //有一个参数时的构造函数 public Node(Object data) &#123; this.data = data;&#125; //有两个参数时的构造函数 public Node(Object data, Node next) &#123; this.data = data; this.next = next; &#125;&#125; 定义链栈类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package cn.dataStructure.LinkStack;public class LinkStack &#123; private Node top; //栈顶元素的引用 //置空 public void clear() &#123; top = null; &#125; //判空 public boolean isEmpty() &#123; return top == null; &#125; //求长度 public int length() &#123; Node p = top; int length = 0; while(p!=null) &#123; p = p.next; length++; &#125; return length; &#125; //取栈顶元素并返回值 public Object peek() &#123; if (!isEmpty()) return top.data; else return null; &#125; //入栈 public void push(Object x) &#123; Node p = new Node(x); p.next = top; top = p; &#125; //出栈 public Object pop() &#123; if (isEmpty()) &#123; return null; &#125; else &#123; Node p = top; //p指向被删结点，引用p是因为要返回被删结点值 top = top.next; return p.data; &#125; &#125; //输出 public void display() &#123; Node p = top; while (p!=null) &#123; System.out.print(p.data.toString() + \" \"); p = p.next; &#125; &#125;&#125; 测试类： 12345678910111213141516package cn.dataStructure.LinkStack;public class LinkStackTest &#123; public static void main(String[] args) &#123; LinkStack LS = new LinkStack(); //批量压入栈 for (int i = 0; i &lt; 10; i++) &#123; LS.push(i); &#125; //输出 LS.display(); System.out.println(\"\\n\" + \"被删除元素为：\" + LS.pop()); LS.display(); &#125;&#125; 3.2 队列 定义： 队列是另一种特殊的线性表，它的特殊性体现在队列只允许在表尾插入数据元素，在表头删除数据元素，所以队列也是一种操作受限的特殊线性表 特点： 先进先出 几个基本操作： 1234567891011121314151617181920&#96;&#96;&#96;isEmpty()&#96;&#96;&#96;：置空&#96;&#96;&#96;length()&#96;&#96;&#96;：求取队列数据元素个数&#96;&#96;&#96;peek()&#96;&#96;&#96;：读取队首元素并返回其值。&#96;&#96;&#96;offer()&#96;&#96;&#96;：入队操作，将数据元素x插入到队列中使其成为新的队尾元素。&#96;&#96;&#96;poll()&#96;&#96;&#96;：出队操作，删除队首元素并返回其值，若队列为空，则返回null。&#96;&#96;&#96;javapublic interface IQueue &#123; public void clear(); public boolean isEmpty(); public int length(); public Object peek(); public void offer(Object x) throws Exception; public Object poll()&#125; 3.2.1 顺序队列​ 与顺序栈类似，在顺序队列的存储结构中，需要分配一块地址连续的存储区域来一次存放队列中从队首到队尾的所有元素。这样也可以用一维数组来表示，假设数组名为queueElem，数组最大容量为maxSize，由于队列的入队操作只能在当前队列的队尾进行，而出队操作只能在当前队列的队首进行，所以需加上变量front和rear来分别指示队首队尾元素在数组中的位置，其初始值都为0，在非空队列中，front指向队首元素，rear指向队尾元素的下一个存储位置 假溢出： ​ 从图3.16(d)可以看出,若此时还需要将数据元素H入队,H应该存放于rear=6的位置处,顺序队列则会因数组下标越界而引起“溢出”,但此时顺序队列的首部还空出了两个数据元素的存储空间。因此,这时的“溢出”并不是由于数组空间不够而产生的溢出。这种因顺序队列的多次人队和出队操作后出现有存储空间,但不能进行人队操作的溢出现象称为”假溢出”。​ 要解决“假溢出”问题,最好的办法就是把顺序队列所使用的存储空间看成是一个逻辑上首尾相连的循环队列。当rear或 front到达 maxSize-1后,再加1就自动到0。这种转换可利用Java语言中对整型数据求模(或取余)运算来实现,即令rear=(rear+1)%maxSize。显然,当rear= maxSize-1时,rear加1后,rear的值就为0。这样,就不会出现顺序队列数组的头部有空的存储空间,而队尾却因数组下标越界而引起的假溢出现象。 循环顺序队列类： 图中会发现一个问题：即循环顺序队列的判空和判满条件都是front==rear 解决循环顺序队列的队空和队满的判断问题常采用以下3种方法： 少用一个存储单元 设置一个标志变量 ​ 在程序设计过程中引进一个标志变量fag,其初始值置为0,每当入队操作成功后就置flag=1;每当出队操作成功后就置fag=0,则此时队空的判断条件为: front==rear&amp;&amp; flag==0,而队满的判断条件为:front==rear&amp;&amp;flag==1。 设置一个计数器 ​ 在程序设计过程中引进一个计数变量num,其初始值置为0,每当入队操作成功后就将计数变量num的值加1;每当出队操作成功后就将计数变量num的值减1,则此时队空的判断条件为:num==0,而队满的判断条件为:num&gt;0&amp;&amp;front==rear 循环队列实现： 接口 12345678910package cn.dataStructure.SqQueue;public interface IQueue &#123; public void clear(); public boolean isEmpty(); public int length(); public Object peek(); public void offer(Object x) throws Exception; public Object poll();&#125; 实现类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package cn.dataStructure.SqQueue;public class CircleSqQueueimpl implements IQueue &#123; private Object[] queueElem; private int front,rear; //构造函数 public CircleSqQueueimpl(int maxSize) &#123; front = rear = 0; //队首、队尾初始化为0 queueElem = new Object[maxSize];//为队列分配maxSize个存储单元 &#125; @Override public void clear() &#123; front = rear = 0; &#125; @Override public boolean isEmpty() &#123; return front == rear; //判空采用第一种方式 &#125; @Override public int length() &#123; return (rear - front + queueElem.length) % queueElem.length; &#125; @Override public Object peek() &#123; if (front ==rear) return null; else return queueElem[front]; &#125; @Override public void offer(Object x) throws Exception &#123; if ((rear+1) % queueElem.length == front) &#123; throw new Exception(\"队列已满\"); &#125; else &#123; queueElem[rear] = x; rear = (rear+1) % queueElem.length;//以免假溢出 &#125; &#125; @Override public Object poll() &#123; if (front == rear) &#123; return null; &#125; else &#123; Object t = queueElem[front]; front = (front + 1) % queueElem.length; return t; &#125; &#125; public void display() &#123; if(!isEmpty()) &#123; for (int i = front; i != rear; i= (i+1)%queueElem.length) &#123; System.out.print(queueElem[i].toString() + \" \"); &#125; &#125;else &#123; System.out.println(\"此队列为空\"); &#125; &#125;&#125; 测试类： 1234567891011121314package cn.dataStructure.SqQueue;public class CircleSqQueueTest &#123; public static void main(String[] args) throws Exception&#123; CircleSqQueueimpl circleSQ= new CircleSqQueueimpl(6); System.out.println(\"长度为：\" + circleSQ.length() + \"判空\" + circleSQ.isEmpty()); circleSQ.display(); //批量导入数据元素 for (int i = 0; i &lt; 5; i++) &#123; circleSQ.offer(i); &#125; circleSQ.display(); &#125;&#125;","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"数据结构","slug":"技术栈/数据结构","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"专业课","slug":"专业课","permalink":"https://ericzikun.github.io/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"数据结构","slug":"数据结构","permalink":"https://ericzikun.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"链表","slug":"链表","permalink":"https://ericzikun.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"栈","slug":"栈","permalink":"https://ericzikun.github.io/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"https://ericzikun.github.io/tags/%E9%98%9F%E5%88%97/"}]},{"title":"css+div布局总结","slug":"css-div布局总结","date":"2020-07-09T11:28:32.000Z","updated":"2020-07-09T14:09:06.452Z","comments":true,"path":"2020/07/09/css-div布局总结/","link":"","permalink":"https://ericzikun.github.io/2020/07/09/css-div%E5%B8%83%E5%B1%80%E6%80%BB%E7%BB%93/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 几个月前花了一周多学了点前端，但也只是浅尝辄止，实战中还是由于css底子不好导致了很多布局的问题，故沉下心来再次把css复习了一遍，理顺了实战中遇到的一些布局问题，包括多浏览器、不同尺寸屏幕的适配问题也更加清晰了许多，有关适配性的经验将会在另一博客中着重总结，此博客主要针对css的基础布局进行梳理，方便复查！期间，也发现了一个查前端文档的网站：MDN 定位 相对定位以当前为参照物移动指定的距离注意：相对定位，被定位的元素会占据原有的物理位置 1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;相对定位&lt;/title&gt; &lt;style&gt; div&#123; width:200px; height:200px; &#125; #box1&#123; background: red; &#125; #box2&#123; background:green; /*相对定位 以当前为参照物移动指定的距离 注意：相对定位，被定位的元素会占据原有的物理位置 */ position:relative; /*移动定位元素*/ top:200px;/*bottom*/ left:200px;/*right*/ &#125; #box3&#123; background: blue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"box1\"&gt;&lt;/div&gt; &lt;div id=\"box2\"&gt;&lt;/div&gt; &lt;div id=\"box3\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 绝对定位 绝对定位的元素不会占据原有的物理位置 以其他元素作为参考物移动指定距离的定位方式 关于绝对定位的参考点： 如果元素的外层元素是非static（有了除默认值之外的定位设置）那么这个外层元素就成为该元素的定位参考点 如果元素的外层元素没有设置任何position的值，那么该元素将寻找距离自己最近的其他设定过position的外层元素作为参照物(必须为嵌套层) 如果该元素的外层元素没有任何一个元素采用position定位，那么此时定位参考元素变为body或者说页面 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;绝对定位&lt;/title&gt; &lt;style&gt; div&#123; width:200px; height:200px; &#125; #box1&#123; background: red; &#125; #box2&#123; background:green; position: absolute; /*top:200px; left:200px;*/ &#125; #box3&#123; background: blue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"box1\"&gt;&lt;/div&gt; &lt;div id=\"box2\"&gt;&lt;/div&gt; &lt;div id=\"box3\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 加上 top:200px; left:200px; 参考点为(0,0)绝对左上角，而不是红色框左上角 如果外层元素没有定位，但是外外层（爷爷类）有定位，则有： 如果元素的外层元素没有设置任何position的值，那么该元素将寻找距离自己最近的其他设定过position的外层元素作为参照物(必须为嵌套层) 子类参考点为外外层（爷爷类） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;绝对定位&lt;/title&gt; &lt;style&gt; div&#123; width:200px; height:200px; &#125; #box1&#123; background: red; &#125; #box2&#123; width:500px; height:500px; background:green; border:1px solid green; /*绝对定位： 绝对定位的元素不会占据原有的物理位置 以其他元素作为参考物移动指定距离的定位方式 关于绝对定位的参考点： 1.如果元素的外层元素是非static（有了除默认值之外的定位设置）那么这个外层元素就成为该元素的定位参考点 2.如果元素的外层元素没有设置任何position的值，那么该元素将寻找距离自己最近的其他设定过position的外层元素作为参照物(必须为嵌套层) 3.如果该元素的外层元素没有任何一个元素采用position定位，那么此时定位参考元素变为body或者说页面 */ /*position: absolute;*/ /*top:200px;*/ /*left:200px;*/ position: relative; &#125; #father&#123; width:200px; height:200px; background: aquamarine; /*父类做一个定位*/ /*position:relative;*/ margin:20px; &#125; #son&#123; width:100px; height:100px; background:fuchsia; position:absolute; /*son的参考点是father，但前提是father这一层（外层）必须有定位*/ top:100px; left:100px; &#125; #box3&#123; background: blue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"box1\"&gt;&lt;/div&gt; &lt;div id=\"box2\"&gt; &lt;div id=\"father\"&gt; &lt;div id=\"son\"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=\"box3\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 如果son的father没有定位，box2也没定位，则其absolute参考点为body： 固定定位垂直滚动条： background:palevioletred; 12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;固定定位&lt;/title&gt; &lt;style&gt; div&#123; height:2000px; background:palevioletred; &#125; #box&#123; width:80px; height:400px; background: green; position: fixed; right:0px; /*距离右边为0*/ top:300px; /*距离顶部为0*/ &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;1 &lt;div&gt; &lt;div id=\"box\"&gt;&lt;/div&gt; &lt;/div&gt;2&lt;/body&gt;&lt;/html&gt; 堆叠顺序 注意：只能支持定位元素！！！ 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;z-index设置定位元素z轴的距离(定位元素的显示顺序)&lt;/title&gt; &lt;style&gt; #one&#123; width:200px; height:200px; background: red; position: absolute; top:100px; left:100px; z-index:1; &#125; #two&#123; width:200px; height:200px; background: yellow; position:absolute; top:150px; left:150px; z-index:999; /*这里设置1仍然是黄色盖住红色*/ &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;!--注意：z-index属性只支持定位元素--&gt; &lt;div id=\"one\"&gt;&lt;/div&gt; &lt;div id=\"two\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 首先看代码先后顺序，渲染的是one在前，two在后，所以黄色会盖住红色，z-index可以提高层级别 display回顾浮动：任意元素一旦浮动，display都会失效 预备知识： div:默认为块状元素：具有宽高属性，并且独占一行 Span:默认为行内元素：没有宽高属性，不会独占一行 块状&amp;行内元素的互换： 如果将div设置为行内元素display: inline，则会使得div无宽高属性，且不会独占一行！ 也可以将span设置为块状元素:display:block，则可以有宽高属性，独占一行！ 如果用了很多块状元素div，但是会独占一行，如果不想独占一行（想并列显示），则要将其改为 行内块元素：display:inline-block——既有宽高属性 且不会独占一行，但涉及图文混排，故此方式用的较少，浮动使用较多 接着将div的display改为表格属性display:table-cell,变为单元格形式， 隐藏元素： display:none可以将div块进行隐藏，并且不会占用原物理空间（类似：visibility:hidden隐藏效果，但仍占用物理空间） 如果元素是使用visibility设置的隐藏方式，那么只能用visibility：visible的方式来让元素显示;对应的，display:none要用display:block来显示！ 总结：visibility:设置元素是否显示visible 显示 hidden隐藏 注意：visibility和display:none的区别：visibility的隐藏方式仅隐藏了内容的显示，其占用的空间依旧占用，而display:none的隐藏方式是彻底隐藏该元素的内容和位置。 应用：网页中鼠标未选中时隐藏，选中时显示 展开 设置透明度：opacity: 0;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;display&lt;/title&gt; &lt;style&gt; div,span&#123; width:200px; height:200px; &#125; div&#123; background: red; /*设置为行内元素*/ /*display: inline;*/ /*设置元素为行内块元素，具有宽高属性，并且不会独占一行*/ /*display: inline-block;*/ /*display: table-cell;*/ /*隐藏元素*/ /*display: none;*/ /*display: block;*/ /*隐藏元素，占据原有物理位置*/ /*visibility: hidden;*/ /*如果元素是使用visibility设置的隐藏方式，那么只能用visibility：visible的方式来让元素显示*/ /*visibility:visible;*/ /* visibility:设置元素是否显示 visible 显示 hidden隐藏 注意：visibility和display:none的区别：visibility的隐藏方式仅隐藏了内容的显示，其占用的空间依旧占用，而display:none的隐藏方式是 测地隐藏该元素的内容和位置。 */ /*设置元素透明度*/ opacity: 0; &#125; span&#123; /*设置为块状元素*/ /*display: block;*/ background: fuchsia; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;!--div:块状元素：具有宽高属性，并且独占一行--&gt; &lt;div&gt;这是块状元素&lt;/div&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt;&lt;!-- &lt;div&gt;这是块状元素&lt;/div&gt;--&gt; &lt;!--行内元素：没有宽高属性，不会独占一行--&gt; &lt;span&gt;这是行内元素&lt;/span&gt;&lt;/body&gt;&lt;/html&gt; 盒子模型 预备知识：盒子与盒子之间的距离——margin：外间距 盒子内部的content与边框的间距——padding：内边距 边框也有一个厚度：border：边框 margin 快速设置： 顺序：上右下左margin:10px 10px 10px 10px上下20px，左右都为10pxmargin:20px 10px 20px上下20px，左右为10pxmargin:20px 10px 居中显示：如果想要div居中，可以margin:10px auto;实现块状元素居中显示，如果改为浮动，则display失效,无法居中 外边距合并： 几个特点： 块级元素的垂直相邻外边距会合并 行内元素实际上不占上下外边距，行内元素的的左右外边距不合并 浮动元素的外边距也不会合并 允许指定负的外边距值，不过使用时要小心 取两者最大值，而非之和！ 合并只有上下 子父类div： margin负值： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;盒子模型&lt;/title&gt; &lt;style&gt; #box&#123; width:620px; height:620px; background: pink; &#125; #box&gt;div:nth-child(1)&#123; width:600px; height:200px; background: blue; float:left; &#125; #box&gt;div:nth-child(2)&#123; width:200px; height:200px; background: yellow; float:left; &#125; #box&gt;div:nth-child(3)&#123; width:200px; height:200px; background: green; float:left; /*当前元素与其他元素之间的距离 外间距*/ /*margin-top:10px;*/ /*margin-bottom:10px;*/ /*margin-left:10px;*/ /*margin-right:10px;*/ /*margin:10px 10px 10px 10px;*/ /*margin:20px 10px 20px;*/ /*margin:20px 10px;*/ margin:10px; &#125; #box&gt;div:nth-child(4)&#123; width:200px; height:200px; background: aqua; float:left; &#125; #box&gt;div:nth-child(5)&#123; width:600px; height:200px; background: blueviolet; float:left; &#125; #block&#123; width:1200px; background: blue; height: 40px; /*实现让块状元素居中显示*/ margin:10px auto; &#125; #box1&#123; margin-top: -100px; width:200px; background: red; height:200px; margin-bottom: 100px; &#125; #box2&#123; width:200px; background: yellow; height: 200px; margin-top:50px; &#125; #father&#123; width: 500px; height:200px; /*加一像素红色 实线边框*/ border: 1px solid red; background: green; &#125; #son&#123; width:200px; height:100px; background: pink; margin-top:20px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"box\"&gt; &lt;div&gt;顶部元素&lt;/div&gt; &lt;div&gt;左侧元素&lt;/div&gt; &lt;div id=\"self\"&gt;当前元素&lt;/div&gt; &lt;div&gt;右侧元素&lt;/div&gt; &lt;div&gt;底部元素&lt;/div&gt; &lt;/div&gt; &lt;!--可以设置块状元素居中--&gt; &lt;div id=\"block\"&gt;&lt;/div&gt; &lt;!--外间距合并--&gt; &lt;div id=\"box1\"&gt;&lt;/div&gt; &lt;div id=\"box2\"&gt;&lt;/div&gt; &lt;!--包含式外间距合并--&gt; &lt;div id=\"father\"&gt; &lt;div id=\"son\"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 边框border设置为border: 5px solid red 每条边分开设置： 统一设置（大多数情况下）： 双线注意： padding 总结： padding的宽高要记录在盒子模型的宽高之内，于此相同的是border也要记录在盒子模型的宽高之内，但是margin并不算在宽高之内。所以在书写宽高时注意减掉内边距和边框(标准盒模型) 宽高指的是content的！回顾如图： 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;padding&lt;/title&gt; &lt;style&gt; span&#123; border:1px solid red; float:left; /*padding-top:10px;*/ /*padding-left:10px;*/ /*padding-right:10px;*/ /*padding-bottom: 10px;*/ /*padding:10px;*/ /*padding:10px 20px;*/ /*padding: 10px 20px 15px;*/ padding: 10px 20px 30px 40px; &#125; div&#123; width:150px; height:150px; background: red; clear: both; border:5px solid red; padding: 20px; /*margin:20px;*/ /* padding的宽高要记录在盒子模型的宽高之内，于此相同的是border也要记录在盒子模型的宽高之内，但是margin并不算在宽高之内。所以各位在书写宽高时注意减掉内边距和边框(标准盒模型) */ &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;span&gt;这是内容&lt;/span&gt; &lt;div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 怪异盒模型： 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;怪异盒&lt;/title&gt; &lt;style&gt; div&#123; width:200px; height:200px; padding:20px; border:5px solid red; background: red; /*设置怪异盒*/ box-sizing: border-box; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; div+css布局注意事项 参考视频：b站：一天学会DIV+CSS布局","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"React","slug":"技术栈/React","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/React/"}],"tags":[{"name":"css","slug":"css","permalink":"https://ericzikun.github.io/tags/css/"},{"name":"布局","slug":"布局","permalink":"https://ericzikun.github.io/tags/%E5%B8%83%E5%B1%80/"},{"name":"盒子模型","slug":"盒子模型","permalink":"https://ericzikun.github.io/tags/%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Convlstm时空预测经验之谈（本科毕设）","slug":"Convlstm时空预测经验之谈","date":"2020-05-16T05:08:46.000Z","updated":"2020-06-07T09:24:50.834Z","comments":true,"path":"2020/05/16/Convlstm时空预测经验之谈/","link":"","permalink":"https://ericzikun.github.io/2020/05/16/Convlstm%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B%E7%BB%8F%E9%AA%8C%E4%B9%8B%E8%B0%88/","excerpt":"","text":"Convlstm新手实战&nbsp;&nbsp;&nbsp;毕设临近截止，故写一篇心得以供新手学习，理论在知乎上有很多介绍的不错的文章，这里强烈推荐微信公众号：AI蜗牛车，这位东南老哥写了时空预测系列文章，能够帮助了解时空领域模型的演变，同时也向他请教了一些训练技巧。&nbsp;&nbsp;&nbsp;我的本科毕设大概是这样的：先计算某个区域的风险，计算得到一段时间的风险矩阵，这里用的是自己的模型去计算的，数据如何生成，本文不做赘述，主要讲解如果通过每个时刻下的矩阵数据去预测未来的矩阵。 回顾理论基础&nbsp;&nbsp;&nbsp;在ConvLSTM中，网络用于捕获数据集中的时空依赖性。ConvLSTM和FC-LSTM之间的区别在于，ConvLSTM将LSTM的前馈方法从Hadamard乘积变为卷积，即input-to-gate和gate-to-gate两个方向的运算均做卷积,也就是之前W和h点乘改为卷积（*）。 ConvLSTM的主要公式如下所示： 详细可参考：【时空序列预测第二篇】Convolutional LSTM Network-paper reading 官方keras案例&nbsp;&nbsp;&nbsp;实战过的朋友应该了解，关于Convlstm，可参考的案例非常少，基本上就集中在keras的官方案例（电影帧预测——视频预测官方案例）知乎解说 官方模型核心代码： 1234567891011121314151617181920212223242526272829from keras.models import Sequentialfrom keras.layers.convolutional import Conv3Dfrom keras.layers.convolutional_recurrent import ConvLSTM2Dfrom keras.layers.normalization import BatchNormalizationimport numpy as npimport pylab as plt seq = Sequential()seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(None, 40, 40, 1), padding='same', return_sequences=True))seq.add(BatchNormalization())seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True))seq.add(BatchNormalization())seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True))seq.add(BatchNormalization())seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True))seq.add(BatchNormalization())seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same', data_format='channels_last'))seq.compile(loss='binary_crossentropy', optimizer='adadelta') &nbsp;&nbsp;&nbsp;模型结构可以如官方一样：用前20个预测后20个，这里先解释一下官方模型结构的维度： &nbsp;&nbsp;&nbsp; （如已熟悉，请跳过）对于新手来说，看上去似乎很复杂，其实弄清楚后会发现不过如此，请耐心听我讲完：先从第一个Convlstm说起，输入的是(None, 40, 40, 1)，输出的维度（None，None，40，40，40），这里的输入维度（input_shape）其实是每个时刻下的输入，如下图：比如这里用20个预测后20个，那么整理的第一个样本就是0至19个矩阵，label（标签）就是20至39个矩阵，每一个矩阵维度为（40，40，1）最后的这个1为通道数，如果是图片，那就对应多通道了，那么整理的样本X就应该是（样本个数,20，40，40，1），对应标签Y就是（样本个数，20，40，40，1）这样每个样本和标签才能一一对应，由于reurn_sequence为true，即每个时刻单元都有输出，也就是20个预测20个嘛，那么第一层的Convlstm输出的维度就是（None，None，40，40，40）这里第一个None是batchsize毫无疑问，第二个其实就是20，至于最后一个维度是40，和filter个数直接相关，（因为一个卷积核对样本做一次特征提取，40个就有40个特征提取）。&nbsp;&nbsp;&nbsp;接下来N层Convlstm均如此，最后为啥要接一个Conv3d，很好解释，因为你的label维度是（样本个数，20，40，40，1），这里的最后维度还得回归到1啊，所以Conv3d的filter这才设置为了1，以此类推，如果你的一个数据是三通道的图像，这里filter自然就是3了，一定要和label维度对应即可。 ConvLSTM参数介绍 filters: 卷积核的数目 kernel_size： 卷积核大小（1乘1的state-to-state kernel size很难抓住时空移动的特征，所以效果差很多，所以更大的size更能够获取时空的联系） strides： (1,1)为卷积的步长，即卷积核向右和向下一次移动几格，默认步长为1 padding： 补0，为“valid”或 “same”。若要保证卷积核提取特征后前后维度一致，那就“same” data_format: 即红绿蓝三个通道(channel)是在前面还是在后面，channels_last (默认) （width, height, channel）或 channels_first (channel, width, height) 之一, 输入中维度的顺序 activation： 激活函数，即下图中的RELU层，为预定义的激活函数名，如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x） 模型改造&nbsp;&nbsp;&nbsp;不过我由于数据量比较少，我把模型结构改造成了20个预测1个（样本数较少的童鞋可以参考），在convlstm最后一个层的reurn_sequence参数改为flase、Conv3d改2d即可。&nbsp;&nbsp;&nbsp;其实了解了reurn_sequence这个参数后，改造就顺理成章了，在最后一个Convlstm这里将reurn_sequence改为false，那么就只在最后一个单元有输出了，第二个None维度就没了，然后再把Conv3d改为2d即可，这样就要求整理数据集的时候，样本和标签分别整理成这样：(样本数，20，40，40，1) 和（样本数，40，40，1），也就是20个预测1个。 12345678910111213141516171819202122232425262728from keras.models import Sequentialfrom keras.layers.convolutional import Conv3D ,Conv2Dfrom keras.layers.convolutional_recurrent import ConvLSTM2Dfrom keras.layers.normalization import BatchNormalizationfrom keras_contrib.losses import DSSIMObjectiveimport numpy as npseq = Sequential()seq.add(ConvLSTM2D(filters=30, kernel_size=(3, 3), input_shape=(None, 60, 93, 3), padding='same', return_sequences=True))seq.add(BatchNormalization()) seq.add(ConvLSTM2D(filters=30, kernel_size=(3, 3), padding='same', return_sequences=True))seq.add(BatchNormalization()) seq.add(ConvLSTM2D(filters=30, kernel_size=(3, 3), padding='same', return_sequences=False))seq.add(BatchNormalization()) seq.add(Conv2D(filters=3, kernel_size=(3, 3), activation='sigmoid', padding='same', data_format='channels_last'))seq.compile(loss= DSSIMObjective(kernel_sizesize=3), optimizer='adadelta')seq.summary() 模型经验及调参&nbsp;&nbsp;&nbsp;先看看结果图吧，随便抽一张示意一下，预测的点相对比较准确，但是模糊度还没解决掉，毕竟只训练了十几分钟，有这个效果也还算可以了： &nbsp;&nbsp;&nbsp; 整个模型看上去不算复杂，但是实际效果比较差，有以下几个要稍微注意的地方： 1.矩阵数据是否过于稀疏，如果0太多，建议先转成图片再做训练，否则效果会奇差无比，原因可能是求梯度的时候网络出了问题，直接崩了。 2.如果输入是图片张量，需要提前做好归一化，我用的简单处理，直接元素除255.0，显示的时候再乘回来即可，可能有一丢丢颜色误差，但是不太影响。 3.预测图片出现模糊大概有以下几个原因：（1）网络结构不够优（继续调就完事了），往往这种情况下，得到的预测点也不会太准确。（2）由于是多个时刻下的数据去预测一个，那么必然存在信息叠加（融合），这样导致的模糊是不可避免的，如果数据量很大，那么可以采用20帧预测20帧这样的结构，应该会有效减缓一点模糊程度。（3）重要： 损失函数若使用MSE则会默认模糊，如果换成SSIM（结构相似性）则会明显改观（亲测有效）&nbsp;&nbsp;&nbsp;在模糊处理方面，我也想尝试改进，但是还没有找到比较好的方式，蜗牛车老哥建议调小学习率，训练时间长一点，亲测有效！反卷积也尝试了，但是效果不佳，后期准备使用TrajGRU来实战（预测解码模块采用了上采样层理论上应该会提高清晰度）。 &nbsp;&nbsp;&nbsp;模型调参的过程其实是最无聊也最艰辛的，无非就是改改层结构，多一层少一层，改一下filter、batchsize个数，时空预测这种图像的预测和别的领域有一点不同，文本的只要acc、f1-score上去了就行，所以可以用grid search来自动化调参，但是图像预测还必须得肉眼去看效果，否则结果真可能是千差万别，loss看上去已经很低了但是效果很差的情况比比皆是，尝试多换几种loss来实验，后面也还可以尝试自定义loss看效果，整个调参过程确实是不断试错的过程，两个字：”炼丹”! ————————下图为2020.6.1更新,毕设最新效果，采用trick—————————","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Convlstm","slug":"深度学习/Convlstm","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Convlstm/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Convlstm","slug":"Convlstm","permalink":"https://ericzikun.github.io/tags/Convlstm/"},{"name":"时空预测","slug":"时空预测","permalink":"https://ericzikun.github.io/tags/%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B/"},{"name":"时序","slug":"时序","permalink":"https://ericzikun.github.io/tags/%E6%97%B6%E5%BA%8F/"},{"name":"Lstm","slug":"Lstm","permalink":"https://ericzikun.github.io/tags/Lstm/"},{"name":"CNN","slug":"CNN","permalink":"https://ericzikun.github.io/tags/CNN/"}]},{"title":"React项目迅速搭建+antd传文件+nginx反向代理","slug":"React项目迅速搭建-antd传文件-nginx反向代理","date":"2020-05-16T02:00:14.000Z","updated":"2020-06-07T09:36:09.367Z","comments":true,"path":"2020/05/16/React项目迅速搭建-antd传文件-nginx反向代理/","link":"","permalink":"https://ericzikun.github.io/2020/05/16/React%E9%A1%B9%E7%9B%AE%E8%BF%85%E9%80%9F%E6%90%AD%E5%BB%BA-antd%E4%BC%A0%E6%96%87%E4%BB%B6-nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/","excerpt":"","text":"React 入门+实战（antd上传文件接受返回+部署服务器Nginx反向代理）React 项目迅速搭建前期工作： 1、必须安装nodejs 注意：安装nodejs稳定版本 2、安装cnpm：用cnpm替代npm 安装cnpm: npm install -g cnpm --registry=https://registry.npm.taobao.org 3、用yarn替代npm yarn的安装： 第一种方法：参考官方文档https://yarn.bootcss.com/ 第二种方法：cnpm install -g yarn 或者 npm install -g yarn搭建项目（初始化）：搭建React开发环境的第一种方法（老-推荐）： https://reactjs.org/docs/create-a-new-react-app.html 1. 再次提醒：必须要安装nodejs 注意：安装nodejs稳定版本 2. 安装脚手架工具 （单文件组件项目生成工具） 只需要安装一次 npm install -g create-react-app / cnpm install -g create-react-app 3. 创建项目 （可能创建多次） 找到项目要创建的目录： create-react-app reactdemo 4. cd 到项目里面 cd reactdemo npm start 或者 yarn start运行项目 npm run build 或者 yarn build 生成项目（个人喜欢用yarn）项目目录结构： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一般更多需要关注的在于src文件，assets可以放css文件、图片等素材，component就是自定义的组件，比如这里我定义了Home和Result组件，这两个组件最终在src下的App.js根组件中挂载，这里的两个组件其实是我写的两个页面，并且在根组件App.js中实现路由跳转。 组件结构：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; React有两种写组件的方式，一种用class extends..来写，另一种用函数式来写，本人较亲耐于函数式，简洁，并且没有this指向的问题，也就是React 后更新的hook特性，第一种写组件的方式中，初始化变量需要：this.setState()，而hook特性带来了useState（）,写法更加简洁，容易理解。 hook：用const定义function方法来替换以往class定义的方式 useState()：初始化一个变量——const [variable,setVariable] = useState(variable0)后面变量赋值直接用:setVarible(newVariable),即可改变变量状态 useEffect()——相当于是以往的生命周期函数： componentDidMount（）；当组件被挂载时，立即启动useEffect，而useEffect的参数是一个function 123456import React, &#123; Component &#125; from 'react'const Home = () =&gt; &#123; return ( )&#125;export default Home antd实战upload传文件12345678910111213141516171819202122232425262728293031import React, &#123; Component &#125; from 'react'import &#123; Upload, message, Button &#125; from 'antd'import &#123; UploadOutlined &#125; from '@ant-design/icons'...const Home = () =&gt; &#123; const props = &#123; name: 'file',//name得看接口需求，name与接口需要的name一致 action: '/test/upload/file',//接口路径 data: &#123;file&#125; ,//接口需要的参数，无参数可以不写 multiple: false,//支持多个文件 showUploadList: true,//展示文件列表 headers: &#123; // \"Content-Type\": \"multipart/form-data\" &#125;, &#125; return ( ... &lt;Upload &#123;...props&#125; fileList=&#123;file&#125; onChange=&#123;onChange&#125; &gt; &lt;Button type=\"Link\" shape=\"round\" size=\"large\"&gt; &lt;UploadOutlined /&gt; Select file &lt;/Button&gt; &lt;/Upload&gt; ) &#125;export default Home upload传文件这里有几个坑： 本地localhost地址要去调服务器下的接口，应该先解决跨域问题：网上有不少直接在package.json中改，这个现在行不通了，亲测要安装http-proxy-middleware即：npm install http-proxy-middleware --save或yarn add http-proxy-middleware --save安装完后，在src下新建一个setupProxy.js，附上以下代码： 12345678910const &#123;createProxyMiddleware&#125; = require('http-proxy-middleware')module.exports = function (app) &#123; // proxy第一个参数为要代理的路由 // 第二参数中target为代理后的请求网址，changeOrigin是否改变请求头，其他参数请看官网 app.use(createProxyMiddleware('/test', &#123; target: 'http://xxx.xx.xxx.xxx...', # 这里就是你要跨到的服务器接口地址 changeOrigin: true &#125;))&#125; 这里修改了之后，记得重启服务后，跨域设置才能生效（yarn start 或 npm start），接着在upload中action位置写上接口后面的地址，不能写全部地址了，得写相对地址，否则会报跨域错误：错误如下： 再就是关于前后端请求头的问题，比较麻烦，每个人遇到的情况都不一定相同，我是传一个xml/pdf文件到后端，接受一个response返回，header里面，我如果像postman那样写&quot;Content-Type&quot;: &quot;multipart/form-data&quot;反而传不过去，但是注释掉后，能正常把文件传到后端，一段时间后在浏览器network里面可以接收到response： 部署到win服务器Nginx反向代理 安装、启动Nginx&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我演示的是win服务器的部署，其他的可能略有区别，首先安装Nginx，直接去官网下载安装即可，win下启动cmd (1)直接双击nginx.exe，双击后一个黑色的弹窗一闪而过 (2)打开cmd命令窗口，切换到nginx解压目录下，输入命令 nginx.exe 或者 start nginx ，回车即可&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在服务器输入：http://localhost:80 ，有Nginx界面则启动成成功。 配置Nginx&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 配置之前，需要先把React项目打包，打包很简单，直接yarn build，会在项目目录生成一个build文件： 然后将项目复制到你的win服务器上，在Nginx安装目录下有一个Nginx的conf文件，主要改一下前端端口以及root地址（也就是build文件夹所在地址）： 123456789101112server &#123; listen 80; server_name smartcite; #charset koi8-r; #access_log logs&#x2F;host.access.log main; location &#x2F; &#123; root C:\\samart_cite_fronted\\project_pdf2xml\\project_pdf2xml\\build; index index.html; &#125; listen ：设置的端口号 server_name ：访问的名字 root ：你项目所放的地址 index index.html ：你的入口html文件因为是单页应用，所以，是根据路由跳转，为避免出现404，我们需要重写至index.html Nginx常用命令：nginx开启命令：start nginx nginx停止命令：nginx -s quit nginx重启命令：nginx -s reload总结：Nginx部署过程注意：windows server部署react项目 1.安装并启用nginx服务 2.在nginx.conf文件中,添加server配置,端口号改为自己想要设置的,然后热重载配置123456789server &#123; listen 80; server_name localhost; location &#x2F; &#123; root C:\\samart_cite_web; index index.html; try_files $uri $uri&#x2F; &#x2F;index.html; &#125;&#125; 3.安装Openssl（ssh工具方便传文件，也可以用FileZilla（mac端）、filetransfer（win端）） 4.windows防火墙添加入站规则 将80端口或上述设置的端口添加进去 5.踩坑：如果前后端在一个服务器上，但是端口不同，同样属于跨域，可以直接在接口地址action位置写全部的接口路径，但是要让后端在响应头里面加几句话允许跨域： 部署到linux服务器相比于Win部署，大差不差！没有win上的防火墙那一步，相对省心一些！ 1.在服务器上安装linux版本的nginx 2.在服务器上建立一个项目文件夹，例如：/home/smartcite 3.将本地React项目的build包传到linux下的/home/smartcite, 4.修改linux服务器上的nignx配置：找到default.conf文件，一般在/etc/nginx/sites-available里面，有的也可能在/etc/nginx/conf.d下修改采用：vi default，修改里面对应的root地址(这个地址就是你的项目在linux服务器上的路径)参考博客：https://blog.csdn.net/wly_er/article/details/82348593https://www.jb51.net/article/152781.htm","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"React","slug":"技术栈/React","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/React/"}],"tags":[{"name":"React","slug":"React","permalink":"https://ericzikun.github.io/tags/React/"},{"name":"antd","slug":"antd","permalink":"https://ericzikun.github.io/tags/antd/"},{"name":"nginx反向代理","slug":"nginx反向代理","permalink":"https://ericzikun.github.io/tags/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/"}]},{"title":"爬虫&正则表达式基础","slug":"爬虫-正则表达式","date":"2020-04-18T14:43:24.000Z","updated":"2020-04-26T06:12:01.710Z","comments":true,"path":"2020/04/18/爬虫-正则表达式/","link":"","permalink":"https://ericzikun.github.io/2020/04/18/%E7%88%AC%E8%99%AB-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"爬虫流程： 正则表达式 正则表达式由字符和操作符组成 常用操作符： 举例： 经典正则表达式： eg： Re库： re库可以采用raw string类型表示正则表达式，表示为： r&#39;text&#39;, PS: raw string是不包含对转义符再次转义的字符串 例如：r&#39;[1‐9]\\d{5}&#39;和r&#39;\\d{3}‐\\d{8}|\\d{4}‐\\d{7}&#39; Re库主要函数： re.search： 在整个字符串中搜索匹配 123456import rematch = re.search(r'[1-9]\\d&#123;5&#125;','BIT 100081')if match: print(match.group(0))#100081 re.match: 与re.search类似，但是是从字符串开始位置起匹配表达式，返回match对象 12345import rematch = re.match(r'[1-9]\\d&#123;5&#125;','BIT 100081')if match: print(match.group(0))#报错，因为BIT未匹配到 re.findall: 1234import rels = re.findall(r'[1-9]\\d&#123;5&#125;','BIT100081 TSU10084')ls# ['10081','10084'] re.split: 12345import rere.split(r'[1-9]\\d&#123;5&#125;','BIT100081 TSU10084')#['BIT','TSU','']re.split(r'[1-9]\\d&#123;5&#125;','BIT100081 TSU10084',maxsplit=1)#['BIT','TSU100084'] re.finditer: 1234import refor m in re.finditer(r'[1-9]\\d&#123;5&#125;','BIT100081 TSU100084')# 100081# 100084 re.sub: 123import rere.sub(r'[1-9]\\d&#123;5&#125;',':zipcode',\"BIT100081 TSU100084\")# 'BIT:zipcode TSU:zipcode' re库的面向对象方法： 1rst = re.search(r'[1-9]\\d&#123;5&#125;',\"BIT 100081\") #对比——函数式：一次性操作 12pat = re.compile(r'[1-9]\\d&#123;5&#125;')rst = pat.search('BIT 100081') #面向对象用法 **re库的match对象：** match对象是一次匹配的结果，包含匹配的很多信息 12345match = re.search(r'[1-9]\\d&#123;5&#125;','BIT 100081')if match: print(match.group(0))type(match)# &lt;class '_sre.SRE_Match'&gt; 12345678910111213141516import rem =re.search(r'[1-9]\\d&#123;5&#125;','BIT100081 TSU100084')m.string#\"BIT100081 TSU100084\"m.pos# 0m.endpos# 19m.group(0)# '100081'm.start()# 3m.end()# 9m.span()# (3,9) re库的贪婪匹配和最小匹配： 12345678# 贪婪匹配——默认采用贪婪匹配，即输出匹配最长的子串match = re.search(r'PY.*N','PYANBNCNDN') # *代表对前一个字符,也就是'.'的无限次或0次扩展，而'.'为任意match.group(0)# 'PYANBNCNDN'#最小匹配：加一个问号match = re.search(r'PY.*?N','PYANBNCNDN')match.group(0) 总结： 参考：b站视频：嵩天教授的Python网络爬虫与信息提取课程","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"文本处理","slug":"技巧/文本处理","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"}],"tags":[{"name":"文本处理","slug":"文本处理","permalink":"https://ericzikun.github.io/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"},{"name":"爬虫","slug":"爬虫","permalink":"https://ericzikun.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://ericzikun.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"Transformer 原理+源码分析总结(Tensorflow官方源码)","slug":"Transformer-原理-源码分析总结-Tensorflow官方源码","date":"2020-04-11T09:49:11.000Z","updated":"2020-04-12T07:09:27.959Z","comments":true,"path":"2020/04/11/Transformer-原理-源码分析总结-Tensorflow官方源码/","link":"","permalink":"https://ericzikun.github.io/2020/04/11/Transformer-%E5%8E%9F%E7%90%86-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93-Tensorflow%E5%AE%98%E6%96%B9%E6%BA%90%E7%A0%81/","excerpt":"","text":"Transformer理解参考博客：https://jalammar.github.io/illustrated-transformer/https://github.com/aespresso/a_journey_into_math_of_mlhttps://www.tensorflow.org/tutorials/text/transformer#encoder_and_decoder &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学了 TextCNN、LSTM后，谈起如今NLP最流行、最热的模型，当然是Transformer、bert，语言模型、命名实体识别、机器翻译等任务，很多都开始用Transformer，或者说是bert预训练模型来做，在机器阅读理解榜单中（SQuAD2.0），机器成绩已经超越人类表现！这些天看了几个经典博客、视频，最后读了一遍源码，加深了对模型的理解，整体结构也基本上理顺了。 背景：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; transformer是谷歌大脑在2017年底发表的论文 attention is all you need中所提出的seq2seq模型。现在已经取得了大范围的应用和扩展,而BERT就是从 transformer中衍生出来的预训练语言模型。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其中主要的应用的方式是2步——先进行预训练语言模型——然后把预训练的模型适配给下游任务（分类、生成、标记等）。其中：预训练模型非常重要,预训练的模型的性能直接影响下游任务。 原理先上图吧： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 整个Transformer结构分为：Encoding（编码器）和Decoding（解码器）两大部分；而编码器又有N个编码器层，解码器也有N个解码器层；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 那么先把一个编码器层搞清楚，串联N个就能理解了；理解好了编码器，解码器就快了。一个编码器层包含五个组成部分：$\\begin{cases} 1. Positional Encoding\\\\2.Multi-Head Attention\\\\3. Add\\&amp;Norm\\\\4.FeedForward\\\\5.Add\\&amp;Norm\\end{cases}$看似很复杂，一个一个来就不怕： 1.Positional Encoding&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 首先得清楚为何Transformer用位置嵌入？在LSTM中我们用一个词一个词灌进去，从而学习了时序关系，但是transformer模型没有循环神经网络的迭代操作, 它是将所有词一起喂进去，并行操作的。所以我们必须提供每个字的位置信息给transformer, 才能识别出语言中的顺序关系。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 位置嵌入的定义其实就是作者自定义的一个函数，来做到区别每个词在句子中的位置，仅此而已。定义 ：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 位置嵌入的维度为$[max \\ sequence \\ length, \\ embedding \\ dimension]$, 嵌入的维度同词向量的维度, $max \\ sequence \\ length$属于超参数, 指的是限定的最大单个句长.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在transformer模型中,一般以字为单位，不需要分词了, 首先我们要初始化字向量为$[vocab \\ size, \\ embedding \\ dimension]$, $vocab \\ size$为总共的字库数量, $embedding \\ dimension$为字向量的维度, 也是每个字的向量。（这里的理解和之前的TextCNN LSTM中的$Embedding$一致！）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在这里论文中使用了$sine$和$cosine$函数的线性变换来提供给模型位置信息:$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\quad PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$上式中$pos$指的是句中字的位置, 取值范围是$[0, \\ max \\ sequence \\ length)$, $i$指的是词向量的维度, 取值范围是$[0, \\ embedding \\ dimension)$, 上面有$sin$和$cos$一组公式, 也就是对应着$embedding \\ dimension$维度的一组奇数和偶数的序号的维度, 例如$0, 1$一组, $2, 3$一组, 分别用上面的$sin$和$cos$函数做处理, 从而产生不同的周期性变化。看源码（就是对应的上面的公式）： 1234567891011121314151617def get_angles(pos, i, d_model): angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)) return pos * angle_ratesdef positional_encoding(position, d_model): angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model) # 将 sin 应用于数组中的偶数索引（indices）；2i angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # 将 cos 应用于数组中的奇数索引；2i+1 angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) pos_encoding = angle_rads[np.newaxis, ...] return tf.cast(pos_encoding, dtype=tf.float32) 输出这个周期性的矩阵图： 123456789pos_encoding = positional_encoding(50, 512)print (pos_encoding.shape)plt.pcolormesh(pos_encoding[0], cmap='RdBu')plt.xlabel('Depth')plt.xlim((0, 512))plt.ylabel('Position')plt.colorbar()plt.show() 2.Muti-Head Attention&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在前面的基础上，我们已经有了词向量矩阵和位置嵌入了，例如有一些样本。维度是：$[batch size, \\ sequence \\ length]$,再在字典中找到对应的字向量，变为：$[batch size, \\ sequence \\ length, \\ embedding \\ dimension]$,同时我们再加上位置嵌入（位置嵌入维度一致，直接元素相加即可），相加后的维度还是$[batch size, \\ sequence \\ length, \\ embedding \\ dimension]$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 要了解Muti-Head Attention，首先要知道self-attention,Multi无非是在其基础上并行了多个头而已。 2.1 self Attention&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Attention机制的创新点就在于这里，为了学到多重含义的表示，我们想让一个字的向量包含这句话所有字的一个相关程度（后面还会说），那么首先初始化三个权重矩阵$W_Q、W_K、W_V$，然后将$X_{embedding}$与这三个权重矩阵相乘，得到$Q、K、V$也就是：$$\\begin{cases}Q=X_{embedding} W_Q \\ K=X_{embedding} W_K \\ V=X_{embedding} W_V\\end{cases}$$下面用图来理解更舒适！ 得到了$Q、K、V$之后 那么我们用$q\\times k$也就是对于一个字（中文是字，英文是词）它的score包含所有的自身$q$和别的字的$k$相乘,当然这里相乘肯定是和$k$的转置相乘哈！从而就可以得到一个注意力矩阵！(点积：两个向量越相似，点积则越大！)这里你会观察到，对角线上也就是每个字，自己对自己的相关程度，一行就是一个字中所有字与它的相关性。然后再对每一行做归一化（$softmax$），这样就保证对一个字来说，所有字与它的相关程度概率和为1！然后论文中又除了一个$\\sqrt{d_k}$,是为了把注意力矩阵变成标准正态分布，使得softmax归一化后的结果更加稳定，以便于反向传播时候获取平衡的梯度，最后将注意力矩阵给$V$加权，为啥要给$V$加权，其实就是因为注意力矩阵维度是$[batch \\ size, \\ sequence \\ length, \\ sequence \\ length]$，而$V$维度是$[batch \\ size ,\\ sequence \\ length, \\ embedding \\ dimension]$为了使得维度保持不变，则乘以$V$后为: $[batch \\ size ,\\ sequence \\ length, \\ embedding \\ dimension]$,从而再次和$X_{embedding}$的维度相同了，是不是很妙！$${Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$下图是论文中对$d_k$的解释： 源码分析：包含$Q，K$相乘，注意到相乘的时候有个转置操作，以及后面对$V$加权，和我们给出的公式其实是一致的，这里的mask语句后面会讲。 1234567891011121314151617181920212223242526272829303132333435def scaled_dot_product_attention(q, k, v, mask): \"\"\"计算注意力权重。 q, k, v 必须具有匹配的前置维度。 k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。 虽然 mask 根据其类型（填充或前瞻）有不同的形状， 但是 mask 必须能进行广播转换以便求和。 参数: q: 请求的形状 == (..., seq_len_q, depth) k: 主键的形状 == (..., seq_len_k, depth) v: 数值的形状 == (..., seq_len_v, depth_v) mask: Float 张量，其形状能转换成 (..., seq_len_q, seq_len_k)。默认为None。 返回值: 输出，注意力权重 \"\"\" matmul_qk = tf.matmul(q, k, transpose_b=True) # (..., seq_len_q, seq_len_k) # 缩放 matmul_qk dk = tf.cast(tf.shape(k)[-1], tf.float32) scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # 将 mask 加入到缩放的张量上。 if mask is not None: scaled_attention_logits += (mask * -1e9) # softmax 在最后一个轴（seq_len_k）上归一化，因此分数 （对注意力矩阵每一行归一化，则每个字的注意力向量一行，就是与其余字的相关程度，和为1） # 相加等于1。 attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # (..., seq_len_q, seq_len_k) 注意力矩阵 output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v) 再把注意力矩阵乘V return output, attention_weights 2.2 Multi-Head Attention&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 那么对于多头其实是一样的，只是在开始的时候将$embedding \\ dimension$分割成了$h$份（头的个数）,这里每个头权重都不同，多头训练效果理论上肯定更好（反正想着就是这样，至于为什么，也不好解释），最后再把它及联拼接。大佬的图展现的很好： 源码分析：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 self.depth = d_model // self.num_heads #本来QKV维度：【batch size，seq.length,embed dim】,拆分后就是【batch size，seq length，h，embed dim/h】 # depth 就是用embed dim 除头的个数 self.wq = tf.keras.layers.Dense(d_model) #初始化qkv矩阵 self.wk = tf.keras.layers.Dense(d_model) self.wv = tf.keras.layers.Dense(d_model) self.dense = tf.keras.layers.Dense(d_model) def split_heads(self, x, batch_size): \"\"\"拆分embedding dimension维度到 (num_heads, depth)， 这里的 depth=embed dim/h 转置结果使得形状为 (batch_size, num_heads, seq_len, depth) \"\"\" x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) return tf.transpose(x, perm=[0, 2, 1, 3]) def call(self, v, k, q, mask): batch_size = tf.shape(q)[0] q = self.wq(q) # (batch_size, seq_len, d_model) k = self.wk(k) # (batch_size, seq_len, d_model) v = self.wv(v) # (batch_size, seq_len, d_model) q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth) k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth) v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth) # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth) 之前没有分头的时候是：【batch size,seq #length,embed dim】 # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k) scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, mask) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth) 转置操作 concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model) 级联操作 看下图 output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model) return output, attention_weights 2.3 Mask有两个mask：$\\begin{cases}1.padding \\ mask \\\\2.lookahead mask（翻译任务中预测文本时(decoder部分))\\end{cases}$ 2.3.1 padding mask&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 当我们在确定$Max \\ length$时候，对于不够长的句子肯定要做$padding$但是对于为0的那一部分在$softmax$时候会变为1：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 回顾$softmax$函数: $$\\sigma (\\mathbf {z} )_{i}= \\frac {e^{z_i} } {\\sum _{j=1} ^ {K} e^ {z_j} }$$ $$ $$ $e^0$是1, 是有值的, 这样的话 $softmax$ 中被 $padding$ 的部分就参与了运算, 就等于是让无效的部分参与了运算,这样肯定不对, 这时就需要做一个$mask$让这些无效区域不参与运算, 我们一般给无效区域加一个很大的负数的偏置, 也就是: $$z_{illegal}=z_{illegal}+bias_{illegal}$$$$bias_{illegal}\\to-\\infty$$$$e^{z_{illegal}}\\to0$$ 源码分析：123456def create_padding_mask(seq): seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # 添加额外的维度来将填充加到 # 注意力对数（logits）。 return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len) 输出效果(把原本为0的地方变成了1)： 12x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])create_padding_mask(x) 最后再在一句代码中体现，如果mask不是None，则在此处乘以负无穷： 1234567891011121314151617181920212223242526272829303132333435def scaled_dot_product_attention(q, k, v, mask): \"\"\"计算注意力权重。 q, k, v 必须具有匹配的前置维度。 k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。 虽然 mask 根据其类型（填充或前瞻）有不同的形状， 但是 mask 必须能进行广播转换以便求和。 参数: q: 请求的形状 == (..., seq_len_q, depth) k: 主键的形状 == (..., seq_len_k, depth) v: 数值的形状 == (..., seq_len_v, depth_v) mask: Float 张量，其形状能转换成 (..., seq_len_q, seq_len_k)。默认为None。 返回值: 输出，注意力权重 \"\"\" matmul_qk = tf.matmul(q, k, transpose_b=True) # (..., seq_len_q, seq_len_k) # 缩放 matmul_qk dk = tf.cast(tf.shape(k)[-1], tf.float32) scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # 将 mask 加入到缩放的张量上。 if mask is not None: scaled_attention_logits += (mask * -1e9) # softmax 在最后一个轴（seq_len_k）上归一化，因此分数 （对注意力矩阵每一行归一化，则每个字的注意力向量一行，就是与其余字的相关程度，和为1） # 相加等于1。 attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # (..., seq_len_q, seq_len_k) 注意力矩阵 output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v) 再把注意力矩阵乘V return output, attention_weights 2.3.2 Lookahead mask&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这里我也不知道咋翻译（前瞻遮罩？）这个mask操作就是在翻译的时候，要预测第三个词，将仅使用第一个和第二个词，与此类似，预测第四个词，仅使用第一个，第二个和第三个词，依此类推。 源码分析：123def create_look_ahead_mask(size): mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) return mask # (seq_len, seq_len) 输出结果： (每一行是一个时刻，第一个时刻，遮盖了后两个（遮盖操作后也就是变为了1）)，用第一个字预测第二个字；第二个时刻，遮盖了第三个字，用第1、2个字预测第三个字；第三个时刻，则是用前三个字去预测结束符……当然这里其实每次预测的时候解码器还加上了编码器输出的embedding向量，这一点后面会详细说！ 1234x = tf.random.uniform((1, 3))temp = create_look_ahead_mask(x.shape[1])print(x)temp 3.Add&amp;Norm3.1残差连接归纳：模型太深，需要避免梯度消失&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我们在上一步得到了经过注意力矩阵加权之后的$V$, 也就是$Attention(Q, \\ K, \\ V)$, 我们对它进行一下转置, 使其和$X_{embedding}$的维度一致, 也就是$[batch \\ size, \\ sequence \\ length, \\ embedding \\ dimension]$, 然后把他们加起来做残差连接, 直接进行元素相加, 因为他们的维度一致:$$X_{embedding} + Attention(Q, \\ K, \\ V)$$在之后的运算里, 每经过一个模块的运算, 都要把运算之前的值和运算之后的值相加, 从而得到残差连接, 训练的时候可以使梯度直接走捷径反传到最初始层:$$X + SubLayer(X) $$ 3.2 $LayerNorm$:归纳：加速收敛&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Layer Normalization$的作用是把神经网络中隐藏层归一为标准正态分布, 也就是$i.i.d$独立同分布, 以起到加快训练速度, 加速收敛的作用:$$ \\mu_i=\\frac {1} {m}\\sum^{m} _ { i=1 } x _ {ij} $$上式中以矩阵的行$(row)$为单位求均值;$$\\sigma^{2} _ { j } =\\frac { 1 } { m } \\sum^ { m } _ { i=1 } (x _ { ij } -\\mu_ { j } )^ { 2 } $$上式中以矩阵的行$(row)$为单位求方差;$$LayerNorm(x)=\\alpha \\odot \\frac{x_{ij}-\\mu_{i}}{\\sqrt{\\sigma^{2}_{i}+\\epsilon}} + \\beta \\tag{eq.6}$$然后用每一行的每一个元素减去这行的均值, 再除以这行的标准差, 从而得到归一化后的数值, $\\epsilon$是为了防止除$0$;之后引入两个可训练参数$\\alpha, \\ \\beta$来弥补归一化的过程中损失掉的信息, 注意$\\odot$表示元素相乘而不是点积, 我们一般初始化$\\alpha$为全$1$, 而$\\beta$为全$0$. 源码分析：在源码中就是一个加号代表了一切！out1 = self.layernorm1(x + attn_output) 123456789101112131415161718192021222324class EncoderLayer(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, dff, rate=0.1): super(EncoderLayer, self).__init__() self.mha = MultiHeadAttention(d_model, num_heads) self.ffn = point_wise_feed_forward_network(d_model, dff) # 前向传播 self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.dropout1 = tf.keras.layers.Dropout(rate) self.dropout2 = tf.keras.layers.Dropout(rate) def call(self, x, training, mask): attn_output, _ = self.mha(x, x, x, mask) # (batch_size, input_seq_len, d_model) 三个x代表上一层的输出，N个encoder是串联的 attn_output = self.dropout1(attn_output, training=training) out1 = self.layernorm1(x + attn_output) # (batch_size, input_seq_len, d_model) 残差连接 ffn_output = self.ffn(out1) # (batch_size, input_seq_len, d_model) ffn_output = self.dropout2(ffn_output, training=training) out2 = self.layernorm2(out1 + ffn_output) # (batch_size, input_seq_len, d_model) return out2 4.FeedForward&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前向传播层，就是一个全连接，dff设置了内部全连接层数，和以往的没什么区别，不多说了。 源码分析：12345def point_wise_feed_forward_network(d_model, dff): # dff内部层维数 return tf.keras.Sequential([ tf.keras.layers.Dense(dff, activation='relu'), # (batch_size, seq_len, dff) tf.keras.layers.Dense(d_model) # (batch_size, seq_len, d_model) ]) Transformer encoder整体结构1). 字向量与位置编码:$$X = EmbeddingLookup(X) + PositionalEncoding $$$$X \\in \\mathbb{R}^{batch \\ size \\ * \\ seq. \\ len. \\ * \\ embed. \\ dim.} $$2). 自注意力机制:$$Q = Linear(X) = XW_{Q}$$$$K = Linear(X) = XW_{K} $$$$V = Linear(X) = XW_{V}$$$$X_{attention} = SelfAttention(Q, \\ K, \\ V) $$3). 残差连接与$Layer \\ Normalization$$$X_{attention} = X + X_{attention} $$$$X_{attention} = LayerNorm(X_{attention}) $$4). $FeedForward$, 其实就是两层线性映射并用激活函数激活, 比如说$ReLU$:$$X_{hidden} = Activate(Linear(Linear(X_{attention})))$$5). 重复3).:$$X_{hidden} = X_{attention} + X_{hidden}$$$$X_{hidden} = LayerNorm(X_{hidden})$$$$X_{hidden} \\in \\mathbb{R}^{batch \\ size \\ * \\ seq. \\ len. \\ * \\ embed. \\ dim.} $$ Transformer decoder部分 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 还是这个图最直观，注意观察编码器和解码器的差异，最下面的一块其实差不多，只是解码器加了一个Mask，这个mask当然是Lookahead mask，因为翻译任务里面，我们是在解码器中输入一个词，解码器拿着编码器最终隐藏层输出的向量来预测下一个词，所以需要去遮盖后面的词： **解码器预测过程：** 第1时刻——输入'I，解码器拿着编码器输出的embedding向量去预测'am'。 第2时刻——输入'am'，解码器拿着embedding向量 + 'I' 去预测 'a' 第3时刻——输入'a',编码器拿着embedding向量 + 'I' + 'a' 去预测 'student'... 源码分析：123456789101112131415161718192021222324252627282930313233343536class DecoderLayer(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, dff, rate=0.1): super(DecoderLayer, self).__init__() self.mha1 = MultiHeadAttention(d_model, num_heads) #decoder中有两个MultiHeadAttention，最下面一个有Lookahead mask，上面一个有padding mask self.mha2 = MultiHeadAttention(d_model, num_heads) self.ffn = point_wise_feed_forward_network(d_model, dff) self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.dropout1 = tf.keras.layers.Dropout(rate) self.dropout2 = tf.keras.layers.Dropout(rate) self.dropout3 = tf.keras.layers.Dropout(rate) def call(self, x, enc_output, training, look_ahead_mask, padding_mask): # enc_output.shape == (batch_size, input_seq_len, d_model) attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, d_model)下面一个，这里的输入和encoder一样 也是三个x attn1 = self.dropout1(attn1, training=training) out1 = self.layernorm1(attn1 + x) attn2, attn_weights_block2 = self.mha2( enc_output, enc_output, out1, padding_mask) # (batch_size, target_seq_len, d_model) 上面一个，这里的输入不同，要注意：是两个encoder输出和一个decoder输出；但是维数都是一样的 attn2 = self.dropout2(attn2, training=training) out2 = self.layernorm2(attn2 + out1) # (batch_size, target_seq_len, d_model) ffn_output = self.ffn(out2) # (batch_size, target_seq_len, d_model) ffn_output = self.dropout3(ffn_output, training=training) out3 = self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, d_model) return out3, attn_weights_block1, attn_weights_block2 # 再有N个decoder串联 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这里的self.mha1和self.mha2就是图中解码器层定义的两个Multi-Head Attention，这里上面的Multi-Head Attention也就是self.mha2，它是只做了padding mask的，这个和编码器的一致，但是下面的这个Multi-Head Attention（self.mha1）就不一样了，它的mask自然是Lookahead mask，用于遮盖后面的词，现在基本上前后就可以串起来了！注意看两个的输入： x, x, look_ahead_mask)```1&#96;&#96;&#96;self.mha2(enc_output, enc_output, out1, padding_mask) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于下面的Multi-Head Attentionself.mha1，它和编码器层那里的代码一致，都是接收三个相同的x（也就是q、k、v）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 但是对于上面的Multi-Head Attentionself.mha2，它的输入是不同的，它是用的编码器的输出和解码器下面的Multi-Head Attentionself.mha1的输出out1来共同输出out3，之前不理解为什么编码器那里要写三个x，写一个不也可以吗？反正都是一样，现在明白了，是为了和解码器这里的输入做到格式一致！！！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 编码器层、解码器层都理解完了，最后编码器串联N个，解码器串联N个就OK啦！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一个图说明了一切： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 之前不理解为什么画了这么多的线，一根线不行吗？还真不行，因为每次的解码器在预测的时候需要拿编码器输出的embedding向量呀！！ 源码分析：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 编码器和解码器无非就是做了N个编码器层和解码器层，然后这里的training代表的是否训练，因为训练的时候和预测的时候不一样。编码器： 123456789101112131415161718192021222324252627282930313233class Encoder(tf.keras.layers.Layer): def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1): super(Encoder, self).__init__() self.d_model = d_model self.num_layers = num_layers self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model) self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] self.dropout = tf.keras.layers.Dropout(rate) def call(self, x, training, mask): seq_len = tf.shape(x)[1] # 将嵌入和位置编码相加。 x = self.embedding(x) # (batch_size, input_seq_len, d_model) x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) x += self.pos_encoding[:, :seq_len, :] x = self.dropout(x, training=training) for i in range(self.num_layers): x = self.enc_layers[i](x, training, mask) #上一层的输出是下一层的输入 体现在这里 return x # (batch_size, input_seq_len, d_model) 解码器： 123456789101112131415161718192021222324252627282930313233343536class Decoder(tf.keras.layers.Layer): def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1): super(Decoder, self).__init__() self.d_model = d_model self.num_layers = num_layers self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model) self.pos_encoding = positional_encoding(maximum_position_encoding, d_model) self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] self.dropout = tf.keras.layers.Dropout(rate) def call(self, x, enc_output, training, look_ahead_mask, padding_mask): seq_len = tf.shape(x)[1] attention_weights = &#123;&#125; x = self.embedding(x) # (batch_size, target_seq_len, d_model) x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) x += self.pos_encoding[:, :seq_len, :] x = self.dropout(x, training=training) for i in range(self.num_layers): x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask) attention_weights['decoder_layer&#123;&#125;_block1'.format(i+1)] = block1 attention_weights['decoder_layer&#123;&#125;_block2'.format(i+1)] = block2 # x.shape == (batch_size, target_seq_len, d_model) return x, attention_weights","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Transformer","slug":"深度学习/Transformer","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://ericzikun.github.io/tags/Transformer/"},{"name":"源码","slug":"源码","permalink":"https://ericzikun.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://ericzikun.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}]},{"title":"终端命令及Colab白嫖必备","slug":"终端命令及Colab白嫖必备","date":"2020-04-10T15:24:35.000Z","updated":"2020-04-11T01:22:25.751Z","comments":true,"path":"2020/04/10/终端命令及Colab白嫖必备/","link":"","permalink":"https://ericzikun.github.io/2020/04/10/%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4%E5%8F%8AColab%E7%99%BD%E5%AB%96%E5%BF%85%E5%A4%87/","excerpt":"","text":"Linux 常用命令下载、解压命令（colab常用）：12wget + urlunzip + filename 基本命令：123456789101112131415161718ls----------------------显示当前文件夹所有文件ls -l ------------------显示当前文件夹所有文件及文件夹的详细信息(包含隐藏文件，就是文件名前带点的文件)pwd --------------------显示当前的pathcd + path --------------表示到某一路径下cd …/ ------------------表示切换到上一层路径。sudo su ----------------改为root权限mkdir /data/path -------新增文件夹vim path ---------------查看某一文件：wq--------------------保存退出（查看或者修改某文件后）rm -rf path ------------删除pathrm test.txt ------------删除test.txtcontrol + c ------------退出当前对话cp &lt;文件&gt;&lt;目标文件&gt;或者----cp是copy 的缩写。用于复制文件或文件夹cp -r&lt;文件夹&gt;&lt;目标文件夹&gt;--cp是copy 的缩写。用于复制文件或文件夹touch test.txt----------创建一个文本文件用树结构查看文件夹先输入brew install tree 安装一个软件tree命令格式：tree 文件夹 树结构图： Colab 命令：初始化：（每次都需要）1234567891011#使用GPU代码（必须）!pip install tensorflow-gpu==1.15#改变工作文件夹----一定要到谷歌云盘中（必须，否则数据会丢失）import ospath = \"/content/drive/My Drive\"os.chdir(path)os.listdir(path)#看一下GPU情况!nvidia-smi Colab不掉线：在Colab网页，右键检查，控制台console中输入一下代码回车，能够时不时点击页面，保证Colab不掉线 12345678910function ClickConnect()&#123; console.log(\"Working\"); document .querySelector(\"#top-toolbar &gt; colab-connect-button\") .shadowRoot .querySelector(\"#connect\") .click()&#125; setInterval(ClickConnect,60000)","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"Linux","slug":"技巧/Linux","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/Linux/"}],"tags":[{"name":"效率","slug":"效率","permalink":"https://ericzikun.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"Mac","slug":"Mac","permalink":"https://ericzikun.github.io/tags/Mac/"},{"name":"Linux","slug":"Linux","permalink":"https://ericzikun.github.io/tags/Linux/"},{"name":"终端","slug":"终端","permalink":"https://ericzikun.github.io/tags/%E7%BB%88%E7%AB%AF/"}]},{"title":"保研经验总结","slug":"保研经验总结","date":"2020-04-09T12:40:45.000Z","updated":"2020-07-09T14:22:37.576Z","comments":true,"path":"2020/04/09/保研经验总结/","link":"","permalink":"https://ericzikun.github.io/2020/04/09/%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/","excerpt":"","text":"保研经验分享 个人背景：​ 本人武汉理工大学航海技术专业，专业绩点排名：3/230,保研综合排名：1/230，四级：545，六级：496，一作EI会议论文一篇，国家大学生创新项目负责人，武汉理工大学自主创新项目负责人，获得多年奖学金、华中赛等小奖。 参加面试的学校：上交海洋，浙大海洋，武大水利，武大信管 最终去向：武大信管情报学 保研前期准备：​ 面对2019年暑期的夏令营，早在寒假就开始搜集大量学校的信息，当时还不知道有类似保研通、保研岛这种保研信息平台，走了些弯路。寒假我做了几件事： 1.简单梳理报名时间点：​ 将所要报夏令营学校的历年时间点大致梳理，比如像金融专业，有的学校的夏令营早在3、4月份就开始报名了，所以一定提前了解报名时间段，错过时间段这种低级错误千万不能犯，身边也确实有同学因为拖到最后一天报名，网崩了没报上的！ ​ 建议用表格排列好相关学校，因为夏令营竞争相对比较激烈，各路大神都出来秀肌肉，对于本科学校一般、排名一般、科研寥寥无几的同学可能会比较被动，学校如果报高了，会有被刷的风险，在不确定自己会不会被刷的情况下，一定得多报一些学校，同一档次的学校报几个，不要瞧不起某些学校，最后说不定就是你的保底学校要了你，每年情况都不一样，结果瞬息万变，不要一味的相信自己专业学长学姐的经验，我们宿舍当时都过分依赖去年学长的经验，下意识认为今年武大水利也会要三四个我们专业的，结果只要了俩，一切以稳为前提！！ 心仪学校 学院网址 时间点 备注 武汉大学 xxx 6月 xxx 上海交大 xxx 7月 xxx 2.完成一份得体的简历、个人陈述：​ 一份得体的简历在面试过程中尤为重要，寒假时间较多，可以抽时间做好一份‘完美’的简历，我印象中，面试的老师有对我简历有表示满意的，有位老师事后基本上就看我的简历跟我对话，面试过程中也不会去详细的看你的材料。 ​ 简历不需要太浮夸，过于注重美观也没必要，但是一定要简洁、调理清晰！ 当时我觉得自己找的那种什么模板大礼包之类的好丑好丑，幸亏女友给我了一个她用的，清新好看又五脏俱全！页数最多不超过两页，期初我做了两页，后来删了一页，感觉一页更加精华和简约，把一页纸尽量塞满你的大学成就，低含金量的就不要放上去了，比如一个院唱歌、拔河比赛之类的，尽量放和学习相关的内容，我的简历大致分为以下几个板块： 简历中几个注意的点： 登记照一定去好一点的照相馆照，一般也就不到100块吧，但是可以把你修的还阔以的！ 基本信息中的邮箱地址，一定要用学校邮箱，这样显得更加正式，千万别用qq邮箱，后缀显得没有识别度，老师喜欢用xxx.edu.cn来识别你的学校，起码不会把你当成垃圾邮件忽略！ 本科如果做了科研，可以在基本信息中加上本科研究方向，尽可能与你申请的学校相近！如果差别太大就别写了。 软件掌握情况，可以简要写几个硬实力一点的，office能不写就不写，论文排版方面，如果做过数学建模的同学，应该知道latex，有的老师面试时候特意问是不是用latex排的版，可能老师比较喜欢用吧，会用的可以写上去！编程等工科的软件就不用说了，会的越多越好！ 论文、软著、专利这些学术成果尽量用引用的格式来写，这样一目了然，论文中把自己的名字突出即可！（仿照老师网上的论文展示） 总结一下：整份简历要尽量匹配你所报的学校方向和老师，突出对方想看到的信息（可以加红），所以不同学校简历侧重点一定有所不同，需要小改动，建议后期直接用pdf编辑器完成，对于小改动效率比较高！ ​ 个人陈述需要根据每个学校的要求来写，字数要求不一，通过查阅去年该学校学院的通知来获知，额，我的文采很差很差，很感谢我的女友，个人陈述写了初稿之后给了女友和阿姨帮忙修改，反复修改之后，一份满意的个人陈述就大功告成了，前期我也看过一些网上的，但是写的太差太差了，可能真正用心写的别人也不会放在网上吧，所以不要期望于把别人的拿过来改改就行，每个人的经历都是独一无二的，别人不能替代你的想法！自己用心好好写，好好改就好了！ 3.提前准备好相关证明材料：​ 由于大三下学期会超级无敌忙，建议在寒假准备好相关的证明材料，把奖状、证书、前五个学期的排名、成绩单都准备好、开学直接去学院盖章就行了，不要拖到最后！！避免扎堆，每份证明做好pdf电子版本，后期每个学校都需要打印很多材料！ 4.主攻笔试、面试：​ 通过了层层筛选，很荣幸你通过了学校初审，剩下的重中之重当当然是笔试和面试，但是如果真正等到暑假再准备，对于跨专业的同学来说就不太够了，如果准备跨专业，寒假就可以开始准备相关课程的学习了，同时一定得找到去年去了该校夏令营的学长学姐问清楚笔试面试的题目，这个很重要！我当时就是有学长给了我英语面试的题库，面试过程还算比较满意，只要做足了准备，就不会怕！ ​ 注意了！！有论文的同学，一定把自己写的论文弄熟练，虽然是自己写的，时间长了难免会忘记一些细节，有论文在面试过程中很占优势，去了几个学校能感觉到本科手握一两篇论文的相对比较少（顶尖学校顶尖专业除外）所以一定要把这个优势发挥好，能够和面试老师对答如流，其实有些老师真的不一定对你的研究很感兴趣，就是问问你的思路啊，其中某个模型的优劣？或者让你总体概括一下你的论文，武大水利曾有一位老师就是问我”你来几句话介绍一下你的论文，考察一下你的概括能力“ 夏令营和预推免的区别​ 夏令营的竞争压力要远大于九月的预推免，因为夏令营很多都是一个大佬报了N个学校，最后隔了N-1的，很多学校面临被割的风险，所以在九月预推免中还会在招一波，实质上两者是一样的，只是考核方式大多学校预推免简单一点。 ​ 不要以为夏令营他没要你，预推免就一定不会要你，很多时候学校他也怕被割，预推免需要招一波学生，又比如我们宿舍，夏令营时候看见北航交通A+学科很难进，结果九月我们宿舍不敢报，保研边缘的同学报了被顺利录取，这种案例还真不少，认得的双非的同学最后去的学校都很好，关键在于你敢不敢报！！很多时候不是学校不要你，是你压根没给他机会选你啊！ ​ 综上，对自己有个相对客观的定位后，尽可能多报一些学校，千万不要只报两三个学校，虽然报名过程很麻烦，但是挺一挺就过去了，身边当时嫌麻烦没报名的同学最后基本上都后悔了，保研就这一次，在有限时间内做出自己最优的决策，去到自己最理想的学校，学自己喜欢的专业，尽量不要给自己留下遗憾！","categories":[{"name":"升学就业","slug":"升学就业","permalink":"https://ericzikun.github.io/categories/%E5%8D%87%E5%AD%A6%E5%B0%B1%E4%B8%9A/"},{"name":"保研","slug":"升学就业/保研","permalink":"https://ericzikun.github.io/categories/%E5%8D%87%E5%AD%A6%E5%B0%B1%E4%B8%9A/%E4%BF%9D%E7%A0%94/"}],"tags":[{"name":"保研","slug":"保研","permalink":"https://ericzikun.github.io/tags/%E4%BF%9D%E7%A0%94/"},{"name":"经验","slug":"经验","permalink":"https://ericzikun.github.io/tags/%E7%BB%8F%E9%AA%8C/"},{"name":"升学，读研","slug":"升学，读研","permalink":"https://ericzikun.github.io/tags/%E5%8D%87%E5%AD%A6%EF%BC%8C%E8%AF%BB%E7%A0%94/"}]},{"title":"Mac安装gensim踩坑","slug":"Mac安装gensim踩坑","date":"2020-04-08T02:22:27.000Z","updated":"2020-04-08T11:28:24.788Z","comments":true,"path":"2020/04/08/Mac安装gensim踩坑/","link":"","permalink":"https://ericzikun.github.io/2020/04/08/Mac%E5%AE%89%E8%A3%85gensim%E8%B8%A9%E5%9D%91/","excerpt":"","text":"mac环境pip install gensim报错用pip安装gensim包时，显示：ERROR: Exception:Traceback (most recent call last): 用了多种方式未果，采用镜像安装成功！ 1pip install gensim -i https://pypi.doubanio.com/simple","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"报错","slug":"技巧/报错","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%8A%A5%E9%94%99/"}],"tags":[{"name":"报错","slug":"报错","permalink":"https://ericzikun.github.io/tags/%E6%8A%A5%E9%94%99/"}]},{"title":"mac安装cnpm踩坑","slug":"mac安装cnpm踩坑","date":"2020-04-08T02:21:50.000Z","updated":"2020-04-08T11:28:28.849Z","comments":true,"path":"2020/04/08/mac安装cnpm踩坑/","link":"","permalink":"https://ericzikun.github.io/2020/04/08/mac%E5%AE%89%E8%A3%85cnpm%E8%B8%A9%E5%9D%91/","excerpt":"","text":"mac安装cnpm报错错误如图：报错原因：npm WARN checkPermissions Missing write access to /usr/local/lib/node_modules此报错原因是由于没有权限，加上sudo即可解决！sudo npm install -g cnpm –registry=https://registry.npm.taobao.org大功告成：","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"报错","slug":"技巧/报错","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%8A%A5%E9%94%99/"}],"tags":[{"name":"报错","slug":"报错","permalink":"https://ericzikun.github.io/tags/%E6%8A%A5%E9%94%99/"}]},{"title":"keras模型可视化利器","slug":"keras模型可视化利器","date":"2020-04-08T02:20:20.000Z","updated":"2020-05-16T02:15:32.181Z","comments":true,"path":"2020/04/08/keras模型可视化利器/","link":"","permalink":"https://ericzikun.github.io/2020/04/08/keras%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%A9%E5%99%A8/","excerpt":"","text":"安装graphviz使用Mac的brew安装即可，命令行： 1brew install graphviz 查看graphviz安装到的路径1brew list graphviz 出现下图： 添加环境变量到路径12import osos.environ[\"PATH\"] += os.pathsep + '/usr/local/Cellar/graphviz' 运行代码运行绘制模型plot_model代码之前，预先定义好一个model，例如： 12345678910111213141516171819from keras.models import Sequentialfrom tensorflow import * from keras.layers.embeddings import Embedding from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, Input, Lambda,BatchNormalization from keras.models import Modelmodel = Sequential()model.add(Embedding(100001, 300, input_length=50)) #使用Embeeding层将每个词编码转换为词向量model.add(Conv1D(256, 5, padding='same'))model.add(MaxPooling1D(3, 3, padding='same'))model.add(Conv1D(128, 5, padding='same'))model.add(MaxPooling1D(3, 3, padding='same'))model.add(Conv1D(64, 3, padding='same'))model.add(Flatten())model.add(Dropout(0.1))model.add(BatchNormalization()) # (批)规范化层model.add(Dense(256, activation='relu'))model.add(Dropout(0.1))model.add(Dense(10, activation='softmax')) 绘制模型图 123from keras.utils import plot_modelimport pydotplot_model(model,to_file='CNNmodel.png',show_shapes=True,show_layer_names=False) 此处再补充一个: model.summary函数，可以也可以输出图形结构: 1model.summary()","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"模型可视化","slug":"深度学习/模型可视化","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Keras","slug":"Keras","permalink":"https://ericzikun.github.io/tags/Keras/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://ericzikun.github.io/tags/Tensorflow/"}]},{"title":"文本处理中常见的Str/list转换","slug":"文本处理中常见的Str-list转换","date":"2020-04-08T02:19:44.000Z","updated":"2020-04-08T11:27:14.594Z","comments":true,"path":"2020/04/08/文本处理中常见的Str-list转换/","link":"","permalink":"https://ericzikun.github.io/2020/04/08/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84Str-list%E8%BD%AC%E6%8D%A2/","excerpt":"","text":"归纳在文本处理的nlp领域，经常需要将大量文本格式进行不断的转换进而达到模型输入的需求，每次转换我总在尝试，也觉得很费时间，但其实每次用到的函数大同小异，不希望经常做重复的工作，遂总结一些常见类型转换，方便以后随时调用。常用的函数: 123456789split: str.split(str&#x3D;&quot;&quot;, num&#x3D;string.count(str)) split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串replace: replace(rgExp, replaceText, max)：可以替换任意指定的字符join: &quot;str&quot;.join(),连接字符串数组。将字符串、元组、列表中的元素以指定的字符(分隔符)连接生成一个新的字符串strip(str)： 可以去除头尾指定字符，参数为空时，默认去除字符串中头尾的空格字符（常用来去掉读取txt后的换行符） 1.形式1：脱去一层list) 1234all_words2 = []for sentence in all_words: all_words2.append(\"\".join(sentence))print(all_words2) 2.形式2：将每个list里面的字符串合并成一个字符串（以适用onehot、tfidf向量的输入） 123456789101112#将list of list转换为list 以适合CountVectorizer函数的格式all_data_str = []for i in range(len(all_data)): sentence= '' for j in range(len(all_data[i])): word = all_data[i][j] if j&gt;0: sentence = sentence+' '+word else: sentence = sentence + word all_data_str.append(sentence)print(all_data_str[:2]) 3.复杂的形式：保存pd.DataFrame后，再读取有时候会出现第一步转换： 1234567891011B = A[0:5]all_words = []sentence = \"\"words=[]for i in range(len(B)): sentence = B[i].strip(\"[]\").replace(\"\\'\",\"\").replace(\",\",\"\").split(\"\\n\") cur_words = [] for word in sentence: cur_words.append(word) all_words.append(cur_words)print(all_words) 第二步转换： 1234all_words2 = []for sentence in all_words: all_words2.append(\"\".join(sentence))print(all_words2)","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"文本处理","slug":"技巧/文本处理","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"}],"tags":[{"name":"文本处理","slug":"文本处理","permalink":"https://ericzikun.github.io/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"},{"name":"预处理","slug":"预处理","permalink":"https://ericzikun.github.io/tags/%E9%A2%84%E5%A4%84%E7%90%86/"},{"name":"格式化文本","slug":"格式化文本","permalink":"https://ericzikun.github.io/tags/%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%96%87%E6%9C%AC/"}]},{"title":"朴素贝叶斯理论","slug":"朴素贝叶斯理论","date":"2020-04-08T02:16:54.000Z","updated":"2020-04-09T07:14:54.296Z","comments":true,"path":"2020/04/08/朴素贝叶斯理论/","link":"","permalink":"https://ericzikun.github.io/2020/04/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/","excerpt":"","text":"（三）朴素贝叶斯定义 ：朴素贝叶斯是基于贝叶斯定理和特征条件独立假设的分类方法。首先学习输入/输出的联合概率分布,然后基于此模型,对给定的输入$x$,利用贝叶斯定理求出后验概率最大的输出$y$。 模型:首先学习先验概率分布：$P(Y=c_k),k=1,2,…,K$ , $c_k$代表某一类，也就是计算该类别的概率（在样本中我们已知）然后学习条件概率分布：$P(X=x|Y=c_k)=P(X^{1}=x^{1},…,X^{n}=x^{n}|Y=c_k)$，给定一个类别$c_k$，计算该样本各个特征的概率,比如该样本第一个特征为朴素贝叶斯法对条件概率分布作了条件独立性的假设：$$P(X^{(1)}=x^{(1)}|Y=c_k)P(X^{(2)}=x^{(2)}|Y=c_k)…P(X^{(j)}=x^{(j)}|Y=c_k)$$上式变成:$$\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$在分类时,通过学习到的模型计算后验概率分布,由贝叶斯定理得到:$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}$$将条件独立性假设得到的等式代入,并且注意到分母都是相同的,所以得到朴素贝叶斯分类器:$$y=argmax_{c_k}P(Y=c_k)\\prod_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 算法:使用极大似然估计法估计相应的先验概率率:$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,…,K$$以及条件概率：$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^{N}I(y_{i}=c_k)}$$计算条件独立性假设下的实例各个取值的可能性,选取其中的最大值作为输出。 使用贝叶斯估计虽然保证了所有连乘项的概率都大于0，不会再出现某一项为0结果为0的情况。但若一个样本数据时高维的，比如说100维（100其实并不高），连乘项都是0-1之间的，那100个0-1之间的数相乘，最后的数一定是非常非常小了，可能无限接近于0。对于程序而言过于接近0的数可能会造成下溢出，也就是精度不够表达了。所以我们会给整个连乘项取对数，这样哪怕所有连乘最后结果无限接近0，那取完log以后数也会变得很大（虽然是负的很大），计算机就可以表示了。同样，多项连乘取对数，对数的连乘可以表示成对数的相加，在计算上也简便了。所以在实际运用中，不光需要使用贝叶斯估计（保证概率不为0），同时也要取对数（保证连乘结果不下溢出）。 代码：参考代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196# coding=utf-8# Author:Dodo# Date:2018-11-17# Email:lvtengchao@pku.edu.cn'''数据集：Mnist训练集数量：60000测试集数量：10000------------------------------运行结果： 正确率：84.3% 运行时长：103s'''import numpy as npimport timedef loadData(fileName): ''' 加载文件 :param fileName:要加载的文件路径 :return: 数据集和标签集 ''' #存放数据及标记 dataArr = []; labelArr = [] #读取文件 fr = open(fileName) #遍历文件中的每一行 for line in fr.readlines(): #获取当前行，并按“，”切割成字段放入列表中 #strip：去掉每行字符串首尾指定的字符（默认空格或换行符） #split：按照指定的字符将字符串切割成每个字段，返回列表形式 curLine = line.strip().split(',') #将每行中除标记外的数据放入数据集中（curLine[0]为标记信息） #在放入的同时将原先字符串形式的数据转换为整型 #此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算 dataArr.append([int(int(num) &gt; 128) for num in curLine[1:]]) #将标记信息放入标记集中 #放入的同时将标记转换为整型 labelArr.append(int(curLine[0])) #返回数据集和标记 return dataArr, labelArrdef NaiveBayes(Py, Px_y, x): ''' 通过朴素贝叶斯进行概率估计 :param Py: 先验概率分布 :param Px_y: 条件概率分布 :param x: 要估计的样本x :return: 返回所有label的估计概率 ''' #设置特征数目 featrueNum = 784 #设置类别数目 classNum = 10 #建立存放所有标记的估计概率数组 P = [0] * classNum #对于每一个类别，单独估计其概率 for i in range(classNum): #初始化sum为0，sum为求和项。 #在训练过程中对概率进行了log处理，所以这里原先应当是连乘所有概率，最后比较哪个概率最大 #但是当使用log处理时，连乘变成了累加，所以使用sum sum = 0 #获取每一个条件概率值，进行累加 for j in range(featrueNum): sum += Px_y[i][j][x[j]] #最后再和先验概率相加（也就是式4.7中的先验概率乘以后头那些东西，乘法因为log全变成了加法） P[i] = sum + Py[i] #max(P)：找到概率最大值 #P.index(max(P))：找到该概率最大值对应的所有（索引值和标签值相等） return P.index(max(P))def test(Py, Px_y, testDataArr, testLabelArr): ''' 对测试集进行测试 :param Py: 先验概率分布 :param Px_y: 条件概率分布 :param testDataArr: 测试集数据 :param testLabelArr: 测试集标记 :return: 准确率 ''' #错误值计数 errorCnt = 0 #循环遍历测试集中的每一个样本 for i in range(len(testDataArr)): #获取预测值 presict = NaiveBayes(Py, Px_y, testDataArr[i]) #与答案进行比较 if presict != testLabelArr[i]: #若错误 错误值计数加1 errorCnt += 1 #返回准确率 return 1 - (errorCnt / len(testDataArr))def getAllProbability(trainDataArr, trainLabelArr): ''' 通过训练集计算先验概率分布和条件概率分布 :param trainDataArr: 训练数据集 :param trainLabelArr: 训练标记集 :return: 先验概率分布和条件概率分布 ''' #设置样本特诊数目，数据集中手写图片为28*28，转换为向量是784维。 # （我们的数据集已经从图像转换成784维的形式了，CSV格式内就是） featureNum = 784 #设置类别数目，0-9共十个类别 classNum = 10 #初始化先验概率分布存放数组，后续计算得到的P(Y = 0)放在Py[0]中，以此类推 #数据长度为10行1列 Py = np.zeros((classNum, 1)) #对每个类别进行一次循环，分别计算它们的先验概率分布 #计算公式为书中\"4.2节 朴素贝叶斯法的参数估计 公式4.8\" for i in range(classNum): #下方式子拆开分析 #np.mat(trainLabelArr) == i：将标签转换为矩阵形式，里面的每一位与i比较，若相等，该位变为Ture，反之False #np.sum(np.mat(trainLabelArr) == i):计算上一步得到的矩阵中Ture的个数，进行求和(直观上就是找所有label中有多少个 #为i的标记，求得4.8式P（Y = Ck）中的分子) #np.sum(np.mat(trainLabelArr) == i)) + 1：参考“4.2.3节 贝叶斯估计”，例如若数据集总不存在y=1的标记，也就是说 #手写数据集中没有1这张图，那么如果不加1，由于没有y=1，所以分子就会变成0，那么在最后求后验概率时这一项就变成了0，再 #和条件概率乘，结果同样为0，不允许存在这种情况，所以分子加1，分母加上K（K为标签可取的值数量，这里有10个数，取值为10） #参考公式4.11 #(len(trainLabelArr) + 10)：标签集的总长度+10. #((np.sum(np.mat(trainLabelArr) == i)) + 1) / (len(trainLabelArr) + 10)：最后求得的先验概率 Py[i] = ((np.sum(np.mat(trainLabelArr) == i)) + 1) / (len(trainLabelArr) + 10) #转换为log对数形式 #log书中没有写到，但是实际中需要考虑到，原因是这样： #最后求后验概率估计的时候，形式是各项的相乘（“4.1 朴素贝叶斯法的学习” 式4.7），这里存在两个问题：1.某一项为0时，结果为0. #这个问题通过分子和分母加上一个相应的数可以排除，前面已经做好了处理。2.如果特诊特别多（例如在这里，需要连乘的项目有784个特征 #加一个先验概率分布一共795项相乘，所有数都是0-1之间，结果一定是一个很小的接近0的数。）理论上可以通过结果的大小值判断， 但在 #程序运行中很可能会向下溢出无法比较，因为值太小了。所以人为把值进行log处理。log在定义域内是一个递增函数，也就是说log（x）中， #x越大，log也就越大，单调性和原数据保持一致。所以加上log对结果没有影响。此外连乘项通过log以后，可以变成各项累加，简化了计算。 #在似然函数中通常会使用log的方式进行处理 Py = np.log(Py) #计算条件概率 Px_y=P（X=x|Y = y） #计算条件概率分成了两个步骤，下方第一个大for循环用于累加，参考书中“4.2.3 贝叶斯估计 式4.10”，下方第一个大for循环内部是 #用于计算式4.10的分子，至于分子的+1以及分母的计算在下方第二个大For内 #初始化为全0矩阵，用于存放所有情况下的条件概率 Px_y = np.zeros((classNum, featureNum, 2)) #对标记集进行遍历 for i in range(len(trainLabelArr)): #获取当前循环所使用的标记 label = trainLabelArr[i] #获取当前要处理的样本 x = trainDataArr[i] #对该样本的每一维特诊进行遍历 for j in range(featureNum): #在矩阵中对应位置加1 #这里还没有计算条件概率，先把所有数累加，全加完以后，在后续步骤中再求对应的条件概率 Px_y[label][j][x[j]] += 1 #第二个大for，计算式4.10的分母，以及分子和分母之间的除法 #循环每一个标记（共10个） for label in range(classNum): #循环每一个标记对应的每一个特征 for j in range(featureNum): #获取y=label，第j个特诊为0的个数 Px_y0 = Px_y[label][j][0] #获取y=label，第j个特诊为1的个数 Px_y1 = Px_y[label][j][1] #对式4.10的分子和分母进行相除，再除之前依据贝叶斯估计，分母需要加上2（为每个特征可取值个数） #分别计算对于y= label，x第j个特征为0和1的条件概率分布 Px_y[label][j][0] = np.log((Px_y0 + 1) / (Px_y0 + Px_y1 + 2)) Px_y[label][j][1] = np.log((Px_y1 + 1) / (Px_y0 + Px_y1 + 2)) #返回先验概率分布和条件概率分布 return Py, Px_yif __name__ == \"__main__\": start = time.time() # 获取训练集 print('start read transSet') trainDataArr, trainLabelArr = loadData('../Mnist/mnist_train.csv') # 获取测试集 print('start read testSet') testDataArr, testLabelArr = loadData('../Mnist/mnist_test.csv') #开始训练，学习先验概率分布和条件概率分布 print('start to train') Py, Px_y = getAllProbability(trainDataArr, trainLabelArr) #使用习得的先验概率分布和条件概率分布对测试集进行测试 print('start to test') accuracy = test(Py, Px_y, testDataArr, testLabelArr) #打印准确率 print('the accuracy is:', accuracy) #打印时间 print('time span:', time.time() -start)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"朴素贝叶斯","slug":"机器学习/朴素贝叶斯","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"}],"tags":[{"name":"统计学习方法","slug":"统计学习方法","permalink":"https://ericzikun.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"https://ericzikun.github.io/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"}]},{"title":"我的第一篇博客文章","slug":"Java","date":"2020-04-07T06:18:08.000Z","updated":"2020-04-08T11:27:18.258Z","comments":true,"path":"2020/04/07/Java/","link":"","permalink":"https://ericzikun.github.io/2020/04/07/Java/","excerpt":"","text":"","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"Java","slug":"技术栈/Java","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Java/"}],"tags":[]},{"title":"React","slug":"React","date":"2020-04-07T06:18:08.000Z","updated":"2020-04-08T11:28:09.624Z","comments":true,"path":"2020/04/07/React/","link":"","permalink":"https://ericzikun.github.io/2020/04/07/React/","excerpt":"","text":"","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"React","slug":"技术栈/React","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/React/"}],"tags":[]},{"title":"我的第一篇博客文章","slug":"python","date":"2020-04-07T06:18:08.000Z","updated":"2020-04-08T11:28:06.052Z","comments":true,"path":"2020/04/07/python/","link":"","permalink":"https://ericzikun.github.io/2020/04/07/python/","excerpt":"","text":"","categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"Python","slug":"技术栈/Python","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Python/"}],"tags":[]},{"title":"提升效率集锦","slug":"提升效率小技巧","date":"2020-02-08T02:24:52.000Z","updated":"2020-04-09T07:15:17.096Z","comments":true,"path":"2020/02/08/提升效率小技巧/","link":"","permalink":"https://ericzikun.github.io/2020/02/08/%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87%E5%B0%8F%E6%8A%80%E5%B7%A7/","excerpt":"","text":"Mac提升效率软件1.Alfred无论在mac上还是win上，在提升效率这一领域，永远是不断的追寻制高点，从win转mac也有将近半年之久了，谈起”如何提高效率“，无疑是我平时休闲之余最爱的关注点，例如像win上的everything：它将我的电脑中的所有文件存为字典以便秒速访问，类似于Mac上的Alfred，设定快捷方式：option+空格来唤醒，找文件：find+文件名；使用搜索功能可以用默认，也可以自定义：比如我自定义了：zh+内容（跳转知乎搜索）；db+内容（豆瓣搜索）…，大大提升了切换页面浏览器的效率！这还不是最强的，work flow功能可以嵌入自定义脚本，脚本可以去github上找，我最喜欢用的是：OCR功能，工作流：先截图（存到了剪切板）—》调出Alfred，输出OCR，将图片转化为文字返回到剪切板，速度大概在2秒左右：（教程直接百度有，但是其中要去百度OCR申请key，不麻烦）当然，还可以当做计算器，偶尔用一下： 2.Paste强推这个复制粘贴板，简单来说就是将近期复制粘贴的内容存在一块板上，可以访问历史，我平时用的非常多！command+shift+v调出剪切板 3.截图软件：Snipaste &amp; Xnip下载Xnip完全是因为它可以截长图；Snipaste我从win一直用到mac，用到的最多功能就是贴图：可以将任意截图贴在屏幕上、到剪切板上、保存图片。（码代码+写论文必备） 4.快捷键一览表cheatsheet一个command键调出任何软件全部的快捷键，不多说，喜欢快捷键的必备！ 5.思维导图Xmind Zen有很多高质量的模板，也适合做知识点的梳理！ 6.Mactex：论文排版必备就是win中的Latex，安装教程稍微有点复杂，主要在配环境上，我的另外一篇博客已经有详细教程传送门 7.Omnifocus：时间管理软件管理时间的重要性不多说，虽然我依然做的不够好，但是还是要规划好时间! 8.Chrome浏览器：重在插件！谷歌学术插件：消除广告插件：Adblock plus最好用的插件：暴力猴：支持js脚本扩展，很多脚本多特定网页做了优化!比如：豆瓣电影：相应脚本会在右边显示可供观看的资源再比如SCI-hub，去谷歌搜论文时，SCi-hub脚本插件可以识别出论文doi，直接跳转下载pdf：即使右侧没有pdf，也能通过doi，转到sci-hub网页下载，实现英文论文全覆盖Onetab插件：页面多了后，一键汇总，将全部网页显示在一个页面上The Great Suspender 插件：将长时间没观看的网页从内存中关掉，当你想看的时候再次唤醒即可，可以大大减小内存消耗！ 9.Mathphix：公式OCR必备不多说，知道就是赚到，不用手敲公式了，直接截图调入OCR识别出Latex格式的公式，如果想放进word：先粘贴到mathtype软件，再粘贴到word；如果用Latex，就直接粘贴到Latex即可！每个月限制50次，如果超了，再换个邮箱即可，谁还没几个邮箱啊！ 10.调用远端服务器必备：Royal Tsx &amp; FileZillaRoyal TSX类似于Xshell，不多说！ FileZilla传文件非常快，和自身网速相当，传文件到远端很方便！","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"提升效率","slug":"技巧/提升效率","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87/"}],"tags":[{"name":"效率","slug":"效率","permalink":"https://ericzikun.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"学术","slug":"学术","permalink":"https://ericzikun.github.io/tags/%E5%AD%A6%E6%9C%AF/"},{"name":"Mac","slug":"Mac","permalink":"https://ericzikun.github.io/tags/Mac/"}]},{"title":"利用TextCNN对IMDB做文本分类任务","slug":"利用TextCNN对Cnews做文本分类任务","date":"2020-02-01T13:46:12.000Z","updated":"2020-04-12T07:30:16.383Z","comments":true,"path":"2020/02/01/利用TextCNN对Cnews做文本分类任务/","link":"","permalink":"https://ericzikun.github.io/2020/02/01/%E5%88%A9%E7%94%A8TextCNN%E5%AF%B9Cnews%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"参考博客： imdb预处理 TextCNN模型 1.下载kaggle数据集,并进行文本预处理：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# 导入相应的包import pandas as pdimport warningsimport reimport matplotlib.pyplot as pltfrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsfrom keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesfrom keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, Bidirectionalfrom keras.models import Sequentialfrom sklearn.model_selection import train_test_splitwarnings.filterwarnings('ignore')# 读取数据df1 = pd.read_csv('word2vec-nlp-tutorial/labeledTrainData.tsv', sep='\\t', error_bad_lines=False)df2 = pd.read_csv('word2vec-nlp-tutorial/imdb_master.csv', encoding=\"latin-1\")df3 = pd.read_csv('word2vec-nlp-tutorial/testData.tsv', sep='\\t', error_bad_lines=False)df2 = df2.drop(['Unnamed: 0','type','file'],axis=1)df2.columns = [\"review\",\"sentiment\"]df2 = df2[df2.sentiment != 'unsup']df2['sentiment'] = df2['sentiment'].map(&#123;'pos': 1, 'neg': 0&#125;)# 合并数据df = pd.concat([df1, df2]).reset_index(drop=True)train_texts = df.reviewtrain_labels = df.sentimenttest_texts = df3.review# 英文缩写替换def replace_abbreviations(text): texts = [] for item in text: item = item.lower().replace(\"it's\", \"it is\").replace(\"i'm\", \"i am\").replace(\"he's\", \"he is\").replace(\"she's\", \"she is\")\\ .replace(\"we're\", \"we are\").replace(\"they're\", \"they are\").replace(\"you're\", \"you are\").replace(\"that's\", \"that is\")\\ .replace(\"this's\", \"this is\").replace(\"can't\", \"can not\").replace(\"don't\", \"do not\").replace(\"doesn't\", \"does not\")\\ .replace(\"we've\", \"we have\").replace(\"i've\", \" i have\").replace(\"isn't\", \"is not\").replace(\"won't\", \"will not\")\\ .replace(\"hasn't\", \"has not\").replace(\"wasn't\", \"was not\").replace(\"weren't\", \"were not\").replace(\"let's\", \"let us\")\\ .replace(\"didn't\", \"did not\").replace(\"hadn't\", \"had not\").replace(\"waht's\", \"what is\").replace(\"couldn't\", \"could not\")\\ .replace(\"you'll\", \"you will\").replace(\"you've\", \"you have\") item = item.replace(\"'s\", \"\") texts.append(item) return texts# 删除标点符号及其它字符def clear_review(text): texts = [] for item in text: item = item.replace(\"&lt;br /&gt;&lt;br /&gt;\", \"\") item = re.sub(\"[^a-zA-Z]\", \" \", item.lower()) texts.append(\" \".join(item.split())) return texts# ＃ 删除停用词 ＋ 词形还原def stemed_words(text): stop_words = stopwords.words(\"english\") lemma = WordNetLemmatizer() texts = [] for item in text: words = [lemma.lemmatize(w, pos='v') for w in item.split() if w not in stop_words] texts.append(\" \".join(words)) return texts # ＃ 文本预处理def preprocess(text): text = replace_abbreviations(text) text = clear_review(text) text = stemed_words(text) return text train_texts = preprocess(train_texts)test_texts = preprocess(test_texts) 2.token编码、padding操作、切分数据集也就是建立onehot向量，这里只取频率排行前6000个单词构建词典，令max_features = 6000; 然后将每个样本（句子）定为长度130（不够长补0，多余截断） 12345678910111213141516171819max_features = 6000texts = train_texts + test_texts# 转换为onehot向量，num_words：保留词频前max_features的词汇，其他词删去。仅num_words-1保留最常用的词。tok = Tokenizer(num_words=max_features)tok.fit_on_texts(texts)vocab = tok.word_index# 将文本按照词典编号的方式进行编码list_tok = tok.texts_to_sequences(texts)#对每个样本最大长度做限制，定为130，其余补0maxlen = 130seq_tok = pad_sequences(list_tok, maxlen=maxlen)x_train = seq_tok[:len(train_texts)] #只取到train_texts的样本y_train = train_labelsembed_size = 128 #此为通过embedding矩阵乘法，我们想让一个样本（句子）中每个单词压缩成的向量维度x_train,x_test,y_train,y_test = train_test_split(x_train,y_train) #切分训练、测试集 3.整理好数据后，采用TensorFlow中的CNN模型或者TextCNN模型进行训练 123456789101112131415161718192021222324252627282930313233343536373839404142434445import pandas as pd import jieba from keras.models import Sequentialfrom keras.layers.merge import concatenate from keras.utils import to_categorical from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from tensorflow import * from keras.layers.embeddings import Embedding from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, Input, Lambda,BatchNormalization from sklearn.metrics import accuracy_score, f1_score from keras.models import Modelimport numpy as np#构建TextCNN模型#模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接def TextCNN_model_1(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test): main_input = Input(shape=(130,), dtype='float64') # 词嵌入（使用预训练的词向量） embedder = Embedding(len(vocab) + 1, 300, input_length=130, trainable=False) embed = embedder(main_input) # 词窗大小分别为3,4,5 cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed) cnn1 = MaxPooling1D(pool_size=128)(cnn1) cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed) cnn2 = MaxPooling1D(pool_size=127)(cnn2) cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed) cnn3 = MaxPooling1D(pool_size=126)(cnn3) # 合并三个模型的输出向量 cnn = concatenate([cnn1, cnn2, cnn3], axis=-1) flat = Flatten()(cnn) drop = Dropout(0.2)(flat) main_output = Dense(2, activation='softmax')(drop) model = Model(inputs=main_input, outputs=main_output) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) one_hot_labels = keras.utils.to_categorical(y_train, num_classes=2) # 将标签转换为one-hot编码 model.fit(x_train_padded_seqs, one_hot_labels, batch_size=800, epochs=10) #y_test_onehot = keras.utils.to_categorical(y_test, num_classes=3) # 将标签转换为one-hot编码 result = model.predict(x_test_padded_seqs) # 预测样本属于每个类别的概率 result_labels = np.argmax(result, axis=1) # 获得最大概率对应的标签# y_predict = list(map(str, result_labels))# print('准确率', metrics.accuracy_score(y_test, y_predict))# print('平均f1-score:', metrics.f1_score(y_test, y_predict, average='weighted')) print('准确率', accuracy_score(y_test, result_labels)) print('平均f1-score:', f1_score(y_test, result_labels, average='weighted')) 4.训练结果(在colab上训练)： 5.模型理解：这里可以通过model.summary或者keras中的plot_model来输出模型整体结构，帮助理解，plot_model报错踩坑 尤其是像这种有concatenate函数合并的结构，图形可视化很直观，方便学习。 123456789101112131415161718192021main_input = Input(shape=(50,), dtype='float64')# 词嵌入（使用预训练的词向量）embedder = Embedding(10000 + 1, 300, input_length=50, trainable=False)embed = embedder(main_input)# 词窗大小分别为3,4,5cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)cnn1 = MaxPooling1D(pool_size=48)(cnn1)cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)cnn2 = MaxPooling1D(pool_size=47)(cnn2)cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)cnn3 = MaxPooling1D(pool_size=46)(cnn3)# 合并三个模型的输出向量cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)flat = Flatten()(cnn)drop = Dropout(0.2)(flat)main_output = Dense(10, activation='softmax')(drop)model = Model(inputs=main_input, outputs=main_output)from keras.utils import plot_modelimport pydotplot_model(model,to_file='TextCNNmodel2.png',show_shapes=True,show_layer_names=False) ​ 图一 ​ 图二 ​ 图三 ​ 相比于图像领域的CNN，文本处理中的TextCNN有一定的差异，第一是在维度上面，由图二（普通CNN）：每一个卷积核宽度就是每个词向量的长度，从上向下滑动，没有图像领域中的横向滑动，故调用Conv1D，每次滑动几个单词就是卷积核的高度，如图三：三种不同高度的卷积核，能够每次读取不同相邻个数的单词，这里有点像n-gram的感觉，不同高度的卷积核能够提取不同的特征，而这些特征恰好能够体现词与词之间的关联， ​ 图一中：每一层的第一个维度是None，这里的None就是你的batchsize，每次处理的样本个数，因为这个参数是在model.fit中定义，所以这里显示none； 输入层：维度为batchsize $\\times$ input_length ,input_length就是我们的一个句子的长度，就像图二中 输入层左边从上到下就是一句话，一个句子就是一个样本。 embedding层：将batchsize $\\times$ input_length输入embedding层中，embedding层的参数300，就是我们想要每个词向量维度变成的长度，为什么要这么做呢？因为如果每个词都是用onehot向量，那么整个单词词典有多长，词向量就有多长，这样第一：计算能力要求会非常高，当词典无限大的时候也没法办了，第二：onehot向量只体现了词频，无法体现语义上的含义，我们更希望采用一个低维的向量来刻画单词本身，embedding的作用就是降维，当输入之后，embedding层用input_length $\\times$(len(vocab)+1) 来与 (len(vocab)+1)$\\times$300相乘，就将每个句子变成了：input_length $\\times$300的矩阵，从而实现词向量的降维，而这一层刚开始就是起到初始化的作用。这里不同于word2vec的是：word2vec的目的是训练词向量，而embedding是训练词向量的一种方式，或者说在整个模型任务达到收敛后，embedding层训练出来的词向量就是切合任务需求的（这里模型后面层可能是二分类，也可能是多分类或者等等任务）。 卷积层：这里TextCNN设置了3、4、5的三个不同高度的卷积核，每次滑动的时候进行向量乘法，padding选为same就是让卷积之后得到的长度和原来长度（50）一致，举例：当卷积核高度为3时，步长为1向下滑动，每滑动一次生成一个向量值，那么要保证前后长度都为50的话，就要在原来的50长度下面加2个padding值。 池化层：参数中如果不特别设定步长，keras默认和池化大小（pool_size）相同，定为48就是因为没算padding的0.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"TextCNN","slug":"深度学习/TextCNN","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN/"}],"tags":[{"name":"TextCNN","slug":"TextCNN","permalink":"https://ericzikun.github.io/tags/TextCNN/"},{"name":"Imdb","slug":"Imdb","permalink":"https://ericzikun.github.io/tags/Imdb/"},{"name":"文本分类","slug":"文本分类","permalink":"https://ericzikun.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"}]},{"title":"Mactex写论文","slug":"Mactex写论文","date":"2019-12-22T02:18:54.000Z","updated":"2020-05-16T02:36:45.219Z","comments":true,"path":"2019/12/22/Mactex写论文/","link":"","permalink":"https://ericzikun.github.io/2019/12/22/Mactex%E5%86%99%E8%AE%BA%E6%96%87/","excerpt":"","text":"前言这两个下载mactex花了不少时间，参考了不少文章，有的推荐texstudio、texpad等等，最终还是比较喜欢mactex+sublime text3+skim，环境配置方面并不算麻烦，只要一步一步来就ok！本文主要分为如下几个部分，帮助大家迅速安装Mac版的latex，成为论文高产者！ 安装Mactex如果在官网下载mactex，速度非常慢，这里推荐使用mac的必备终端插件homebrew，不用担心学习成本，很简单几步就可以完成可以参考网站： 打开mac终端(在实用工具里面) 输入： 1&#x2F;usr&#x2F;bin&#x2F;ruby -e &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install)&quot; 更改为国内中科大镜像资源：在访达中搜索： brew_install,用文本编辑器打开： 将下方内容用#注释: 1BREW_REPO &#x3D; “https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;brew“.freeze 并在下面一行添加： 1BREW_REPO &#x3D; &quot;git:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;brew.git&quot;.freeze 安装brew： 1&#x2F;usr&#x2F;bin&#x2F;ruby ~&#x2F;brew_install 用brew安装 mactex 1brew cask install mactex 等待几十分钟（有进度条），中间不要断网息屏，matex最终安装成功！！ 下载&amp;配置sublime此处我是直接在公众号找链接下载的，很多公众号都可以下载（微信搜索mac软件会有一堆），找到sublime text3的资源并下载安装即可，很简单！可以设置中文再配置 进入Package Control 官网，复制灰色区块的代码（记得对应好版本）； 打开下载好的 Sublime Text；使用快捷键“control+”（就在Esc键的下方）打开 Console，这时，会在底部看到弹出一个白色窗口；然后，将刚才复制的代码粘贴到控制面板；按下“Enter”回车键。待 install 完成后， 退出并重启软件； Sublime Text重启后，按下 ’ Command+Shift+P ‘ 打开 Command pallet，输入命令“Install Package”，按下Enter回车建。 完成之后，再输入“LaTeX Tools”，找到这一项并安装； 安装完成后，退出并重启Sublime Text。从而完成了Mac LaTeX 的配置了。 安装pdf预览器skim为了方便左边编译，右边实时更新pdf，这里需要调用skim软件：下载地址在左上角skim中选项一栏进行设置：将预设编译器改为sublime，打钩检查文件变化，好了，到这里基本上就完成了好了，现在新建一个sublime文件就可以了，由于很生成很多文件，所以一定在新建文件夹里面新建一个.tex的文件（这一步很重要，否则编译不出来），编译的快捷键为：command+B下面给两个大家来测试一下： 包含中文的代码：1234567%!TEX program &#x3D; xelatex \\documentclass&#123;article&#125;\\usepackage&#123;xeCJK, fontspec, xunicode, xltxtra&#125; \\begin&#123;document&#125;hello world!成功\\end&#123;document&#125; IEEE官方提供的会议论文模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282\\documentclass[conference]&#123;IEEEtran&#125;\\IEEEoverridecommandlockouts% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\\usepackage&#123;cite&#125;\\usepackage&#123;amsmath,amssymb,amsfonts&#125;\\usepackage&#123;algorithmic&#125;\\usepackage&#123;graphicx&#125;\\usepackage&#123;textcomp&#125;\\usepackage&#123;xcolor&#125;\\def\\BibTeX&#123;&#123;\\rm B\\kern-.05em&#123;\\sc i\\kern-.025em b&#125;\\kern-.08em T\\kern-.1667em\\lower.7ex\\hbox&#123;E&#125;\\kern-.125emX&#125;&#125;\\begin&#123;document&#125;\\title&#123;Real-vessel trajectory data-based in crowed inland waterway\\\\&#123;\\footnotesize \\textsuperscript&#123;*&#125;Note: Sub-titles are not captured in Xplore andshould not be used&#125;\\thanks&#123;Identify applicable funding agency here. If none, delete this.&#125;&#125;\\author&#123;\\IEEEauthorblockN&#123;1\\textsuperscript&#123;st&#125; Given Name Surname&#125;\\IEEEauthorblockA&#123;\\textit&#123;dept. name of organization (of Aff.)&#125; \\\\\\textit&#123;name of organization (of Aff.)&#125;\\\\City, Country \\\\email address&#125;\\and\\IEEEauthorblockN&#123;2\\textsuperscript&#123;nd&#125; Given Name Surname&#125;\\IEEEauthorblockA&#123;\\textit&#123;dept. name of organization (of Aff.)&#125; \\\\\\textit&#123;name of organization (of Aff.)&#125;\\\\City, Country \\\\email address&#125;\\and\\IEEEauthorblockN&#123;3\\textsuperscript&#123;rd&#125; Given Name Surname&#125;\\IEEEauthorblockA&#123;\\textit&#123;dept. name of organization (of Aff.)&#125; \\\\\\textit&#123;name of organization (of Aff.)&#125;\\\\City, Country \\\\email address&#125;\\and\\IEEEauthorblockN&#123;4\\textsuperscript&#123;th&#125; Given Name Surname&#125;\\IEEEauthorblockA&#123;\\textit&#123;dept. name of organization (of Aff.)&#125; \\\\\\textit&#123;name of organization (of Aff.)&#125;\\\\City, Country \\\\email address&#125;\\and\\IEEEauthorblockN&#123;5\\textsuperscript&#123;th&#125; Given Name Surname&#125;\\IEEEauthorblockA&#123;\\textit&#123;dept. name of organization (of Aff.)&#125; \\\\\\textit&#123;name of organization (of Aff.)&#125;\\\\City, Country \\\\email address&#125;\\and\\IEEEauthorblockN&#123;6\\textsuperscript&#123;th&#125; Given Name Surname&#125;\\IEEEauthorblockA&#123;\\textit&#123;dept. name of organization (of Aff.)&#125; \\\\\\textit&#123;name of organization (of Aff.)&#125;\\\\City, Country \\\\email address&#125;&#125;\\maketitle\\begin&#123;abstract&#125;This document is a model and instructions for \\LaTeX.This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.\\end&#123;abstract&#125;\\begin&#123;IEEEkeywords&#125;component, formatting, style, styling, insert\\end&#123;IEEEkeywords&#125;\\section&#123;Introduction&#125;This document is a model and instructions for \\LaTeX.Please observe the conference page limits. \\section&#123;Ease of Use&#125;\\subsection&#123;Maintaining the Integrity of the Specifications&#125;The IEEEtran class file is used to format your paper and style the text. All margins, column widths, line spaces, and text fonts are prescribed; please do not alter them. You may note peculiarities. For example, the head marginmeasures proportionately more than is customary. This measurement and others are deliberate, using specifications that anticipate your paper as one part of the entire proceedings, and not as an independent document. Please do not revise any of the current designations.\\section&#123;Prepare Your Paper Before Styling&#125;Before you begin to format your paper, first write and save the content as a separate text file. Complete all content and organizational editing before formatting. Please note sections \\ref&#123;AA&#125;--\\ref&#123;SCM&#125; below for more information on proofreading, spelling and grammar.Keep your text and graphic files separate until after the text has been formatted and styled. Do not number text heads---&#123;\\LaTeX&#125; will do that for you.\\subsection&#123;Abbreviations and Acronyms&#125;\\label&#123;AA&#125;Define abbreviations and acronyms the first time they are used in the text, even after they have been defined in the abstract. Abbreviations such as IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use abbreviations in the title or heads unless they are unavoidable.\\subsection&#123;Units&#125;\\begin&#123;itemize&#125;\\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as &#96;&#96;3.5-inch disk drive&#39;&#39;.\\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.\\item Do not mix complete spellings and abbreviations of units: &#96;&#96;Wb&#x2F;m\\textsuperscript&#123;2&#125;&#39;&#39; or &#96;&#96;webers per square meter&#39;&#39;, not &#96;&#96;webers&#x2F;m\\textsuperscript&#123;2&#125;&#39;&#39;. Spell out units when they appear in text: &#96;&#96;. . . a few henries&#39;&#39;, not &#96;&#96;. . . a few H&#39;&#39;.\\item Use a zero before decimal points: &#96;&#96;0.25&#39;&#39;, not &#96;&#96;.25&#39;&#39;. Use &#96;&#96;cm\\textsuperscript&#123;3&#125;&#39;&#39;, not &#96;&#96;cc&#39;&#39;.)\\end&#123;itemize&#125;\\subsection&#123;Equations&#125;Number equations consecutively. To make your equations more compact, you may use the solidus (~&#x2F;~), the exp function, or appropriate exponents. Italicize Roman symbols for quantities and variables, but not Greek symbols. Use a long dash rather than a hyphen for a minus sign. Punctuate equations with commas or periods when they are part of a sentence, as in:\\begin&#123;equation&#125;a+b&#x3D;\\gamma\\label&#123;eq&#125;\\end&#123;equation&#125;Be sure that the symbols in your equation have been defined before or immediately following the equation. Use &#96;&#96;\\eqref&#123;eq&#125;&#39;&#39;, not &#96;&#96;Eq.~\\eqref&#123;eq&#125;&#39;&#39; or &#96;&#96;equation \\eqref&#123;eq&#125;&#39;&#39;, except at the beginning of a sentence: &#96;&#96;Equation \\eqref&#123;eq&#125; is . . .&#39;&#39;\\subsection&#123;\\LaTeX-Specific Advice&#125;Please use &#96;&#96;soft&#39;&#39; (e.g., \\verb|\\eqref&#123;Eq&#125;|) cross references insteadof &#96;&#96;hard&#39;&#39; references (e.g., \\verb|(1)|). That will make it possibleto combine sections, add equations, or change the order of figures orcitations without having to go through the file line by line.Please don&#39;t use the \\verb|&#123;eqnarray&#125;| equation environment. Use\\verb|&#123;align&#125;| or \\verb|&#123;IEEEeqnarray&#125;| instead. The \\verb|&#123;eqnarray&#125;|environment leaves unsightly spaces around relation symbols.Please note that the \\verb|&#123;subequations&#125;| environment in &#123;\\LaTeX&#125;will increment the main equation counter even when there are noequation numbers displayed. If you forget that, you might write anarticle in which the equation numbers skip from (17) to (20), causingthe copy editors to wonder if you&#39;ve discovered a new method ofcounting.&#123;\\BibTeX&#125; does not work by magic. It doesn&#39;t get the bibliographicdata from thin air but from .bib files. If you use &#123;\\BibTeX&#125; to produce abibliography you must send the .bib files. &#123;\\LaTeX&#125; can&#39;t read your mind. If you assign the same label to asubsubsection and a table, you might find that Table I has been crossreferenced as Table IV-B3. &#123;\\LaTeX&#125; does not have precognitive abilities. If you put a\\verb|\\label| command before the command that updates the counter it&#39;ssupposed to be using, the label will pick up the last counter to becross referenced instead. In particular, a \\verb|\\label| commandshould not go before the caption of a figure or a table.Do not use \\verb|\\nonumber| inside the \\verb|&#123;array&#125;| environment. Itwill not stop equation numbers inside \\verb|&#123;array&#125;| (there won&#39;t beany anyway) and it might stop a wanted equation number in thesurrounding equation.\\subsection&#123;Some Common Mistakes&#125;\\label&#123;SCM&#125;\\begin&#123;itemize&#125;\\item The word &#96;&#96;data&#39;&#39; is plural, not singular.\\item The subscript for the permeability of vacuum $\\mu_&#123;0&#125;$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter &#96;&#96;o&#39;&#39;.\\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)\\item A graph within a graph is an &#96;&#96;inset&#39;&#39;, not an &#96;&#96;insert&#39;&#39;. The word alternatively is preferred to the word &#96;&#96;alternately&#39;&#39; (unless you really mean something that alternates).\\item Do not use the word &#96;&#96;essentially&#39;&#39; to mean &#96;&#96;approximately&#39;&#39; or &#96;&#96;effectively&#39;&#39;.\\item In your paper title, if the words &#96;&#96;that uses&#39;&#39; can accurately replace the word &#96;&#96;using&#39;&#39;, capitalize the &#96;&#96;u&#39;&#39;; if not, keep using lower-cased.\\item Be aware of the different meanings of the homophones &#96;&#96;affect&#39;&#39; and &#96;&#96;effect&#39;&#39;, &#96;&#96;complement&#39;&#39; and &#96;&#96;compliment&#39;&#39;, &#96;&#96;discreet&#39;&#39; and &#96;&#96;discrete&#39;&#39;, &#96;&#96;principal&#39;&#39; and &#96;&#96;principle&#39;&#39;.\\item Do not confuse &#96;&#96;imply&#39;&#39; and &#96;&#96;infer&#39;&#39;.\\item The prefix &#96;&#96;non&#39;&#39; is not a word; it should be joined to the word it modifies, usually without a hyphen.\\item There is no period after the &#96;&#96;et&#39;&#39; in the Latin abbreviation &#96;&#96;et al.&#39;&#39;.\\item The abbreviation &#96;&#96;i.e.&#39;&#39; means &#96;&#96;that is&#39;&#39;, and the abbreviation &#96;&#96;e.g.&#39;&#39; means &#96;&#96;for example&#39;&#39;.\\end&#123;itemize&#125;An excellent style manual for science writers is \\cite&#123;b7&#125;.\\subsection&#123;Authors and Affiliations&#125;\\textbf&#123;The class file is designed for, but not limited to, six authors.&#125; A minimum of one author is required for all conference articles. Author names should be listed starting from left to right and then moving down to the next line. This is the author sequence that will be used in future citations and by indexing services. Names should not be listed in columns nor group by affiliation. Please keep your affiliations as succinct as possible (for example, do not differentiate among departments of the same organization).\\subsection&#123;Identify the Headings&#125;Headings, or heads, are organizational devices that guide the reader through your paper. There are two types: component heads and text heads.Component heads identify the different components of your paper and are not topically subordinate to each other. Examples include Acknowledgments and References and, for these, the correct style to use is &#96;&#96;Heading 5&#39;&#39;. Use &#96;&#96;figure caption&#39;&#39; for your Figure captions, and &#96;&#96;table head&#39;&#39; for your table title. Run-in heads, such as &#96;&#96;Abstract&#39;&#39;, will require you to apply a style (in this case, italic) in addition to the style provided by the drop down menu to differentiate the head from the text.Text heads organize the topics on a relational, hierarchical basis. For example, the paper title is the primary text head because all subsequent material relates and elaborates on this one topic. If there are two or more sub-topics, the next level head (uppercase Roman numerals) should be used and, conversely, if there are not at least two sub-topics, then no subheads should be introduced.\\subsection&#123;Figures and Tables&#125;\\paragraph&#123;Positioning Figures and Tables&#125; Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Large figures and tables may span across both columns. Figure captions should be below the figures; table heads should appear above the tables. Insert figures and tables after they are cited in the text. Use the abbreviation &#96;&#96;Fig.~\\ref&#123;fig&#125;&#39;&#39;, even at the beginning of a sentence.\\begin&#123;table&#125;[htbp]\\caption&#123;Table Type Styles&#125;\\begin&#123;center&#125;\\begin&#123;tabular&#125;&#123;|c|c|c|c|&#125;\\hline\\textbf&#123;Table&#125;&amp;\\multicolumn&#123;3&#125;&#123;|c|&#125;&#123;\\textbf&#123;Table Column Head&#125;&#125; \\\\\\cline&#123;2-4&#125; \\textbf&#123;Head&#125; &amp; \\textbf&#123;\\textit&#123;Table column subhead&#125;&#125;&amp; \\textbf&#123;\\textit&#123;Subhead&#125;&#125;&amp; \\textbf&#123;\\textit&#123;Subhead&#125;&#125; \\\\\\hlinecopy&amp; More table copy$^&#123;\\mathrm&#123;a&#125;&#125;$&amp; &amp; \\\\\\hline\\multicolumn&#123;4&#125;&#123;l&#125;&#123;$^&#123;\\mathrm&#123;a&#125;&#125;$Sample of a Table footnote.&#125;\\end&#123;tabular&#125;\\label&#123;tab1&#125;\\end&#123;center&#125;\\end&#123;table&#125;\\begin&#123;figure&#125;[htbp]%\\centerline&#123;\\includegraphics&#123;fig1.png&#125;&#125;\\caption&#123;Example of a figure caption.&#125;\\label&#123;fig&#125;\\end&#123;figure&#125;Figure Labels: Use 8 point Times New Roman for Figure labels. Use words rather than symbols or abbreviations when writing Figure axis labels to avoid confusing the reader. As an example, write the quantity &#96;&#96;Magnetization&#39;&#39;, or &#96;&#96;Magnetization, M&#39;&#39;, not just &#96;&#96;M&#39;&#39;. If including units in the label, present them within parentheses. Do not label axes only with units. In the example, write &#96;&#96;Magnetization (A&#x2F;m)&#39;&#39; or &#96;&#96;Magnetization \\&#123;A[m(1)]\\&#125;&#39;&#39;, not just &#96;&#96;A&#x2F;m&#39;&#39;. Do not label axes with a ratio of quantities and units. For example, write &#96;&#96;Temperature (K)&#39;&#39;, not &#96;&#96;Temperature&#x2F;K&#39;&#39;.\\section*&#123;Acknowledgment&#125;The preferred spelling of the word &#96;&#96;acknowledgment&#39;&#39; in America is without an &#96;&#96;e&#39;&#39; after the &#96;&#96;g&#39;&#39;. Avoid the stilted expression &#96;&#96;one of us (R. B. G.) thanks $\\ldots$&#39;&#39;. Instead, try &#96;&#96;R. B. G. thanks$\\ldots$&#39;&#39;. Put sponsor acknowledgments in the unnumbered footnote on the first page.\\section*&#123;References&#125;Please number citations consecutively within brackets \\cite&#123;IEEEhowto:IEEEtranpage&#125;. The sentence punctuation follows the bracket \\cite&#123;b2&#125;. Refer simply to the reference number, as in \\cite&#123;b3&#125;---do not use &#96;&#96;Ref. \\cite&#123;b3&#125;&#39;&#39; or &#96;&#96;reference \\cite&#123;b3&#125;&#39;&#39; except at the beginning of a sentence: &#96;&#96;Reference \\cite&#123;b3&#125; was the first $\\ldots$&#39;&#39;Number footnotes separately in superscripts. Place the actual footnote at the bottom of the column in which it was cited. Do not put footnotes in the abstract or reference list. Use letters for table footnotes.Unless there are six authors or more give all authors&#39; names; do not use &#96;&#96;et al.&#39;&#39;. Papers that have not been published, even if they have been submitted for publication, should be cited as &#96;&#96;unpublished&#39;&#39; \\cite&#123;b4&#125;. Papers that have been accepted for publication should be cited as &#96;&#96;in press&#39;&#39; \\cite&#123;b5&#125;. Capitalize only the first word in a paper title, except for proper nouns and element symbols.For papers published in translation journals, please give the English citation first, followed by the original foreign-language citation \\cite&#123;b6&#125;.\\bibliographystyle&#123;.&#x2F;bibliography&#x2F;IEEEtran&#125;\\bibliography&#123;.&#x2F;bibliography&#x2F;IEEEabrv,.&#x2F;bibliography&#x2F;IEEEexample&#125;\\vspace&#123;12pt&#125;\\color&#123;red&#125;IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.\\end&#123;document&#125; 按下command+B，就会自动调出skim的pdf啦！英文期刊、会议的模板，都可以在overleaf在线latex网站下载，要注册账号（如果登不了就用梯），然后就可以替换对应的文字部分生成论文了，当然，公式还是要一点点语句功底的，可以对照latex公式手册来进行就好！遇到不会的地方直接百度即可！ 参考博客：https://www.jianshu.com/p/b1e3b029ded5https://zhuanlan.zhihu.com/p/59805070","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"论文排版","slug":"技巧/论文排版","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"}],"tags":[{"name":"效率","slug":"效率","permalink":"https://ericzikun.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"学术","slug":"学术","permalink":"https://ericzikun.github.io/tags/%E5%AD%A6%E6%9C%AF/"},{"name":"Mac","slug":"Mac","permalink":"https://ericzikun.github.io/tags/Mac/"},{"name":"论文排版","slug":"论文排版","permalink":"https://ericzikun.github.io/tags/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"}]},{"title":"文本抽取算法Textrank","slug":"文本抽取算法Textrank","date":"2019-12-20T02:19:13.000Z","updated":"2020-05-04T13:38:03.800Z","comments":true,"path":"2019/12/20/文本抽取算法Textrank/","link":"","permalink":"https://ericzikun.github.io/2019/12/20/%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96%E7%AE%97%E6%B3%95Textrank/","excerpt":"","text":"理论基础Textrank实际上，TextRank就是PageRank在文本上的应用。PageRank是一种用于排序网页的随机算法，它的工作原理是将互联网看作有向图，互联网上的网页视作节点，节点$V_i$到节点$V_j$，的超链接视作有向边，初始化时每个节点的权重$S(V_i)$都是1，以迭代的方式更新每个节点的权重。每次迭代权重的更新表达式如下：$$S(V_i)=(1-d)+d\\times{\\sum_{V_j\\in{In(V_j)}}\\frac{1}{|Out(V_j)|}}$$ 其中$d$是一个介于$(0，1)$之间的常数因子，在PageRank中模拟用户点击链接从而跳出当前网站的概率. $In(V_j)$表示链接到的节点集合. $Out(V_j)$表示从$V_j$出发链接到的节点集合可见，并不是外链越多，网站的PageRank就越高。网站给别的网站做外链越多，每条外链的权重就越低（如垃圾网站之间相互链接）。因为根据式中的分式$\\frac{1}{|Out(V_j)|}$，外链权重跟外链总数成反比，与提供外链的网站权重成正比。如果一个网站的外链都是这种权重很低的外链，那么在迭代中它的PageRank会下降。对于文本关键词提取也是类似的，与之前的TF-IDF算法需要依赖语料库不同，基于TextRank的关键词抽取算法，可以把文本中的每个词看作是一个节点/网页，把文本中词的共现关系看作是边/链接。与PageRank不同的是，PageRank中是有向边，而TextRank中是无向边或可以看作是双向边，具有共现关系的两个词互相指向。参考链接所谓的共现关系，就是对文本进行预处理(分词，去停用词，以及词性标注/筛选)后，设置一个默认大小为m的窗口,在文本中从头到尾依次滑动，同一个窗口中的任意两个词之间都连一条边(无向边，入度$In(V_j)$和出度$Out(V_j)$完全一致)。画出图之后，对每个词$S(V_i)$赋于一个初始值$S_0(V_i)$,然后代入上述公式进行迭代，直到收敛(在某次更新前后，$S(V_i)$不再变化).最终选择按词语/节点的Rank值降序排列，选择TopN作为我们的关键词。 基本步骤： 对给定的文本进行断句，按?。!等进行分隔 对于每个句子，进行分词，去除停用词，词性标注。并保留指定的词性，如名词、动词等,去掉其他无关词性的词语。作为候选关键词。 基于候选关键词，构建图G=(V,E),其中V是节点集/候选关键词集合，通过设置窗口和共现关系构造任意两个节点/词语之间的边，两个节点/词语之间存在边当且仅当这两个节点/词语在长度为m的窗口中共同出现过，窗口从头到尾不断滑动。 根据PageRank迭代公式，初始化每个节点/词语的权重/Rank值(可以是1/N，N为节点/词语数量),针对每个节点/词语，代入上述公式进行迭代，直至收敛。 对所有节点/词语最终的权重/Rank值进行降序排列，选择TopN作为我们的关键词 由5得到最重要的N个词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。例如，文本中有句子“Matlab code for plotting ambiguity function”，如果“Matlab”和“code”均属于候选关键词，则组合成“Matlab code”加入关键词序列。 TextRank与TF-IDF比较1.tf-idf中计算idf值需要依赖于语料库,这给他带来了统计上的优势,即它能够预先知道一个词的重要程度.这是它优于textrank的地方. 而textrank只依赖文章本身,它认为一开始每个词的重要程度是一样的. 2.tf-idf是纯粹用词频的思想(无论是tf还是idf都是)来计算一个词的得分,最终来提取关键词,完全没有用到词之间的关联性. 而textrank用到了词之间的关联性(将相邻的词链接起来),这是其优于tf-idf的地方. TF-IDF和TextRank各有优劣，在实际使用中效果差异不大，可以同时使用互相参考。 参考链接https://blog.csdn.net/sdu_hao/article/details/86768966https://blog.csdn.net/sdu_hao/article/details/86768966","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"文本抽取","slug":"机器学习/文本抽取","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96/"}],"tags":[{"name":"Textrank","slug":"Textrank","permalink":"https://ericzikun.github.io/tags/Textrank/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"https://ericzikun.github.io/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/"},{"name":"NLP","slug":"NLP","permalink":"https://ericzikun.github.io/tags/NLP/"}]},{"title":"MAC软件集锦","slug":"MAC","date":"2019-12-08T02:23:24.000Z","updated":"2020-07-09T14:20:05.891Z","comments":true,"path":"2019/12/08/MAC/","link":"","permalink":"https://ericzikun.github.io/2019/12/08/MAC/","excerpt":"","text":"装机必备：搜狗输入法 去官网下 mac 装软件必备地址：下载软件：网址：https://xclient.info/ 下载安装解压密码：xclient.infohttps://www.macappbox.com/公众号：Mac软件… 常用软件浏览器：chrome （信息量非常大，另起一篇博客细说）pdf：自带用的多；修改方面：pdf expert；acrobat；marginote；思维导图：xmind zen；office：wps写论文：虚拟机：paralles desktop；欧路词典：下载破解的 在csdn上找，Mac天空 - www.mac69.com；markdown：typora看视频用的必备！：inna；自带的QuickTime；压缩文件 解压文件：keka（设置默认打开方式：右键文件–显示简介–全部更改） 提升效率：插件：alfred：工作流：ocr功能：调用百度文本识别api；找文件：用find+空格+文件名；找软件直接搜（详细教程另起）贴图、截图：snipaste截图：xnip最喜欢的剪切板：paste conda deactivate展现快捷键：cheatsheet替代xshell：royal tsx；itermLatex：mactex；新建文件菜单：mac不自带，需要下载插件Downie ：下载网页视频：系统：CleanMyMac Geekbench","categories":[{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"提升效率","slug":"技巧/提升效率","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87/"}],"tags":[{"name":"效率","slug":"效率","permalink":"https://ericzikun.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"学术","slug":"学术","permalink":"https://ericzikun.github.io/tags/%E5%AD%A6%E6%9C%AF/"},{"name":"Mac","slug":"Mac","permalink":"https://ericzikun.github.io/tags/Mac/"}]},{"title":"逻辑回归","slug":"逻辑回归","date":"2019-11-15T02:17:14.000Z","updated":"2020-04-12T07:08:04.871Z","comments":true,"path":"2019/11/15/逻辑回归/","link":"","permalink":"https://ericzikun.github.io/2019/11/15/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"","text":"（五）逻辑回归定义：当给定一个训练集（矩阵），如下： 年龄 工资 学历 逾期 20 4000 本科 YES（Y=1） 25 5000 专科 NO（Y=0） 22 6000 本科 NO 28 5000 专科 YES 27 8000 本科 我们需要去预测最后一个人是否会逾期，归根到底就是要学习从输入到输出的映射$$f:X \\rightarrow Y$$ 最后求出一个条件概率$P(Y|X)$，即$P(Y=1|27，8000，本科)$，其概率大则逾期可能性大，或者说就是比较$P(Y=1|27，8000，本科)$和$P(Y=0|27，8000，本科)$的大小。 那么能否用线性回归来表示呢？比如：$P(Y=1)=w^Tx+b$,观察过值域后会发现是不行的，概率不大于1，而右边值域为负无穷到正无穷，为了使得等号成立，则需要采用逻辑函数$\\sigma$,$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$ 逻辑函数$\\sigma(x)$的值域为$(0,1)$满足概率的值域范围，将$w^Tx+b$作为自变量替换即可，条件概率即为： $$P(Y|X)=\\frac{1}{1+e^{-(w^T+b)}}$$ 对于二分类问题来说： $$P(Y=1|X)=\\frac{1}{1+e^{-w^T+b}}$$ $$P(Y=0|X)=\\frac{e^{-w^T+b}}{1+e^{-w^T+b}}$$ 两个式子可以合并为： $P(y|x,w)=p(y=1|x,w)^y[1-p(y=1|x,w)]^{(1-y)}$ 合并的式子可这样理解：当$y=1$时：$p(y=1|x,w)=p(y=1|x,w)\\times1$ 当$y=0$时：$p(y=0|x,w)=1-p(y=1|x,w)$ 逻辑函数为线性分类器 证明如下： 这是感知机里面的图片，相同的，逻辑回归也是将平面上各种点进行了分类，为了证明逻辑回归是线性的分类器，我们将落在分离平面上的点组成的线方程给求出来，是直线则是线性分类，落在分离平面上的点有一个特点，它到两边的距离都差不多，也就是说它是$y=1$和$y=0$的概率相等： $$P(Y=1|X)=P(Y=0|X)$$ $$\\frac{1}{1+e^{-w^T+b}}=\\frac{e^{-w^T+b}}{1+e^{-w^T+b}}$$ 推导可得： $$w^Tx+b=0$$ 即：分离平面（边界）为线性分类器 逻辑回归的目标函数： 前面已经定义了合并后的： $$P(y|x,w)=p(y=1|x,w)^y[1-p(y=1|x,w)]^{1-y}$$ 下面我们要来由训练集训练出最好的权重$(w)$和偏置$(b)$,从而使得能够最好的拟合我们的数据集$(X)$,因此我们需要最大化：给定样本数据的$x_i,w,b$情况下，对应为label$(y_i)$的概率，也就是条件概率$p(y_i|x_i,w,b)$（最大似然）,想让每一项都最大，那么每个样本条件概率相乘也就最大。 即最大化目标函数： $$\\hat{w},\\hat{b}=armax_{w,b}\\prod_{i=1}^{n}p(y_i|x_i,w,b)$$（寻找$w,b$使得连乘式最大化） $armax_{w,b}\\prod_{i=1}^{n}p(y_i|x_i,w,b)=argmax_{w,b}\\log(\\prod_{i=1}^{n}p(y_i|x_i,w,b))=argmax_{w,b}\\sum_{i=1}^{n} \\\\log p(y_i|x_i,w,b)$ 通常我们不喜欢去求最大值，而是转化为最小值求解，即： $$argmin_{w,b}=-\\sum_{i=1}^{n}\\log{p(y_i|x_i,w)}$$ 逻辑回归的梯度下降法： $argmin_{w,b}-\\sum_{i=1}^{n}\\log{p(y_i|x_i,w)} \\\\ =argmin_{w,b}-\\sum_{i=1}^{n}\\log p(y_i=1|x,w)^{y_i}[1-p(y_i=1|x,w)]^{1-y_i} \\\\ =argmax_{w,b}-\\sum_{i=1}^{n}y_i\\log p(y_i=1|x,w)+(1-y_i)\\log p(y_i=1|x,w)$ 令$-\\sum_{i=1}^{n}y_i\\log \\sigma (w^Tx+b)+(1-y_i)\\log \\sigma (w^Tx+b)$为$L(w,b)$ 对权重$w$求导： 求导知识：$\\begin{cases}\\sigma(x)=\\frac{1}{1+e^{-x}}\\\\sigma^{’}(x)=\\sigma(x)[1-\\sigma(x)]\\\\ (\\log x)^{’} =\\frac{1}{x} \\end{cases}$ $\\frac{\\partial L(w,b)}{\\partial w}=-\\sum_{i=1}^{n}y_i\\frac{\\sigma(w^Tx+b)[1-\\sigma(w^Tx+b)]}{\\sigma(w^Tx+b)}x_i+(1-y_i)\\frac{-\\sigma(w^Tx+b)[1-\\sigma(w^Tx+b)]}{1-\\sigma(w^Tx+b)}x_i \\\\ =-\\sum_{i=1}^{n}y_i[1-\\sigma(w^Tx+b)]x_i+(y_i-1)\\sigma(w^Tx+b)x_i \\\\ =-\\sum_{i=1}^{n}[y_i-\\sigma(w^Tx+b)]x_i \\\\ =\\sum_{i=1}^{n}[\\sigma(w^Tx+b)-y_i]x_i$ 对偏置$b$求导： $\\frac{\\partial L(w,b)}{\\partial b}=\\sum_{i=1}^{n}[\\sigma(w^Tx+b)-y_i]$ （两者相比较，相差一个外部$x_i$相乘） 梯度下降： 初始化$w^1,b^1$ $For ：t=1,2…$ ​ $w^{t+1}=w^t-\\eta \\sum_{i=1}^{n}[\\sigma(w^Tx+b)-y_i]x_i \\\\ b^{t+1}=b^t-\\eta \\sum_{i=1}^{n}[\\sigma(w^Tx+b)-y_i]$ 梯度下降方式又分为：$\\begin{cases}1.标准梯度下降 \\\\ 2.随机梯度下降 \\\\ 3.Mini-batch梯度下降 \\end{cases}$标准梯度下降是在权值更新前对所有样例汇总误差,而随机梯度下降的权值是通过考查某个训练样例来更新的，而mini-batch则是两者的折中。一般来说不管是机器学习还是深度学习算法，我们一般都常用mini-batch，其原因在于，假如我们每次将全部样本丢进去计算，此时的计算量（例如导数、w、b、激活值等等缓存值）占用内存相当之大！对于一般计算机而言承担不起，所以mini-batch的选择其实主要还是根据自身硬件水平，比如深度学习中用GPU时要考虑显卡大小，显卡越好，mini-batch可以越大（一般来说是这样）。 参考博客：https://www.cnblogs.com/limitlessun/p/8611103.htmlhttps://www.pkudodo.com","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"逻辑回归","slug":"机器学习/逻辑回归","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}],"tags":[{"name":"统计学习方法","slug":"统计学习方法","permalink":"https://ericzikun.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"https://ericzikun.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}]},{"title":"决策树","slug":"决策树","date":"2019-11-13T02:17:04.000Z","updated":"2020-04-08T11:26:54.973Z","comments":true,"path":"2019/11/13/决策树/","link":"","permalink":"https://ericzikun.github.io/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"","text":"（四）决策树定义：书中实例：贷款申请样本，通过一个人的年龄、是否有工作、是否有自己的房子、信贷情况这四个特征判定，最终构建模型来判别是否给予贷款，如图：希望通过所给的训练数据学习一个贷款申请的决策树，用来对未来贷款申请进行分类（二分类），决策树可以理解成：有一个根节点开始，往下进行分支，越重要的节点应该离根越近，我们将重要的、影响度大的特征作为根节点，依次向下，其次重要的往下面街接，如图： 熵与条件熵的定义：熵: 表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：$$P(X=x_i)=p_i, i=1,2,…,n$$则随机变量$X$的熵定义为：$$H(X)=-\\sum_{i=1}^{n}p_i\\log{p_i}$$越大的概率，得到的熵值越小，也就是说概率大的确定性大，不确定不就小了嘛，反之亦然；举例：$A$集合：[1,1,1,1,1,1,1,2,2] &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$B$集合：[1,2,3,4,5,6,7,8,9] 显然$A$集合的熵值要低，因为A里面只有两种类别，相对稳定一些，而B中类别太多，熵值就会大很多，而在分类问题中我们当然是希望分支后的数据类别的熵值小，确定性就大嘛，熵值越低，分类效果越好撒同理条件熵 就是表示在已知随机变量X的条件下随机变量Y的不确定性$H(Y|X)$,定义为$X$给定条件下Y的条件概率分布的熵对X的数学期望：$$H(Y|X)=\\sum_{i=1}^{n}p_iH(Y|X=x_i)$$ 信息增益：做决策树目的就是在过程中将熵值不断减小，增益呢，就是熵值下降了多少，通过信息增益来遍历计算所有特征，哪个特征使得我们的信息增益最大，最大的哪个特征就拿过来当做根节点，接着同理把剩下的特征也这么来排序，排出第二个节点，第三个节点。。。信息增益表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度。 特征$A$对训练数据集$D$的信息增益$g(D,A)$,定义为集合$D$的经验熵，经验熵就是不考虑特征，只考虑整个样本label的熵，附上书中实例： $H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：$$g(D,A)=H(D)-H(D|A)$$熵$H(Y)$与条件熵$H(Y|X)$之差成为互信息，此时信息增益等于互信息。信息增益算法：输入：训练数据集$D$和特征$A$；输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$。(1)计算数据集$D$的经验熵$H(D)$$$H(D)=-\\sum_{k=1}^{k}\\frac{|C_k|}{|D|}\\log{\\frac{|C_k|}{|D|}}$$(2)计算特征$A$对数据集$D$的经验条件熵$H(D|A)$$$H(D|A)=\\sum_{i=1}^{n}\\frac{|D_I|}{|D|}H(D_i)=-\\sum_{i=1}^{n}\\frac{|D_i|}{D}\\sum_{k=1}^{k}\\frac{|D_{ik}|}{D_i}\\log_{2}\\frac{|D_{ik}|}{|D_i|}$$(3)计算信息增益：$$g(D,A)=H(D)-H(D,A)$$ 信息增益比：以信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比对其校正：特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即：$$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$以上讨论都是离散值，如果是连续值呢？ ID3算法:核心是在决策树各个结点上应用信息增益准则选择信息增益最大且大于阈值的特征，递归地构建决策树.ID3相当于用极大似然法进行概率模型的选择.甶于算法只有树的生成，所以容易产生过拟合。决策树剪枝策略：为什么要剪枝：决策树过拟合风险很大，预剪枝：边建立决策树边进行剪枝的操作后剪枝：当建立完决策树后进行剪枝操作 代码：参考代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321#coding=utf-8#Author:Dodo#Date:2018-11-21#Email:lvtengchao@pku.edu.cn#Blog:www.pkudodo.com'''数据集：Mnist训练集数量：60000测试集数量：10000------------------------------运行结果：ID3(未剪枝) 正确率：85.9% 运行时长：356s'''import timeimport numpy as npdef loadData(fileName): ''' 加载文件 :param fileName:要加载的文件路径 :return: 数据集和标签集 ''' #存放数据及标记 dataArr = []; labelArr = [] #读取文件 fr = open(fileName) #遍历文件中的每一行 for line in fr.readlines(): #获取当前行，并按“，”切割成字段放入列表中 #strip：去掉每行字符串首尾指定的字符（默认空格或换行符） #split：按照指定的字符将字符串切割成每个字段，返回列表形式 curLine = line.strip().split(',') #将每行中除标记外的数据放入数据集中（curLine[0]为标记信息） #在放入的同时将原先字符串形式的数据转换为整型 #此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算 dataArr.append([int(int(num) &gt; 128) for num in curLine[1:]]) #将标记信息放入标记集中 #放入的同时将标记转换为整型 labelArr.append(int(curLine[0])) #返回数据集和标记 return dataArr, labelArrdef majorClass(labelArr): ''' 找到当前标签集中占数目最大的标签 :param labelArr: 标签集 :return: 最大的标签 ''' #建立字典，用于不同类别的标签技术 classDict = &#123;&#125; #遍历所有标签 for i in range(len(labelArr)): #当第一次遇到A标签时，字典内还没有A标签，这时候直接幅值加1是错误的， #所以需要判断字典中是否有该键，没有则创建，有就直接自增 if labelArr[i] in classDict.keys(): # 若在字典中存在该标签，则直接加1 classDict[labelArr[i]] += 1 else: #若无该标签，设初值为1，表示出现了1次了 classDict[labelArr[i]] = 1 #对字典依据值进行降序排序 classSort = sorted(classDict.items(), key=lambda x: x[1], reverse=True) #返回最大一项的标签，即占数目最多的标签 return classSort[0][0]def calc_H_D(trainLabelArr): ''' 计算数据集D的经验熵，参考公式5.7 经验熵的计算 :param trainLabelArr:当前数据集的标签集 :return: 经验熵 ''' #初始化为0 H_D = 0 #将当前所有标签放入集合中，这样只要有的标签都会在集合中出现，且出现一次。 #遍历该集合就可以遍历所有出现过的标记并计算其Ck #这么做有一个很重要的原因：首先假设一个背景，当前标签集中有一些标记已经没有了，比如说标签集中 #没有0（这是很正常的，说明当前分支不存在这个标签）。 式5.7中有一项Ck，那按照式中的针对不同标签k #计算Cl和D并求和时，由于没有0，那么C0=0，此时C0/D0=0,log2(C0/D0) = log2(0)，事实上0并不在log的 #定义区间内，出现了问题 #所以使用集合的方式先知道当前标签中都出现了那些标签，随后对每个标签进行计算，如果没出现的标签那一项就 #不在经验熵中出现（未参与，对经验熵无影响），保证log的计算能一直有定义 trainLabelSet = set([label for label in trainLabelArr]) #遍历每一个出现过的标签 for i in trainLabelSet: #计算|Ck|/|D| #trainLabelArr == i：当前标签集中为该标签的的位置 #例如a = [1, 0, 0, 1], c = (a == 1): c == [True, false, false, True] #trainLabelArr[trainLabelArr == i]：获得为指定标签的样本 #trainLabelArr[trainLabelArr == i].size：获得为指定标签的样本的大小，即标签为i的样本 #数量，就是|Ck| #trainLabelArr.size：整个标签集的数量（也就是样本集的数量），即|D| p = trainLabelArr[trainLabelArr == i].size / trainLabelArr.size #对经验熵的每一项累加求和 H_D += -1 * p * np.log2(p) #返回经验熵 return H_Ddef calcH_D_A(trainDataArr_DevFeature, trainLabelArr): ''' 计算经验条件熵 :param trainDataArr_DevFeature:切割后只有feature那列数据的数组 :param trainLabelArr: 标签集数组 :return: 经验条件熵 ''' #初始为0 H_D_A = 0 #在featue那列放入集合中，是为了根据集合中的数目知道该feature目前可取值数目是多少 trainDataSet = set([label for label in trainDataArr_DevFeature]) #对于每一个特征取值遍历计算条件经验熵的每一项 for i in trainDataSet: #计算H(D|A) #trainDataArr_DevFeature[trainDataArr_DevFeature == i].size / trainDataArr_DevFeature.size:|Di| / |D| #calc_H_D(trainLabelArr[trainDataArr_DevFeature == i]):H(Di) H_D_A += trainDataArr_DevFeature[trainDataArr_DevFeature == i].size / trainDataArr_DevFeature.size \\ * calc_H_D(trainLabelArr[trainDataArr_DevFeature == i]) #返回得出的条件经验熵 return H_D_Adef calcBestFeature(trainDataList, trainLabelList): ''' 计算信息增益最大的特征 :param trainDataList: 当前数据集 :param trainLabelList: 当前标签集 :return: 信息增益最大的特征及最大信息增益值 ''' #将数据集和标签集转换为数组形式 #trainLabelArr转换后需要转置，这样在取数时方便 #例如a = np.array([1, 2, 3]); b = np.array([1, 2, 3]).T #若不转置，a[0] = [1, 2, 3]，转置后b[0] = 1, b[1] = 2 #对于标签集来说，能够很方便地取到每一位是很重要的 trainDataArr = np.array(trainDataList) trainLabelArr = np.array(trainLabelList).T #获取当前特征数目，也就是数据集的横轴大小 featureNum = trainDataArr.shape[1] #初始化最大信息增益 maxG_D_A = -1 #初始化最大信息增益的特征 maxFeature = -1 #对每一个特征进行遍历计算 for feature in range(featureNum): #“5.2.2 信息增益”中“算法5.1（信息增益的算法）”第一步： #1.计算数据集D的经验熵H(D) H_D = calc_H_D(trainLabelArr) #2.计算条件经验熵H(D|A) #由于条件经验熵的计算过程中只涉及到标签以及当前特征，为了提高运算速度（全部样本 #做成的矩阵运算速度太慢，需要剔除不需要的部分），将数据集矩阵进行切割 #数据集在初始时刻是一个Arr = 60000*784的矩阵，针对当前要计算的feature，在训练集中切割下 #Arr[:, feature]这么一条来，因为后续计算中数据集中只用到这个（没明白的跟着算一遍例5.2） #trainDataArr[:, feature]:在数据集中切割下这么一条 #trainDataArr[:, feature].flat：将这么一条转换成竖着的列表 #np.array(trainDataArr[:, feature].flat)：再转换成一条竖着的矩阵，大小为60000*1（只是初始是 #这么大，运行过程中是依据当前数据集大小动态变的） trainDataArr_DevideByFeature = np.array(trainDataArr[:, feature].flat) #3.计算信息增益G(D|A) G(D|A) = H(D) - H(D | A) G_D_A = H_D - calcH_D_A(trainDataArr_DevideByFeature, trainLabelArr) #不断更新最大的信息增益以及对应的feature if G_D_A &gt; maxG_D_A: maxG_D_A = G_D_A maxFeature = feature return maxFeature, maxG_D_Adef getSubDataArr(trainDataArr, trainLabelArr, A, a): ''' 更新数据集和标签集 :param trainDataArr:要更新的数据集 :param trainLabelArr: 要更新的标签集 :param A: 要去除的特征索引 :param a: 当data[A]== a时，说明该行样本时要保留的 :return: 新的数据集和标签集 ''' #返回的数据集 retDataArr = [] #返回的标签集 retLabelArr = [] #对当前数据的每一个样本进行遍历 for i in range(len(trainDataArr)): #如果当前样本的特征为指定特征值a if trainDataArr[i][A] == a: #那么将该样本的第A个特征切割掉，放入返回的数据集中 retDataArr.append(trainDataArr[i][0:A] + trainDataArr[i][A+1:]) #将该样本的标签放入返回标签集中 retLabelArr.append(trainLabelArr[i]) #返回新的数据集和标签集 return retDataArr, retLabelArrdef createTree(*dataSet): ''' 递归创建决策树 :param dataSet:(trainDataList， trainLabelList) &lt;&lt;-- 元祖形式 :return:新的子节点或该叶子节点的值 ''' #设置Epsilon，“5.3.1 ID3算法”第4步提到需要将信息增益与阈值Epsilon比较，若小于则 #直接处理后返回T #该值的大小在设置上并未考虑太多，观察到信息增益前期在运行中为0.3左右，所以设置了0.1 Epsilon = 0.1 #从参数中获取trainDataList和trainLabelList #之所以使用元祖作为参数，是由于后续递归调用时直数据集需要对某个特征进行切割，在函数递归 #调用上直接将切割函数的返回值放入递归调用中，而函数的返回值形式是元祖的，等看到这个函数 #的底部就会明白了，这样子的用处就是写程序的时候简洁一点，方便一点 trainDataList = dataSet[0][0] trainLabelList = dataSet[0][1] #打印信息：开始一个子节点创建，打印当前特征向量数目及当前剩余样本数目 print('start a node', len(trainDataList[0]), len(trainLabelList)) #将标签放入一个字典中，当前样本有多少类，在字典中就会有多少项 #也相当于去重，多次出现的标签就留一次。举个例子，假如处理结束后字典的长度为1，那说明所有的样本 #都是同一个标签，那就可以直接返回该标签了，不需要再生成子节点了。 classDict = &#123;i for i in trainLabelList&#125; #如果D中所有实例属于同一类Ck，则置T为单节点数，并将Ck作为该节点的类，返回T #即若所有样本的标签一致，也就不需要再分化，返回标记作为该节点的值，返回后这就是一个叶子节点 if len(classDict) == 1: #因为所有样本都是一致的，在标签集中随便拿一个标签返回都行，这里用的第0个（因为你并不知道 #当前标签集的长度是多少，但运行中所有标签只要有长度都会有第0位。 return trainLabelList[0] #如果A为空集，则置T为单节点数，并将D中实例数最大的类Ck作为该节点的类，返回T #即如果已经没有特征可以用来再分化了，就返回占大多数的类别 if len(trainDataList[0]) == 0: #返回当前标签集中占数目最大的标签 return majorClass(trainLabelList) #否则，按式5.10计算A中个特征值的信息增益，选择信息增益最大的特征Ag Ag, EpsilonGet = calcBestFeature(trainDataList, trainLabelList) #如果Ag的信息增益比小于阈值Epsilon，则置T为单节点树，并将D中实例数最大的类Ck #作为该节点的类，返回T if EpsilonGet &lt; Epsilon: return majorClass(trainLabelList) #否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的 # 类作为标记，构建子节点，由节点及其子节点构成树T，返回T treeDict = &#123;Ag:&#123;&#125;&#125; #特征值为0时，进入0分支 #getSubDataArr(trainDataList, trainLabelList, Ag, 0)：在当前数据集中切割当前feature，返回新的数据集和标签集 treeDict[Ag][0] = createTree(getSubDataArr(trainDataList, trainLabelList, Ag, 0)) treeDict[Ag][1] = createTree(getSubDataArr(trainDataList, trainLabelList, Ag, 1)) return treeDictdef predict(testDataList, tree): ''' 预测标签 :param testDataList:样本 :param tree: 决策树 :return: 预测结果 ''' # treeDict = copy.deepcopy(tree) #死循环，直到找到一个有效地分类 while True: #因为有时候当前字典只有一个节点 #例如&#123;73: &#123;0: &#123;74:6&#125;&#125;&#125;看起来节点很多，但是对于字典的最顶层来说，只有73一个key，其余都是value #若还是采用for来读取的话不太合适，所以使用下行这种方式读取key和value (key, value), = tree.items() #如果当前的value是字典，说明还需要遍历下去 if type(tree[key]).__name__ == 'dict': #获取目前所在节点的feature值，需要在样本中删除该feature #因为在创建树的过程中，feature的索引值永远是对于当时剩余的feature来设置的 #所以需要不断地删除已经用掉的特征，保证索引相对位置的一致性 dataVal = testDataList[key] del testDataList[key] #将tree更新为其子节点的字典 tree = value[dataVal] #如果当前节点的子节点的值是int，就直接返回该int值 #例如&#123;403: &#123;0: 7, 1: &#123;297:7&#125;&#125;，dataVal=0 #此时上一行tree = value[dataVal]，将tree定位到了7，而7不再是一个字典了， #这里就可以直接返回7了，如果tree = value[1]，那就是一个新的子节点，需要继续遍历下去 if type(tree).__name__ == 'int': #返回该节点值，也就是分类值 return tree else: #如果当前value不是字典，那就返回分类值 return valuedef test(testDataList, testLabelList, tree): ''' 测试准确率 :param testDataList:待测试数据集 :param testLabelList: 待测试标签集 :param tree: 训练集生成的树 :return: 准确率 ''' #错误次数计数 errorCnt = 0 #遍历测试集中每一个测试样本 for i in range(len(testDataList)): #判断预测与标签中结果是否一致 if testLabelList[i] != predict(testDataList[i], tree): errorCnt += 1 #返回准确率 return 1 - errorCnt / len(testDataList)if __name__ == '__main__': #开始时间 start = time.time() # 获取训练集 trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv') # 获取测试集 testDataList, testLabelList = loadData('../Mnist/mnist_test.csv') #创建决策树 print('start create tree') tree = createTree((trainDataList, trainLabelList)) print('tree is:', tree) #测试准确率 print('start test') accur = test(testDataList, testLabelList, tree) print('the accur is:', accur) #结束时间 end = time.time() print('time span:', end - start)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"决策树","slug":"机器学习/决策树","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"https://ericzikun.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"决策树","slug":"决策树","permalink":"https://ericzikun.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"}]},{"title":"感知机、KNN","slug":"感知机、KNN","date":"2019-11-08T02:12:13.000Z","updated":"2020-04-09T06:11:12.826Z","comments":true,"path":"2019/11/08/感知机、KNN/","link":"","permalink":"https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/","excerpt":"","text":"前言：参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！代码来自：https://www.pkudodo.com （一）感知机定义：感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础。 个人理解：结合看过的《深度学习入门基于python的理论与实现》，感知机说白了就是接受一些信号，输出信号的模型（就像理工科电工科中讲到的逻辑电路一个道理），多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用，也就是说，权重越大，对应该权重的信号的重要性就越高。那么，有同学就疑问了，为什么是线性呢，非线性不能吗，这里可以看看两张图： 用一条直线是可以将图1正常分割开，而无法将第二张图分割，第一张图在编程实现时用到的是简单的逻辑电路（与门、与非门、或门），但是第二张图这种异或门只能通过多层感知机，也就是神经网络才能够实现。 感知机的几何解释:模型公式:$f(x)=sign(w\\cdot x+b)$$w$叫作权值向量,$b$叫做偏置,$sign$是符号函数.$w\\cdot x+b$对应于特征空间中的一个分离超平面$S$,其中$w$是$S$的法向量,$b$是$S$的截距.$S$将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.策略:假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面$S$的总距离。因为误分类点到超平面S的距离是$\\frac{1}{||w||}|w\\cdot{x_0}+b|$.且对于误分类的数据来说,总有:$-y_i(w\\cdot{x_i}+b)&gt;0$成立,因此不考虑$\\frac{1}{||w||}$,就得到感知机的损失函数:$L(w,b)=-\\sum_{x_i\\in{M}} y_i(w\\cdot{x_i}+b)$,其中$M$是误分类点的集合.感知机学习的策略就是选取使损失函数最小的模型参数. 算法:感知机的最优化方法采用随机梯度下降法.首先任意选取一个超平面$w_0$,$b_0$,然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新$w,b$,直到损失函数为0:$$w\\longleftarrow w+\\eta y_ix_i$$$$b\\longleftarrow b+\\eta y_i$$其中$η$表示步长.该算法的直观解释是:当一个点被误分类,就调整$w,b$使分离超平面向该误分类点接近.感知机的解可以不同. 对偶形式: 假设原始形式中的$w_0$和$b_0$均为0,设逐步修改$w$和$b$共$n$次,令$a=nη$,最后学习到的$w,b$可以表示为$w=\\sum_{i=1}^{N}\\alpha y_i x_i,$.那么对偶算法就变为设初始a和b均为0,每次选取数据更新a和b直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度 代码：参考代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148#coding=utf-8#Author:Dodo#Date:2018-11-15#Email:lvtengchao@pku.edu.cn'''数据集：Mnist训练集数量：60000测试集数量：10000------------------------------运行结果：正确率：81.72%（二分类）运行时长：78.6s'''import numpy as npimport timedef loadData(fileName): ''' 加载Mnist数据集 :param fileName:要加载的数据集路径 :return: list形式的数据集及标记 ''' print('start to read data') # 存放数据及标记的list dataArr = []; labelArr = [] # 打开文件 fr = open(fileName, 'r') # 将文件按行读取 for line in fr.readlines(): # 对每一行数据按切割福','进行切割，返回字段列表 curLine = line.strip().split(',') # Mnsit有0-9是个标记，由于是二分类任务，所以将&gt;=5的作为1，&lt;5为-1 if int(curLine[0]) &gt;= 5: labelArr.append(1) else: labelArr.append(-1) #存放标记 #[int(num) for num in curLine[1:]] -&gt; 遍历每一行中除了以第一哥元素（标记）外将所有元素转换成int类型 #[int(num)/255 for num in curLine[1:]] -&gt; 将所有数据除255归一化(非必须步骤，可以不归一化) dataArr.append([int(num)/255 for num in curLine[1:]]) #返回data和label return dataArr, labelArrdef perceptron(dataArr, labelArr, iter=50): ''' 感知器训练过程 :param dataArr:训练集的数据 (list) :param labelArr: 训练集的标签(list) :param iter: 迭代次数，默认50 :return: 训练好的w和b ''' print('start to trans') #将数据转换成矩阵形式（在机器学习中因为通常都是向量的运算，转换成矩阵形式方便运算） #转换后的数据中每一个样本的向量都是横向的 dataMat = np.mat(dataArr) #将标签转换成矩阵，之后转置(.T为转置)。 #转置是因为在运算中需要单独取label中的某一个元素，如果是1xN的矩阵的话，无法用label[i]的方式读取 #对于只有1xN的label可以不转换成矩阵，直接label[i]即可，这里转换是为了格式上的统一 labelMat = np.mat(labelArr).T #获取数据矩阵的大小，为m*n m, n = np.shape(dataMat) #创建初始权重w，初始值全为0。 #np.shape(dataMat)的返回值为m，n -&gt; np.shape(dataMat)[1])的值即为n，与 #样本长度保持一致 w = np.zeros((1, np.shape(dataMat)[1]))# 初始化权重w为1*N的0矩阵 #初始化偏置b为0 b = 0 #初始化步长，也就是梯度下降过程中的n，控制梯度下降速率 h = 0.0001 #进行iter次迭代计算 for k in range(iter): #对于每一个样本进行梯度下降 #李航书中在2.3.1开头部分使用的梯度下降，是全部样本都算一遍以后，统一 #进行一次梯度下降 #在2.3.1的后半部分可以看到（例如公式2.6 2.7），求和符号没有了，此时用 #的是随机梯度下降，即计算一个样本就针对该样本进行一次梯度下降。 #两者的差异各有千秋，但较为常用的是随机梯度下降。 for i in range(m): #获取当前样本的向量 xi = dataMat[i] #获取当前样本所对应的标签 yi = labelMat[i] #判断是否是误分类样本 #误分类样本特征为： -yi(w*xi+b)&gt;=0，详细可参考书中2.2.2小节 #在书的公式中写的是&gt;0，实际上如果=0，说明改点在超平面上，也是不正确的 if -1 * yi * (w * xi.T + b) &gt;= 0: #对于误分类样本，进行梯度下降，更新w和b w = w + h * yi * xi b = b + h * yi #打印训练进度 print('Round %d:%d training' % (k, iter)) #返回训练完的w、b return w, bdef test(dataArr, labelArr, w, b): ''' 测试准确率 :param dataArr:测试集 :param labelArr: 测试集标签 :param w: 训练获得的权重w :param b: 训练获得的偏置b :return: 正确率 ''' print('start to test') #将数据集转换为矩阵形式方便运算 dataMat = np.mat(dataArr) #将label转换为矩阵并转置，详细信息参考上文perceptron中 #对于这部分的解说 labelMat = np.mat(labelArr).T #获取测试数据集矩阵的大小 m, n = np.shape(dataMat) #错误样本数计数 errorCnt = 0 #遍历所有测试样本 for i in range(m): #获得单个样本向量 xi = dataMat[i] #获得该样本标记 yi = labelMat[i] #获得运算结果 result = -1 * yi * (w * xi.T + b) #如果-yi(w*xi+b)&gt;=0，说明该样本被误分类，错误样本数加一 if result &gt;= 0: errorCnt += 1 #正确率 = 1 - （样本分类错误数 / 样本总数） accruRate = 1 - (errorCnt / m) #返回正确率 return accruRateif __name__ == '__main__': #获取当前时间 #在文末同样获取当前时间，两时间差即为程序运行时间 start = time.time() #获取训练集及标签 trainData, trainLabel = loadData('./mnist_train.csv') #获取测试集及标签 testData, testLabel = loadData('./mnist_test.csv') #训练获得权重 w, b = perceptron(trainData, trainLabel, iter = 30) #进行测试，获得正确率 accruRate = test(testData, testLabel, w, b) #获取当前时间，作为结束时间 end = time.time() #显示正确率 print('accuracy rate is:', accruRate) #显示用时时长 print('time span:', end - start) （二）K-邻近定义：$k$近邻法根据其$k$个最邻的训练实例的类别,通过多数表决等方式进行预测.什么是多数表决？我们为了对样本$x$进行归类，通过它周围最近的$k$个点来“投票”，这$k$个点大多数是哪个类型的，则定样本$x$为这个类型，故称为多数表决 模型说明:(1)训练集（样本$x$以及样本$x$对应的label:$y$）(2)距离度量(欧氏距离or曼哈顿距离) 特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用欧氏距离,也可以使用曼哈顿距离.欧式距离：曼哈顿距离：(3)k值 k值较小时,整体模型变得复杂,容易发生过拟合;k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k. 但是K邻近算法也有其局限性： 在预测样本类别时，待预测样本需要与训练集中所有样本计算距离，当训练集数量过高时（例如Mnsit训练集有60000个样本），每预测一个样本都要计算60000个距离，计算代价过高，尤其当测试集数目也较大时（Mnist测试集有10000个）。 K近邻在高维情况下时（高维在机器学习中并不少见），待预测样本需要与依次与所有样本求距离。向量维度过高时使得欧式距离的计算变得不太迅速了。本文在60000训练集的情况下，将10000个测试集缩减为200个，整个过程仍然需要308秒（曼哈顿距离为246秒，但准确度大幅下降）。 使用欧氏距离还是曼哈顿距离，性能上的差别相对来说不是很大，说明欧式距离并不是制约计算速度的主要方式。最主要的是训练集的大小，每次预测都需要与60000个样本进行比对，同时选出距离最近的$k$项 代码：参考代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175#coding=utf-8#Author:Dodo#Date:2018-11-16#Email:lvtengchao@pku.edu.cn'''数据集：Mnist训练集数量：60000测试集数量：10000（实际使用：200）------------------------------运行结果：（邻近k数量：25）向量距离使用算法——欧式距离 正确率：97% 运行时长：308s向量距离使用算法——曼哈顿距离 正确率：14% 运行时长：246s'''import numpy as npimport timedef loadData(fileName): ''' 加载文件 :param fileName:要加载的文件路径 :return: 数据集和标签集 ''' print('start read file') #存放数据及标记 dataArr = []; labelArr = [] #读取文件 fr = open(fileName) #遍历文件中的每一行 for line in fr.readlines(): #获取当前行，并按“，”切割成字段放入列表中 #strip：去掉每行字符串首尾指定的字符（默认空格或换行符） #split：按照指定的字符将字符串切割成每个字段，返回列表形式 curLine = line.strip().split(',') #将每行中除标记外的数据放入数据集中（curLine[0]为标记信息） #在放入的同时将原先字符串形式的数据转换为整型 dataArr.append([int(num) for num in curLine[1:]]) #将标记信息放入标记集中 #放入的同时将标记转换为整型 labelArr.append(int(curLine[0])) #返回数据集和标记 return dataArr, labelArrdef calcDist(x1, x2): ''' 计算两个样本点向量之间的距离 使用的是欧氏距离，即 样本点每个元素相减的平方 再求和 再开方 欧式举例公式这里不方便写，可以百度或谷歌欧式距离（也称欧几里得距离） :param x1:向量1 :param x2:向量2 :return:向量之间的欧式距离 ''' return np.sqrt(np.sum(np.square(x1 - x2))) #马哈顿距离计算公式 # return np.sum(x1 - x2)def getClosest(trainDataMat, trainLabelMat, x, topK): ''' 预测样本x的标记。 获取方式通过找到与样本x最近的topK个点，并查看它们的标签。 查找里面占某类标签最多的那类标签 （书中3.1 3.2节） :param trainDataMat:训练集数据集 :param trainLabelMat:训练集标签集 :param x:要预测的样本x :param topK:选择参考最邻近样本的数目（样本数目的选择关系到正确率，详看3.2.3 K值的选择） :return:预测的标记 ''' #建立一个存放向量x与每个训练集中样本距离的列表 #列表的长度为训练集的长度，distList[i]表示x与训练集中第 ## i个样本的距离 distList = [0] * len(trainLabelMat) #遍历训练集中所有的样本点，计算与x的距离 for i in range(len(trainDataMat)): #获取训练集中当前样本的向量 x1 = trainDataMat[i] #计算向量x与训练集样本x的距离 curDist = calcDist(x1, x) #将距离放入对应的列表位置中 distList[i] = curDist #对距离列表进行排序 #argsort：函数将数组的值从小到大排序后，并按照其相对应的索引值输出 #例如： # &gt;&gt;&gt; x = np.array([3, 1, 2]) # &gt;&gt;&gt; np.argsort(x) # array([1, 2, 0]) #返回的是列表中从小到大的元素索引值，对于我们这种需要查找最小距离的情况来说很合适 #array返回的是整个索引值列表，我们通过[:topK]取列表中前topL个放入list中。 #----------------优化点------------------- #由于我们只取topK小的元素索引值，所以其实不需要对整个列表进行排序，而argsort是对整个 #列表进行排序的，存在时间上的浪费。字典有现成的方法可以只排序top大或top小，可以自行查阅 #对代码进行稍稍修改即可 #这里没有对其进行优化主要原因是KNN的时间耗费大头在计算向量与向量之间的距离上，由于向量高维 #所以计算时间需要很长，所以如果要提升时间，在这里优化的意义不大。（当然不是说就可以不优化了， #主要是我太懒了） topKList = np.argsort(np.array(distList))[:topK] #升序排序 #建立一个长度时的列表，用于选择数量最多的标记 #3.2.4提到了分类决策使用的是投票表决，topK个标记每人有一票，在数组中每个标记代表的位置中投入 #自己对应的地方，随后进行唱票选择最高票的标记 labelList = [0] * 10 #对topK个索引进行遍历 for index in topKList: #trainLabelMat[index]：在训练集标签中寻找topK元素索引对应的标记 #int(trainLabelMat[index])：将标记转换为int（实际上已经是int了，但是不int的话，报错） #labelList[int(trainLabelMat[index])]：找到标记在labelList中对应的位置 #最后加1，表示投了一票 labelList[int(trainLabelMat[index])] += 1 #max(labelList)：找到选票箱中票数最多的票数值 #labelList.index(max(labelList))：再根据最大值在列表中找到该值对应的索引，等同于预测的标记 return labelList.index(max(labelList))def test(trainDataArr, trainLabelArr, testDataArr, testLabelArr, topK): ''' 测试正确率 :param trainDataArr:训练集数据集 :param trainLabelArr: 训练集标记 :param testDataArr: 测试集数据集 :param testLabelArr: 测试集标记 :param topK: 选择多少个邻近点参考 :return: 正确率 ''' print('start test') #将所有列表转换为矩阵形式，方便运算 trainDataMat = np.mat(trainDataArr); trainLabelMat = np.mat(trainLabelArr).T testDataMat = np.mat(testDataArr); testLabelMat = np.mat(testLabelArr).T #错误值技术 errorCnt = 0 #遍历测试集，对每个测试集样本进行测试 #由于计算向量与向量之间的时间耗费太大，测试集有6000个样本，所以这里人为改成了 #测试200个样本点，如果要全跑，将行注释取消，再下一行for注释即可，同时下面的print #和return也要相应的更换注释行 # for i in range(len(testDataMat)): for i in range(200): # print('test %d:%d'%(i, len(trainDataArr))) print('test %d:%d' % (i, 200)) #读取测试集当前测试样本的向量 x = testDataMat[i] #获取预测的标记 y = getClosest(trainDataMat, trainLabelMat, x, topK) #如果预测标记与实际标记不符，错误值计数加1 if y != testLabelMat[i]: errorCnt += 1 #返回正确率 # return 1 - (errorCnt / len(testDataMat)) return 1 - (errorCnt / 200)if __name__ == \"__main__\": start = time.time() #获取训练集 trainDataArr, trainLabelArr = loadData('./mnist_train.csv') #获取测试集 testDataArr, testLabelArr = loadData('./mnist_test.csv') #计算测试集正确率 accur = test(trainDataArr, trainLabelArr, testDataArr, testLabelArr, 25) #打印正确率 print('accur is:%d'%(accur * 100), '%') end = time.time() #显示花费时间print('time span:', end - start)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"感知机&KNN","slug":"机器学习/感知机-KNN","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA-KNN/"}],"tags":[{"name":"感知机","slug":"感知机","permalink":"https://ericzikun.github.io/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"name":"KNN","slug":"KNN","permalink":"https://ericzikun.github.io/tags/KNN/"}]},{"title":"NLP入门实战之——基于词频和TF-IDF，利用朴素贝叶斯机器学习方法新闻分类","slug":"NLP入门实战之——基于词频和TF-IDF，利用朴素贝叶斯机器学习方法新闻分类","date":"2019-11-07T06:18:08.000Z","updated":"2020-04-09T07:14:21.722Z","comments":true,"path":"2019/11/07/NLP入门实战之——基于词频和TF-IDF，利用朴素贝叶斯机器学习方法新闻分类/","link":"","permalink":"https://ericzikun.github.io/2019/11/07/NLP%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E4%B9%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E8%AF%8D%E9%A2%91%E5%92%8CTF-IDF%EF%BC%8C%E5%88%A9%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB/","excerpt":"","text":"本人是零基础的小白，现在从零开始学习NLP，这是学习的一些简单的笔记，如有错误请指正。编译环境：Jupyter NotebookWindows x64本文数据处理主要分为两个板块：一 是数据预处理（Data Preparation）从而获得所需要的特征（feature），如将数据层层处理（分词、停用词过滤、向量化），本文向量化内容由于使用sklearn库，放置第二板块讲解。二 是利用模型（Modeling）解决具体的问题，本文主要采用朴素贝叶斯经典机器学习方法对文本进行分类。 TOC 一、理论基础下面简单回顾一下理论部分（可以直接跳过到实战部分） 1.1 词频(TF)词频（term frequency） 指的是某一个给定的词语在该文件中出现的频率。对于在某一文件里的词语$t_i$来说，它的重要性可表示为：$$ tf_{ij}=\\frac{n_{i,j}}{\\sum_kn_{k,j}} $$其中，$n_{i,j}$是该词在文件$d_j$中出现次数，而分母是文件$d_j$中所有字词出现的次数总和。 1.2 逆向文本频率（IDF）逆向文件频率（inverse document frequency） 是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到，个人理解为：对词频向量的改进，原因在于：词语出现的越多，并不能代表它就越重要，相反，文档中出现的越多，其实它的重要性是降低的，所以TFIDF考虑了单词的重要性而做的对词频的改进，可表示为：$$ tfidf(w)=tf(d,w)\\times{idf(w)}$$（1）其中 $tf(d,w)$ 代表文档d中w的词频（2）$idf(w)=\\log\\frac{N}{N(w)}$，${N}$代表语料库中的文档总数，${N(w)}$代表词语w出现在多少个文档中，出现在文档的次数越多，$\\log$值越小，故称为逆向文本频率 1.3 朴素贝叶斯（Naive Bayesian Model，NBM）朴素贝叶斯的中心思想，在于利用各类别在训练样本中的分布以及类别中各特征元素的分布，计算后验概率，使用极大似然法判断测试样本所属,一般用于简单分类。贝叶斯公式：$$P(B\\mid{A})=\\frac{P(A\\mid{B})P(B)}{P(A)}$$对应分类任务则为：$$P(类别\\mid{特征})=\\frac{P(特征\\mid{类别})P(类别)}{P(特征)}$$垃圾邮件分类（判别模型）举例：$P(特征\\mid{类别})$ 相当于先验概率，也就是我们已知的概率，比如垃圾邮件分类里面，我们已有的数据中正常的类别邮件里面包含“购买”一词的概率，以及垃圾类别里面包含“购买”一次的概率等，$P(类别)$ 就是正常或者垃圾邮件在数据集中的概率，这些概率都已知。那么要判断邮件为正常还是垃圾，则要判断： $P(正常\\mid内容)$ 与 $P(垃圾\\mid内容)$ 的大小 $$P(正常\\mid内容)=\\frac{P(内容\\mid正常)P(正常)}{P(内容)}$$$$P(垃圾\\mid内容)=\\frac{P(内容\\mid垃圾)P(垃圾)}{P(内容)}$$$P(正常)$，$P(垃圾)$ 均已知，$P(内容)$消去，剩下就是要比较$P(内容\\mid正常)$ 和$P(内容\\mid垃圾)$$P(内容\\mid正常)=P(购买，物品，广告，产品\\mid正常)\\=P(购买\\mid正常)P(物品\\mid正常)P(广告\\mid正常)P(产品\\mid正常)$，而这些先验概率前面都已算过，带入计算作比较大小即可。 二、数据预处理数据预处理部分可谓是耗费了大部分的时间，参考了一些博客，但是感觉不是特别详细，其中也遇到了不少麻烦，下面一一讲解到位，非常适合小白参考。 2.1 数据下载及导入首先下载搜狗实验室的文本数据（精简版347MB，tar.gz格式）：下载链接![在这里插入图片描述](https://img-blog.csdnimg.cn/20191122154242974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center =500x80)解压后，得到如下128个txt文件文件格式如下：对于特定格式的文本，我们一般采用正则表达式来提取所需要的信息，代码如下： 1234567891011121314151617181920212223242526272829import reimport osimport pandas as pdimport pickleimport numpy as npimport jieba;# 定义正则表达式patternURL = re.compile(r'&lt;url&gt;(.*?)&lt;/url&gt;', re.S)patternCtt = re.compile(r'&lt;content&gt;(.*?)&lt;/content&gt;', re.S)contents_total = []urls_total=[]labels = []# os.listdir()返回文件夹里所有文件名file = os.listdir(\"C:/Users/84747/Desktop/新建文件夹/SogouCS.reduced\")for i in range(len(file)): file0=file[i] file_path = os.path.join(\"C:/Users/84747/Desktop/新建文件夹/SogouCS.reduced/\", file0)# os.path.join()将路径进行拼接，从而打开每一个txt文件 text = open(file_path, 'rb').read().decode(\"gbk\", 'ignore') # 正则匹配出url和content urls = patternURL.findall(text) contents = patternCtt.findall(text)# 得到所有contents和urls urls_total=urls_total + urls contents_total = contents_total + contentsdf=pd.DataFrame(&#123;'URL':urls_total,'content':contents_total&#125;)#将目前处理的数据用dataframe可视化一下，方便查错df.head() # 显示dataframe的前五行 结果如下（有空值、内容也很乱），后面一步步处理：下面我们再将URL内容再次正则一下，提取官方的分类label： 123456labels=[]for i in range(0,len(urls_total)): patternClass = re.compile(r'http://(.*?).sohu.com', re.S) labels.append(patternClass.findall(urls_total[i]))df=pd.DataFrame(&#123;'label':labels,'URL':urls_total,'content':contents_total&#125;).dropna()df.head() #如果想显示最后五行可用.tail() 其中传统dataframe中dropna() 函数删空值的方法在这里并不适用，结果如下,待会会处理，我们先把label里面的格式调整一下，调整的原因：目前的label格式为list of list，为了方便后面筛选label来替换中文等后续操作，先脱去一层list： 1234567type(labels)# print(labels[0:100])labels2 = []for index in range(len(labels)): labels2.append(' '.join(labels[index])) #将list of list转换为listlabels2[0:100] df.label.unique() 12df=pd.DataFrame(&#123;'label':labels2,'URL':urls_total,'content':contents_total&#125;)df.tail() 好了，到这里label格式已经调好了，接下来需要对label进行中文替换，所以我们需要先把各类label筛选出来，总共有以下label： 1print(df.label.unique()) #将所有不重复的label显示出来 将所需要的label对应的内容进行筛选查看（替换‘career’为各个label，查看相关内容），方便人为辨识类别代码如下： 1df.loc[df['label']== 'career'].tail(20) 接下来就是替换label，通过人为的观察上述各label所对应的分类，将中文替换到下列map映射之中，最后完成label替换： 12345label_mapping=&#123;'sports':'体育', 'house':'房屋','it':'科技', '2008':'奥运', 'women':'女人',\\ 'auto':'汽车','yule':'娱乐', 'news':'时事','learning':'教育', 'business':'财经',\\ 'mil.news':'军事', 'travel':'旅游', 'health':'健康', 'cul':'文化', 'career':'职场'&#125;df['label'] = df['label'].map(label_mapping) #将label进行替换df.head() 回到刚刚提到的空值问题，明明有很多空值，但isnull()查阅后仍然显示false，原因在于：pandas里空值是指NA，包括numpy的np.nan,python的None，pandas对空值进行操作可以用isnull／notnull／isna／notna／fillna／dropna等等，但是，这些操作对空字符串均无效（此处参考链接）。空字符串即“ ”（一个或多个空格），但在excel表格里其实是看不出来，pandas也把它当成有值进行操作。代码如下： 12df.content.replace(to_replace=r'^\\s*$',value=np.nan,regex=True,inplace=True)df.head() 这样一来，就将空值转换成了NaN，从而再可以使用dropna()。 12df2=df.dropna(axis=0, how='any') # 对任意含有NaN的行（axis=0）进行删除df2.head() 再将索引重新排列一下： 12df3=df2.reset_index(drop=True)df3.head() 2.2 结巴分词及停用词过滤此处我没有用前面的数据进行处理（毕竟有42w行数据，作为新手使用小数据集练手足够，后面可能还会发42w行的运行结果，这里采用了前辈整理好的5000行数据进行处理），格式和我之前处理得到的基本一致，不影响大家参考。样例数据导入： 123456789import gensimimport numpyimport pandas as pdimport jieba#python -m pip install --user gensim (gensim包)#pip install jiebadf_news = pd.read_table('./val.txt',names=['category','theme','URL','content'],encoding='utf-8')print(df_news.head())print(df_news.shape) #数据类型 2.2.1 结巴分词：分词之前首先我们要将dataframe的格式转换为list才能适应jieba库，代码如下： 12345678content = df_news.content.values.tolist() #将datafrmae中content转化为listcontent_S = [] #对content中内容进行分词for line in content: current_segment = jieba.lcut(line) if len(current_segment) &gt; 1 and current_segment != '\\r\\n': #换行符 content_S.append(current_segment)df_content=pd.DataFrame(&#123;'content_S':content_S&#125;) #### 将分完词的list转换为dataframedf_content.head() 2.2.2 停用词过滤：需要先下载好一份停用词表，网上有很多，此处提供前辈整理好的素材，很方便 12topwords=pd.read_csv(\"stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')stopwords.head(15) 1234567891011121314151617181920def drop_stopwords(contents,stopwords): contents_clean = [] all_words = [] for line in contents: line_clean = [] for word in line: if word in stopwords: continue line_clean.append(word) contents_clean.append(line_clean) return contents_clean,all_words #print (contents_clean) contents = df_content.content_S.values.tolist() #df转换为liststopwords = stopwords.stopword.values.tolist() #转换为listcontents_clean,all_words = drop_stopwords(contents,stopwords)df_content=pd.DataFrame(&#123;'contents_clean':contents_clean&#125;) #将分完词的list再转换为dfdf_content.head() 三、模型（modeling）贝叶斯分类器12df_train=pd.DataFrame(&#123;'contents_clean':contents_clean,'label':df_news['category']&#125;)df_train.tail() #tail（）展示最后几个数据（一共是5000个数据） 12345df_train.label.unique()#对label做映射label_mapping = &#123;\"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\":5, \"教育\": 6,\"文化\": 7,\"军事\": 8,\"娱乐\": 9,\"时尚\": 0&#125;df_train['label'] = df_train['label'].map(label_mapping) #将label进行替换df_train.head() 将数据切分为训练集（x_train，y_train）和测试集（x_test，y_test） 1234from sklearn.model_selection import train_test_split#将数据集切分为训练和测试集，x代表content，y代表labelx_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)print(len(x_train),len(x_test),len(y_train),len(y_test)) 3.1 文本数据向量化数据向量化之前，我们先要将类型转换为list以适合CountVectorizer（词频）/TfidfVectorizer（逆向文本频率IDF） 123456789#将x_train（numpy.array型转换为list类型，#以适合CountVectorizer/TfidfVectorizer向量化操作）words = []for line_index in range(len(x_train)): words.append(' '.join(x_train[line_index])) #numpy.array转换为listtest_words = []for line_index in range(len(x_test)): test_words.append(' '.join(x_test[line_index])) 3.1.1 基于词频向量化导入sklearn机器学习库中的CountVectorizer词频向量化函数 1234from sklearn.feature_extraction.text import CountVectorizervec = CountVectorizer(analyzer='word', max_features=4000, lowercase = False) #建立向量vec.fit(words) 导入贝叶斯 1234from sklearn.naive_bayes import MultinomialNB #导入贝叶斯classifier = MultinomialNB()classifier.fit(vec.transform(words), y_train)classifier.score(vec.transform(test_words), y_test) #基于词频向量构造的结果 3.1.2 基于TFIDF向量化12345678910from sklearn.feature_extraction.text import TfidfVectorizer #基于TF-IDF向量vectorizer = TfidfVectorizer(analyzer='word', max_features=4000, lowercase = False)vectorizer.fit(words)# 导入贝叶斯from sklearn.naive_bayes import MultinomialNBclassifier = MultinomialNB()classifier.fit(vectorizer.transform(words), y_train)# 计算分类器精度classifier.score(vectorizer.transform(test_words), y_test) 相比之下，TFIDF向量化的结果会偏高一点点，当然，这里采用的是很小的数据集（才5000行），精度很低，如果将42w的数据进行训练，精度应该会提升不少。到此为止，整个搜狗新闻文本分类任务就完成了。 本文到这里就全部结束了，如果有错误或者引用不当，还请指出，我会加以改进！欢迎大家评论留言，相互学习和进步！（前辈整理的数据集后面会上传到csdn上，如有需要可以联系） 参考文章：https://blog.csdn.net/weixin_43269174/article/details/88634129https://blog.csdn.net/sadfassd/article/details/80568321https://www.jianshu.com/p/edad714110fbhttps://blog.csdn.net/maotianyi941005/article/details/84315965https://www.cnblogs.com/datou-swag/articles/10060532.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"朴素贝叶斯","slug":"机器学习/朴素贝叶斯","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"}],"tags":[]}],"categories":[{"name":"技术栈","slug":"技术栈","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/"},{"name":"Java","slug":"技术栈/Java","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Java/"},{"name":"数据结构","slug":"技术栈/数据结构","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"React","slug":"技术栈/React","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/React/"},{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Convlstm","slug":"深度学习/Convlstm","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Convlstm/"},{"name":"技巧","slug":"技巧","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/"},{"name":"文本处理","slug":"技巧/文本处理","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"},{"name":"Transformer","slug":"深度学习/Transformer","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/"},{"name":"Linux","slug":"技巧/Linux","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/Linux/"},{"name":"升学就业","slug":"升学就业","permalink":"https://ericzikun.github.io/categories/%E5%8D%87%E5%AD%A6%E5%B0%B1%E4%B8%9A/"},{"name":"保研","slug":"升学就业/保研","permalink":"https://ericzikun.github.io/categories/%E5%8D%87%E5%AD%A6%E5%B0%B1%E4%B8%9A/%E4%BF%9D%E7%A0%94/"},{"name":"报错","slug":"技巧/报错","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%8A%A5%E9%94%99/"},{"name":"模型可视化","slug":"深度学习/模型可视化","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"朴素贝叶斯","slug":"机器学习/朴素贝叶斯","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"Python","slug":"技术栈/Python","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Python/"},{"name":"提升效率","slug":"技巧/提升效率","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87/"},{"name":"TextCNN","slug":"深度学习/TextCNN","permalink":"https://ericzikun.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN/"},{"name":"论文排版","slug":"技巧/论文排版","permalink":"https://ericzikun.github.io/categories/%E6%8A%80%E5%B7%A7/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"name":"文本抽取","slug":"机器学习/文本抽取","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96/"},{"name":"逻辑回归","slug":"机器学习/逻辑回归","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"决策树","slug":"机器学习/决策树","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"感知机&KNN","slug":"机器学习/感知机-KNN","permalink":"https://ericzikun.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA-KNN/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://ericzikun.github.io/tags/Java/"},{"name":"对象","slug":"对象","permalink":"https://ericzikun.github.io/tags/%E5%AF%B9%E8%B1%A1/"},{"name":"类","slug":"类","permalink":"https://ericzikun.github.io/tags/%E7%B1%BB/"},{"name":"专业课","slug":"专业课","permalink":"https://ericzikun.github.io/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"数据结构","slug":"数据结构","permalink":"https://ericzikun.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"链表","slug":"链表","permalink":"https://ericzikun.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"栈","slug":"栈","permalink":"https://ericzikun.github.io/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"https://ericzikun.github.io/tags/%E9%98%9F%E5%88%97/"},{"name":"css","slug":"css","permalink":"https://ericzikun.github.io/tags/css/"},{"name":"布局","slug":"布局","permalink":"https://ericzikun.github.io/tags/%E5%B8%83%E5%B1%80/"},{"name":"盒子模型","slug":"盒子模型","permalink":"https://ericzikun.github.io/tags/%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"https://ericzikun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Convlstm","slug":"Convlstm","permalink":"https://ericzikun.github.io/tags/Convlstm/"},{"name":"时空预测","slug":"时空预测","permalink":"https://ericzikun.github.io/tags/%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B/"},{"name":"时序","slug":"时序","permalink":"https://ericzikun.github.io/tags/%E6%97%B6%E5%BA%8F/"},{"name":"Lstm","slug":"Lstm","permalink":"https://ericzikun.github.io/tags/Lstm/"},{"name":"CNN","slug":"CNN","permalink":"https://ericzikun.github.io/tags/CNN/"},{"name":"React","slug":"React","permalink":"https://ericzikun.github.io/tags/React/"},{"name":"antd","slug":"antd","permalink":"https://ericzikun.github.io/tags/antd/"},{"name":"nginx反向代理","slug":"nginx反向代理","permalink":"https://ericzikun.github.io/tags/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/"},{"name":"文本处理","slug":"文本处理","permalink":"https://ericzikun.github.io/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"},{"name":"爬虫","slug":"爬虫","permalink":"https://ericzikun.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://ericzikun.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"Transformer","slug":"Transformer","permalink":"https://ericzikun.github.io/tags/Transformer/"},{"name":"源码","slug":"源码","permalink":"https://ericzikun.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://ericzikun.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"效率","slug":"效率","permalink":"https://ericzikun.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"Mac","slug":"Mac","permalink":"https://ericzikun.github.io/tags/Mac/"},{"name":"Linux","slug":"Linux","permalink":"https://ericzikun.github.io/tags/Linux/"},{"name":"终端","slug":"终端","permalink":"https://ericzikun.github.io/tags/%E7%BB%88%E7%AB%AF/"},{"name":"保研","slug":"保研","permalink":"https://ericzikun.github.io/tags/%E4%BF%9D%E7%A0%94/"},{"name":"经验","slug":"经验","permalink":"https://ericzikun.github.io/tags/%E7%BB%8F%E9%AA%8C/"},{"name":"升学，读研","slug":"升学，读研","permalink":"https://ericzikun.github.io/tags/%E5%8D%87%E5%AD%A6%EF%BC%8C%E8%AF%BB%E7%A0%94/"},{"name":"报错","slug":"报错","permalink":"https://ericzikun.github.io/tags/%E6%8A%A5%E9%94%99/"},{"name":"Keras","slug":"Keras","permalink":"https://ericzikun.github.io/tags/Keras/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://ericzikun.github.io/tags/Tensorflow/"},{"name":"预处理","slug":"预处理","permalink":"https://ericzikun.github.io/tags/%E9%A2%84%E5%A4%84%E7%90%86/"},{"name":"格式化文本","slug":"格式化文本","permalink":"https://ericzikun.github.io/tags/%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%96%87%E6%9C%AC/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"https://ericzikun.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"https://ericzikun.github.io/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"学术","slug":"学术","permalink":"https://ericzikun.github.io/tags/%E5%AD%A6%E6%9C%AF/"},{"name":"TextCNN","slug":"TextCNN","permalink":"https://ericzikun.github.io/tags/TextCNN/"},{"name":"Imdb","slug":"Imdb","permalink":"https://ericzikun.github.io/tags/Imdb/"},{"name":"文本分类","slug":"文本分类","permalink":"https://ericzikun.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"论文排版","slug":"论文排版","permalink":"https://ericzikun.github.io/tags/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"name":"Textrank","slug":"Textrank","permalink":"https://ericzikun.github.io/tags/Textrank/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"https://ericzikun.github.io/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/"},{"name":"NLP","slug":"NLP","permalink":"https://ericzikun.github.io/tags/NLP/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"https://ericzikun.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"机器学习","slug":"机器学习","permalink":"https://ericzikun.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"决策树","slug":"决策树","permalink":"https://ericzikun.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"感知机","slug":"感知机","permalink":"https://ericzikun.github.io/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"name":"KNN","slug":"KNN","permalink":"https://ericzikun.github.io/tags/KNN/"}]}