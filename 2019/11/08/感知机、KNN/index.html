<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    

    
    <title>感知机、KNN | Eric’s blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="感知机,KNN" />
    
    <meta name="description" content="前言：参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！代码来自：https:&#x2F;&#x2F;www.pkudodo.com （一）感知机定义：感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础。 个人理解：结合看过的《深度学习入门基于python的">
<meta property="og:type" content="article">
<meta property="og:title" content="感知机、KNN">
<meta property="og:url" content="https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/index.html">
<meta property="og:site_name" content="Eric’s blog">
<meta property="og:description" content="前言：参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！代码来自：https:&#x2F;&#x2F;www.pkudodo.com （一）感知机定义：感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础。 个人理解：结合看过的《深度学习入门基于python的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center">
<meta property="article:published_time" content="2019-11-08T02:12:13.000Z">
<meta property="article:modified_time" content="2020-04-09T06:11:12.826Z">
<meta property="article:author" content="Eric kun">
<meta property="article:tag" content="感知机">
<meta property="article:tag" content="KNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center">
    

    
        <link rel="alternate" href="/" title="Eric’s blog" type="application/atom+xml" />
    

    
        <link rel="icon" href="https://pic2.zhimg.com/80/v2-f19e0e0add10a40489cdb8df576a0f7e_qhd.jpg" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?0d6b4455953d3e1c0917234dfebaa739";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<meta name="generator" content="Hexo 4.2.0"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">主页</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E5%8D%87%E5%AD%A6%E5%B0%B1%E4%B8%9A/">升学就业</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E5%8D%87%E5%AD%A6%E5%B0%B1%E4%B8%9A/%E4%BF%9D%E7%A0%94/">保研</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E5%B7%A7/">技巧</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E5%B7%A7/Linux/">Linux</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E5%B7%A7/%E6%8A%A5%E9%94%99/">报错</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E5%B7%A7/%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87/">提升效率</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E5%B7%A7/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/">文本处理</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E5%B7%A7/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/">论文排版</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/">技术栈</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Java/">Java</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/React/">React</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA-KNN/">感知机&KNN</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96/">文本抽取</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">朴素贝叶斯</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Convlstm/">Convlstm</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TextCNN/">TextCNN</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/">Transformer</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/">模型可视化</a></li></ul></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">联系方式</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>
        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA-KNN/">感知机&KNN</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-感知机、KNN" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        感知机、KNN
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                <span id="busuanzi_container_page_pv" style='display:none' class="article-date">
  <i class="icon-smile icon"></i> 阅读数：<span id="busuanzi_value_page_pv"></span>次
</span>


    <div class="article-date">
      <i class="fa fa-calendar"></i>
      <a href="/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/" class="article-date">
         <time datetime="2019-11-08T02:12:13.000Z" itemprop="datePublished">2019-11-08</time>
      </a>
    </div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/KNN/" rel="tag">KNN</a>, <a class="tag-link" href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/" rel="tag">感知机</a>
    </div>

                

                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <p><strong>前言：</strong><br>参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！<br>代码来自：<br><a href="https://www.pkudodo.com" target="_blank" rel="noopener">https://www.pkudodo.com</a></p>
<h1 id="（一）感知机"><a href="#（一）感知机" class="headerlink" title="（一）感知机"></a>（一）感知机</h1><h2 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h2><p>感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行<strong>线性划分</strong>的分离超平面.是神经网络和支持向量机的基础。</p>
<p>个人理解：结合看过的《深度学习入门基于python的理论与实现》，感知机说白了就是接受一些信号，输出信号的模型（就像理工科电工科中讲到的逻辑电路一个道理），多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用，也就是说，权重越大，对应该权重的信号的重要性就越高。<br><br><img src="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" style="zoom:70%;"><br>那么，有同学就疑问了，为什么是线性呢，非线性不能吗，这里可以看看两张图：<br><img src="https://img-blog.csdnimg.cn/20191127192443953.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" style="zoom:70%;"><br><img src="https://img-blog.csdnimg.cn/20191127192528864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" style="zoom:80%;"></p>
<p>用一条直线是可以将图1正常分割开，而无法将第二张图分割，第一张图在编程实现时用到的是简单的逻辑电路（与门、与非门、或门），但是第二张图这种异或门只能通过多层感知机，也就是神经网络才能够实现。</p>
<p><strong>感知机的几何解释:</strong><br>模型公式:$f(x)=sign(w\cdot x+b)$<br>$w$叫作权值向量,$b$叫做偏置,$sign$是符号函数.<br>$w\cdot x+b$对应于特征空间中的一个分离超平面$S$,其中$w$是$S$的法向量,$b$是$S$的截距.$S$将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.<br><br>策略:<br>假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面$S$的总距离。因为误分类点到超平面S的距离是$\frac{1}{||w||}|w\cdot{x_0}+b|$.且对于误分类的数据来说,总有:$-y_i(w\cdot{x_i}+b)&gt;0$成立,因此不考虑$\frac{1}{||w||}$,就得到感知机的<strong>损失函数</strong>:<br>$L(w,b)=-\sum_{x_i\in{M}} y_i(w\cdot{x_i}+b)$,其中$M$是误分类点的集合.感知机学习的策略就是选取使<strong>损失函数最小的模型参数</strong>.<br></p>
<p>算法:感知机的最优化方法采用随机梯度下降法.首先任意选取一个超平面$w_0$,$b_0$,然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新$w,b$,直到损失函数为0:<br>$$w\longleftarrow w+\eta y_ix_i$$<br>$$b\longleftarrow b+\eta y_i$$<br>其中$η$表示步长.该算法的直观解释是:当一个点被误分类,就调整$w,b$使分离超平面向该误分类点接近.感知机的解可以不同.</p>
<p><strong>对偶形式:</strong> 假设原始形式中的$w_0$和$b_0$均为0,设逐步修改$w$和$b$共$n$次,令$a=nη$,最后学习到的$w,b$可以表示为$w=\sum_{i=1}^{N}\alpha y_i x_i,$.那么对偶算法就变为设初始a和b均为0,每次选取数据更新a和b直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><p><a href="https://www.pkudodo.com" target="_blank" rel="noopener">参考代码:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#Author:Dodo</span></span><br><span class="line"><span class="comment">#Date:2018-11-15</span></span><br><span class="line"><span class="comment">#Email:lvtengchao@pku.edu.cn</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000</span></span><br><span class="line"><span class="string">测试集数量：10000</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：</span></span><br><span class="line"><span class="string">正确率：81.72%（二分类）</span></span><br><span class="line"><span class="string">运行时长：78.6s</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载Mnist数据集</span></span><br><span class="line"><span class="string">    :param fileName:要加载的数据集路径</span></span><br><span class="line"><span class="string">    :return: list形式的数据集及标记</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start to read data'</span>)</span><br><span class="line">    <span class="comment"># 存放数据及标记的list</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment"># 打开文件</span></span><br><span class="line">    fr = open(fileName, <span class="string">'r'</span>)</span><br><span class="line">    <span class="comment"># 将文件按行读取</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 对每一行数据按切割福','进行切割，返回字段列表</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">','</span>)</span><br><span class="line">        <span class="comment"># Mnsit有0-9是个标记，由于是二分类任务，所以将&gt;=5的作为1，&lt;5为-1</span></span><br><span class="line">        <span class="keyword">if</span> int(curLine[<span class="number">0</span>]) &gt;= <span class="number">5</span>:</span><br><span class="line">            labelArr.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labelArr.append(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment">#存放标记</span></span><br><span class="line">        <span class="comment">#[int(num) for num in curLine[1:]] -&gt; 遍历每一行中除了以第一哥元素（标记）外将所有元素转换成int类型</span></span><br><span class="line">        <span class="comment">#[int(num)/255 for num in curLine[1:]] -&gt; 将所有数据除255归一化(非必须步骤，可以不归一化)</span></span><br><span class="line">        dataArr.append([int(num)/<span class="number">255</span> <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line">    <span class="comment">#返回data和label</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptron</span><span class="params">(dataArr, labelArr, iter=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    感知器训练过程</span></span><br><span class="line"><span class="string">    :param dataArr:训练集的数据 (list)</span></span><br><span class="line"><span class="string">    :param labelArr: 训练集的标签(list)</span></span><br><span class="line"><span class="string">    :param iter: 迭代次数，默认50</span></span><br><span class="line"><span class="string">    :return: 训练好的w和b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start to trans'</span>)</span><br><span class="line">    <span class="comment">#将数据转换成矩阵形式（在机器学习中因为通常都是向量的运算，转换成矩阵形式方便运算）</span></span><br><span class="line">    <span class="comment">#转换后的数据中每一个样本的向量都是横向的</span></span><br><span class="line">    dataMat = np.mat(dataArr)</span><br><span class="line">    <span class="comment">#将标签转换成矩阵，之后转置(.T为转置)。</span></span><br><span class="line">    <span class="comment">#转置是因为在运算中需要单独取label中的某一个元素，如果是1xN的矩阵的话，无法用label[i]的方式读取</span></span><br><span class="line">    <span class="comment">#对于只有1xN的label可以不转换成矩阵，直接label[i]即可，这里转换是为了格式上的统一</span></span><br><span class="line">    labelMat = np.mat(labelArr).T</span><br><span class="line">    <span class="comment">#获取数据矩阵的大小，为m*n</span></span><br><span class="line">    m, n = np.shape(dataMat)</span><br><span class="line">    <span class="comment">#创建初始权重w，初始值全为0。</span></span><br><span class="line">    <span class="comment">#np.shape(dataMat)的返回值为m，n -&gt; np.shape(dataMat)[1])的值即为n，与</span></span><br><span class="line">    <span class="comment">#样本长度保持一致</span></span><br><span class="line">    w = np.zeros((<span class="number">1</span>, np.shape(dataMat)[<span class="number">1</span>]))<span class="comment"># 初始化权重w为1*N的0矩阵</span></span><br><span class="line">    <span class="comment">#初始化偏置b为0</span></span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">#初始化步长，也就是梯度下降过程中的n，控制梯度下降速率</span></span><br><span class="line">    h = <span class="number">0.0001</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#进行iter次迭代计算</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(iter):</span><br><span class="line">        <span class="comment">#对于每一个样本进行梯度下降</span></span><br><span class="line">        <span class="comment">#李航书中在2.3.1开头部分使用的梯度下降，是全部样本都算一遍以后，统一</span></span><br><span class="line">        <span class="comment">#进行一次梯度下降</span></span><br><span class="line">        <span class="comment">#在2.3.1的后半部分可以看到（例如公式2.6 2.7），求和符号没有了，此时用</span></span><br><span class="line">        <span class="comment">#的是随机梯度下降，即计算一个样本就针对该样本进行一次梯度下降。</span></span><br><span class="line">        <span class="comment">#两者的差异各有千秋，但较为常用的是随机梯度下降。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment">#获取当前样本的向量</span></span><br><span class="line">            xi = dataMat[i]</span><br><span class="line">            <span class="comment">#获取当前样本所对应的标签</span></span><br><span class="line">            yi = labelMat[i]</span><br><span class="line">            <span class="comment">#判断是否是误分类样本</span></span><br><span class="line">            <span class="comment">#误分类样本特征为： -yi(w*xi+b)&gt;=0，详细可参考书中2.2.2小节</span></span><br><span class="line">            <span class="comment">#在书的公式中写的是&gt;0，实际上如果=0，说明改点在超平面上，也是不正确的</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">-1</span> * yi * (w * xi.T + b) &gt;= <span class="number">0</span>:</span><br><span class="line">                <span class="comment">#对于误分类样本，进行梯度下降，更新w和b</span></span><br><span class="line">                w = w + h *  yi * xi</span><br><span class="line">                b = b + h * yi</span><br><span class="line">        <span class="comment">#打印训练进度</span></span><br><span class="line">        print(<span class="string">'Round %d:%d training'</span> % (k, iter))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回训练完的w、b</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(dataArr, labelArr, w, b)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试准确率</span></span><br><span class="line"><span class="string">    :param dataArr:测试集</span></span><br><span class="line"><span class="string">    :param labelArr: 测试集标签</span></span><br><span class="line"><span class="string">    :param w: 训练获得的权重w</span></span><br><span class="line"><span class="string">    :param b: 训练获得的偏置b</span></span><br><span class="line"><span class="string">    :return: 正确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start to test'</span>)</span><br><span class="line">    <span class="comment">#将数据集转换为矩阵形式方便运算</span></span><br><span class="line">    dataMat = np.mat(dataArr)</span><br><span class="line">    <span class="comment">#将label转换为矩阵并转置，详细信息参考上文perceptron中</span></span><br><span class="line">    <span class="comment">#对于这部分的解说</span></span><br><span class="line">    labelMat = np.mat(labelArr).T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取测试数据集矩阵的大小</span></span><br><span class="line">    m, n = np.shape(dataMat)</span><br><span class="line">    <span class="comment">#错误样本数计数</span></span><br><span class="line">    errorCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">#遍历所有测试样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment">#获得单个样本向量</span></span><br><span class="line">        xi = dataMat[i]</span><br><span class="line">        <span class="comment">#获得该样本标记</span></span><br><span class="line">        yi = labelMat[i]</span><br><span class="line">        <span class="comment">#获得运算结果</span></span><br><span class="line">        result = <span class="number">-1</span> * yi * (w * xi.T + b)</span><br><span class="line">        <span class="comment">#如果-yi(w*xi+b)&gt;=0，说明该样本被误分类，错误样本数加一</span></span><br><span class="line">        <span class="keyword">if</span> result &gt;= <span class="number">0</span>: errorCnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">#正确率 = 1 - （样本分类错误数 / 样本总数）</span></span><br><span class="line">    accruRate = <span class="number">1</span> - (errorCnt / m)</span><br><span class="line">    <span class="comment">#返回正确率</span></span><br><span class="line">    <span class="keyword">return</span> accruRate</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#获取当前时间</span></span><br><span class="line">    <span class="comment">#在文末同样获取当前时间，两时间差即为程序运行时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取训练集及标签</span></span><br><span class="line">    trainData, trainLabel = loadData(<span class="string">'./mnist_train.csv'</span>)</span><br><span class="line">    <span class="comment">#获取测试集及标签</span></span><br><span class="line">    testData, testLabel = loadData(<span class="string">'./mnist_test.csv'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练获得权重</span></span><br><span class="line">    w, b = perceptron(trainData, trainLabel, iter = <span class="number">30</span>)</span><br><span class="line">    <span class="comment">#进行测试，获得正确率</span></span><br><span class="line">    accruRate = test(testData, testLabel, w, b)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取当前时间，作为结束时间</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="comment">#显示正确率</span></span><br><span class="line">    print(<span class="string">'accuracy rate is:'</span>, accruRate)</span><br><span class="line">    <span class="comment">#显示用时时长</span></span><br><span class="line">    print(<span class="string">'time span:'</span>, end - start)</span><br></pre></td></tr></table></figure>

<h1 id="（二）K-邻近"><a href="#（二）K-邻近" class="headerlink" title="（二）K-邻近"></a>（二）K-邻近</h1><h2 id="定义：-1"><a href="#定义：-1" class="headerlink" title="定义："></a>定义：</h2><p>$k$近邻法根据其$k$个最邻的训练实例的类别,通过<strong>多数表决</strong>等方式进行预测.<br><br>什么是多数表决？我们为了对样本$x$进行归类，通过它周围最近的$k$个点来“投票”，这$k$个点大多数是哪个类型的，则定样本$x$为这个类型，故称为多数表决</p>
<p>模型说明:<br>(1)<strong>训练集</strong>（样本$x$以及样本$x$对应的label:$y$）<br>(2)<strong>距离度量</strong>(欧氏距离or曼哈顿距离) 特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用<strong>欧氏距离</strong>,也可以使用曼哈顿距离.<br>欧式距离：<br>曼哈顿距离：<br>(3)<strong>k值</strong> k值较小时,整体模型变得复杂,容易发生过拟合;k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k.</p>
<p>但是K邻近算法也有其局限性：</p>
<ol>
<li><p>在预测样本类别时，待预测样本需要与训练集中所有样本计算距离，当训练集数量过高时（例如Mnsit训练集有60000个样本），每预测一个样本都要计算60000个距离，计算代价过高，尤其当测试集数目也较大时（Mnist测试集有10000个）。</p>
</li>
<li><p>K近邻在高维情况下时（高维在机器学习中并不少见），待预测样本需要与依次与所有样本求距离。向量维度过高时使得欧式距离的计算变得不太迅速了。本文在60000训练集的情况下，将10000个测试集缩减为200个，整个过程仍然需要308秒（曼哈顿距离为246秒，但准确度大幅下降）。</p>
</li>
<li><p>使用欧氏距离还是曼哈顿距离，性能上的差别相对来说不是很大，说明欧式距离并不是制约计算速度的主要方式。最主要的是训练集的大小，每次预测都需要与60000个样本进行比对，同时选出距离最近的$k$项</p>
<br>

</li>
</ol>
<h2 id="代码：-1"><a href="#代码：-1" class="headerlink" title="代码："></a>代码：</h2><p><a href="https://www.pkudodo.com" target="_blank" rel="noopener">参考代码:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#Author:Dodo</span></span><br><span class="line"><span class="comment">#Date:2018-11-16</span></span><br><span class="line"><span class="comment">#Email:lvtengchao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000</span></span><br><span class="line"><span class="string">测试集数量：10000（实际使用：200）</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：（邻近k数量：25）</span></span><br><span class="line"><span class="string">向量距离使用算法——欧式距离</span></span><br><span class="line"><span class="string">    正确率：97%</span></span><br><span class="line"><span class="string">    运行时长：308s</span></span><br><span class="line"><span class="string">向量距离使用算法——曼哈顿距离</span></span><br><span class="line"><span class="string">    正确率：14%</span></span><br><span class="line"><span class="string">    运行时长：246s</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载文件</span></span><br><span class="line"><span class="string">    :param fileName:要加载的文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集和标签集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start read file'</span>)</span><br><span class="line">    <span class="comment">#存放数据及标记</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment">#读取文件</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="comment">#遍历文件中的每一行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment">#获取当前行，并按“，”切割成字段放入列表中</span></span><br><span class="line">        <span class="comment">#strip：去掉每行字符串首尾指定的字符（默认空格或换行符）</span></span><br><span class="line">        <span class="comment">#split：按照指定的字符将字符串切割成每个字段，返回列表形式</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">','</span>)</span><br><span class="line">        <span class="comment">#将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）</span></span><br><span class="line">        <span class="comment">#在放入的同时将原先字符串形式的数据转换为整型</span></span><br><span class="line">        dataArr.append([int(num) <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line">        <span class="comment">#将标记信息放入标记集中</span></span><br><span class="line">        <span class="comment">#放入的同时将标记转换为整型</span></span><br><span class="line">        labelArr.append(int(curLine[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#返回数据集和标记</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcDist</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算两个样本点向量之间的距离</span></span><br><span class="line"><span class="string">    使用的是欧氏距离，即 样本点每个元素相减的平方  再求和  再开方</span></span><br><span class="line"><span class="string">    欧式举例公式这里不方便写，可以百度或谷歌欧式距离（也称欧几里得距离）</span></span><br><span class="line"><span class="string">    :param x1:向量1</span></span><br><span class="line"><span class="string">    :param x2:向量2</span></span><br><span class="line"><span class="string">    :return:向量之间的欧式距离</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.square(x1 - x2)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#马哈顿距离计算公式</span></span><br><span class="line">    <span class="comment"># return np.sum(x1 - x2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getClosest</span><span class="params">(trainDataMat, trainLabelMat, x, topK)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测样本x的标记。</span></span><br><span class="line"><span class="string">    获取方式通过找到与样本x最近的topK个点，并查看它们的标签。</span></span><br><span class="line"><span class="string">    查找里面占某类标签最多的那类标签</span></span><br><span class="line"><span class="string">    （书中3.1 3.2节）</span></span><br><span class="line"><span class="string">    :param trainDataMat:训练集数据集</span></span><br><span class="line"><span class="string">    :param trainLabelMat:训练集标签集</span></span><br><span class="line"><span class="string">    :param x:要预测的样本x</span></span><br><span class="line"><span class="string">    :param topK:选择参考最邻近样本的数目（样本数目的选择关系到正确率，详看3.2.3 K值的选择）</span></span><br><span class="line"><span class="string">    :return:预测的标记</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#建立一个存放向量x与每个训练集中样本距离的列表</span></span><br><span class="line">    <span class="comment">#列表的长度为训练集的长度，distList[i]表示x与训练集中第</span></span><br><span class="line">    <span class="comment">## i个样本的距离</span></span><br><span class="line">    distList = [<span class="number">0</span>] * len(trainLabelMat)</span><br><span class="line">    <span class="comment">#遍历训练集中所有的样本点，计算与x的距离</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(trainDataMat)):</span><br><span class="line">        <span class="comment">#获取训练集中当前样本的向量</span></span><br><span class="line">        x1 = trainDataMat[i]</span><br><span class="line">        <span class="comment">#计算向量x与训练集样本x的距离</span></span><br><span class="line">        curDist = calcDist(x1, x)</span><br><span class="line">        <span class="comment">#将距离放入对应的列表位置中</span></span><br><span class="line">        distList[i] = curDist</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对距离列表进行排序</span></span><br><span class="line">    <span class="comment">#argsort：函数将数组的值从小到大排序后，并按照其相对应的索引值输出</span></span><br><span class="line">    <span class="comment">#例如：</span></span><br><span class="line">    <span class="comment">#   &gt;&gt;&gt; x = np.array([3, 1, 2])</span></span><br><span class="line">    <span class="comment">#   &gt;&gt;&gt; np.argsort(x)</span></span><br><span class="line">    <span class="comment">#   array([1, 2, 0])</span></span><br><span class="line">    <span class="comment">#返回的是列表中从小到大的元素索引值，对于我们这种需要查找最小距离的情况来说很合适</span></span><br><span class="line">    <span class="comment">#array返回的是整个索引值列表，我们通过[:topK]取列表中前topL个放入list中。</span></span><br><span class="line">    <span class="comment">#----------------优化点-------------------</span></span><br><span class="line">    <span class="comment">#由于我们只取topK小的元素索引值，所以其实不需要对整个列表进行排序，而argsort是对整个</span></span><br><span class="line">    <span class="comment">#列表进行排序的，存在时间上的浪费。字典有现成的方法可以只排序top大或top小，可以自行查阅</span></span><br><span class="line">    <span class="comment">#对代码进行稍稍修改即可</span></span><br><span class="line">    <span class="comment">#这里没有对其进行优化主要原因是KNN的时间耗费大头在计算向量与向量之间的距离上，由于向量高维</span></span><br><span class="line">    <span class="comment">#所以计算时间需要很长，所以如果要提升时间，在这里优化的意义不大。（当然不是说就可以不优化了，</span></span><br><span class="line">    <span class="comment">#主要是我太懒了）</span></span><br><span class="line">    topKList = np.argsort(np.array(distList))[:topK]        <span class="comment">#升序排序</span></span><br><span class="line">    <span class="comment">#建立一个长度时的列表，用于选择数量最多的标记</span></span><br><span class="line">    <span class="comment">#3.2.4提到了分类决策使用的是投票表决，topK个标记每人有一票，在数组中每个标记代表的位置中投入</span></span><br><span class="line">    <span class="comment">#自己对应的地方，随后进行唱票选择最高票的标记</span></span><br><span class="line">    labelList = [<span class="number">0</span>] * <span class="number">10</span></span><br><span class="line">    <span class="comment">#对topK个索引进行遍历</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> topKList:</span><br><span class="line">        <span class="comment">#trainLabelMat[index]：在训练集标签中寻找topK元素索引对应的标记</span></span><br><span class="line">        <span class="comment">#int(trainLabelMat[index])：将标记转换为int（实际上已经是int了，但是不int的话，报错）</span></span><br><span class="line">        <span class="comment">#labelList[int(trainLabelMat[index])]：找到标记在labelList中对应的位置</span></span><br><span class="line">        <span class="comment">#最后加1，表示投了一票</span></span><br><span class="line">        labelList[int(trainLabelMat[index])] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#max(labelList)：找到选票箱中票数最多的票数值</span></span><br><span class="line">    <span class="comment">#labelList.index(max(labelList))：再根据最大值在列表中找到该值对应的索引，等同于预测的标记</span></span><br><span class="line">    <span class="keyword">return</span> labelList.index(max(labelList))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(trainDataArr, trainLabelArr, testDataArr, testLabelArr, topK)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试正确率</span></span><br><span class="line"><span class="string">    :param trainDataArr:训练集数据集</span></span><br><span class="line"><span class="string">    :param trainLabelArr: 训练集标记</span></span><br><span class="line"><span class="string">    :param testDataArr: 测试集数据集</span></span><br><span class="line"><span class="string">    :param testLabelArr: 测试集标记</span></span><br><span class="line"><span class="string">    :param topK: 选择多少个邻近点参考</span></span><br><span class="line"><span class="string">    :return: 正确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start test'</span>)</span><br><span class="line">    <span class="comment">#将所有列表转换为矩阵形式，方便运算</span></span><br><span class="line">    trainDataMat = np.mat(trainDataArr); trainLabelMat = np.mat(trainLabelArr).T</span><br><span class="line">    testDataMat = np.mat(testDataArr); testLabelMat = np.mat(testLabelArr).T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#错误值技术</span></span><br><span class="line">    errorCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">#遍历测试集，对每个测试集样本进行测试</span></span><br><span class="line">    <span class="comment">#由于计算向量与向量之间的时间耗费太大，测试集有6000个样本，所以这里人为改成了</span></span><br><span class="line">    <span class="comment">#测试200个样本点，如果要全跑，将行注释取消，再下一行for注释即可，同时下面的print</span></span><br><span class="line">    <span class="comment">#和return也要相应的更换注释行</span></span><br><span class="line">    <span class="comment"># for i in range(len(testDataMat)):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">        <span class="comment"># print('test %d:%d'%(i, len(trainDataArr)))</span></span><br><span class="line">        print(<span class="string">'test %d:%d'</span> % (i, <span class="number">200</span>))</span><br><span class="line">        <span class="comment">#读取测试集当前测试样本的向量</span></span><br><span class="line">        x = testDataMat[i]</span><br><span class="line">        <span class="comment">#获取预测的标记</span></span><br><span class="line">        y = getClosest(trainDataMat, trainLabelMat, x, topK)</span><br><span class="line">        <span class="comment">#如果预测标记与实际标记不符，错误值计数加1</span></span><br><span class="line">        <span class="keyword">if</span> y != testLabelMat[i]: errorCnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回正确率</span></span><br><span class="line">    <span class="comment"># return 1 - (errorCnt / len(testDataMat))</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (errorCnt / <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取训练集</span></span><br><span class="line">    trainDataArr, trainLabelArr = loadData(<span class="string">'./mnist_train.csv'</span>)</span><br><span class="line">    <span class="comment">#获取测试集</span></span><br><span class="line">    testDataArr, testLabelArr = loadData(<span class="string">'./mnist_test.csv'</span>)</span><br><span class="line">    <span class="comment">#计算测试集正确率</span></span><br><span class="line">    accur = test(trainDataArr, trainLabelArr, testDataArr, testLabelArr, <span class="number">25</span>)</span><br><span class="line">    <span class="comment">#打印正确率</span></span><br><span class="line">    print(<span class="string">'accur is:%d'</span>%(accur * <span class="number">100</span>), <span class="string">'%'</span>)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="comment">#显示花费时间</span></span><br><span class="line">print(<span class="string">'time span:'</span>, end - start)</span><br></pre></td></tr></table></figure>
        </div>
        <footer class="article-footer">
            



    <a data-url="https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/" data-id="cka96wxl5000bfnt6hnzi6cfm" class="article-share-link"><i class="fa fa-share"></i>分享到</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "Eric kun"
        },
        "headline": "感知机、KNN",
        "image": "https://ericzikun.github.iohttps://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center",
        "keywords": "感知机 KNN",
        "genre": "机器学习 感知机&KNN",
        "datePublished": "2019-11-08",
        "dateCreated": "2019-11-08",
        "dateModified": "2020-04-09",
        "url": "https://ericzikun.github.io/2019/11/08/感知机、KNN/",
        "description": "前言：参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！代码来自：https://www.pkudodo.com
（一）感知机定义：感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础。
个人理解：结合看过的《深度学习入门基于python的",
        "wordCount": 2812
    }
</script>

</article>

    <section id="comments">
    
        
    <!-- Valine -->
    <div class="vcomments"></div>


    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>关注我 :</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="twitter" href="https://www.zhihu.com/people/feng-kun-33-65" target="_blank" rel="noopener">
                        <i class="icon fa fa-twitter"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="/" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="stack-overflow" href="/" target="_blank" rel="noopener">
                        <i class="icon fa fa-stack-overflow"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/ericzikun" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="weibo" href="/" target="_blank" rel="noopener">
                        <i class="icon fa fa-weibo"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="rss" href="/" target="_blank" rel="noopener">
                        <i class="icon fa fa-rss"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">下一篇</strong>
        <p class="article-nav-title">
        
            决策树
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2019/11/07/NLP%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E4%B9%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E8%AF%8D%E9%A2%91%E5%92%8CTF-IDF%EF%BC%8C%E5%88%A9%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">上一篇</strong>
        <p class="article-nav-title">NLP入门实战之——基于词频和TF-IDF，利用朴素贝叶斯机器学习方法新闻分类</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                
  <div class="widget-wrap widget-list">
      <h3 class="widget-title"></h3>
      <div class="widget">
        <b>联系方式：847473488@qq.com<br/>
        知乎：ERICK</b>
          <!--这里添加你要写的内容-->
      </div>
  </div>

            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Convlstm/" style="font-size: 10px;">Convlstm</a> <a href="/tags/Imdb/" style="font-size: 10px;">Imdb</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/Keras/" style="font-size: 10px;">Keras</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Lstm/" style="font-size: 10px;">Lstm</a> <a href="/tags/Mac/" style="font-size: 20px;">Mac</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/React/" style="font-size: 10px;">React</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/TextCNN/" style="font-size: 10px;">TextCNN</a> <a href="/tags/Textrank/" style="font-size: 10px;">Textrank</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/antd/" style="font-size: 10px;">antd</a> <a href="/tags/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" style="font-size: 10px;">nginx反向代理</a> <a href="/tags/%E4%BF%9D%E7%A0%94/" style="font-size: 10px;">保研</a> <a href="/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/" style="font-size: 10px;">关键词抽取</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" style="font-size: 10px;">决策树</a> <a href="/tags/%E5%8D%87%E5%AD%A6%EF%BC%8C%E8%AF%BB%E7%A0%94/" style="font-size: 10px;">升学，读研</a> <a href="/tags/%E5%AD%A6%E6%9C%AF/" style="font-size: 16.67px;">学术</a> <a href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/" style="font-size: 10px;">感知机</a> <a href="/tags/%E6%8A%A5%E9%94%99/" style="font-size: 13.33px;">报错</a> <a href="/tags/%E6%95%88%E7%8E%87/" style="font-size: 20px;">效率</a> <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" style="font-size: 10px;">文本分类</a> <a href="/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/" style="font-size: 13.33px;">文本处理</a> <a href="/tags/%E6%97%B6%E5%BA%8F/" style="font-size: 10px;">时序</a> <a href="/tags/%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B/" style="font-size: 10px;">时空预测</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%96%87%E6%9C%AC/" style="font-size: 10px;">格式化文本</a> <a href="/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 10px;">正则表达式</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 13.33px;">深度学习</a> <a href="/tags/%E6%BA%90%E7%A0%81/" style="font-size: 10px;">源码</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%BB%88%E7%AB%AF/" style="font-size: 10px;">终端</a> <a href="/tags/%E7%BB%8F%E9%AA%8C/" style="font-size: 10px;">经验</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 16.67px;">统计学习方法</a> <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" style="font-size: 10px;">自然语言处理</a> <a href="/tags/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" style="font-size: 10px;">论文排版</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">逻辑回归</a> <a href="/tags/%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">预处理</a>
        </div>
    </div>


            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">最新文章</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/05/16/Convlstm%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B%E7%BB%8F%E9%AA%8C%E4%B9%8B%E8%B0%88/" class="thumbnail">
    
    
        <span style="background-image:url(https://img-blog.csdnimg.cn/20200516124803395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70)" alt="Convlstm时空预测经验之谈（本科毕设）" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Convlstm/">Convlstm</a></p>
                            <p class="item-title"><a href="/2020/05/16/Convlstm%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B%E7%BB%8F%E9%AA%8C%E4%B9%8B%E8%B0%88/" class="title">Convlstm时空预测经验之谈（本科毕设）</a></p>
                            <p class="item-date"><time datetime="2020-05-16T05:08:46.000Z" itemprop="datePublished">2020-05-16</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/05/16/React%E9%A1%B9%E7%9B%AE%E8%BF%85%E9%80%9F%E6%90%AD%E5%BB%BA-antd%E4%BC%A0%E6%96%87%E4%BB%B6-nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" class="thumbnail">
    
    
        <span style="background-image:url(https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=3038962962,3196507324&fm=26&gp=0.jpg)" alt="React项目迅速搭建+antd传文件+nginx反向代理" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/">技术栈</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%A0%88/React/">React</a></p>
                            <p class="item-title"><a href="/2020/05/16/React%E9%A1%B9%E7%9B%AE%E8%BF%85%E9%80%9F%E6%90%AD%E5%BB%BA-antd%E4%BC%A0%E6%96%87%E4%BB%B6-nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" class="title">React项目迅速搭建+antd传文件+nginx反向代理</a></p>
                            <p class="item-date"><time datetime="2020-05-16T02:00:14.000Z" itemprop="datePublished">2020-05-16</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/04/18/%E7%88%AC%E8%99%AB-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" class="thumbnail">
    
    
        <span style="background-image:url(https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=810461507,1452878411&fm=26&gp=0.jpg)" alt="爬虫&amp;正则表达式基础" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%8A%80%E5%B7%A7/">技巧</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E6%8A%80%E5%B7%A7/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/">文本处理</a></p>
                            <p class="item-title"><a href="/2020/04/18/%E7%88%AC%E8%99%AB-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" class="title">爬虫&amp;正则表达式基础</a></p>
                            <p class="item-date"><time datetime="2020-04-18T14:43:24.000Z" itemprop="datePublished">2020-04-18</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/04/11/Transformer-%E5%8E%9F%E7%90%86-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93-Tensorflow%E5%AE%98%E6%96%B9%E6%BA%90%E7%A0%81/" class="thumbnail">
    
    
        <span style="background-image:url(https://img-blog.csdnimg.cn/20200411142658336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center)" alt="Transformer 原理+源码分析总结(Tensorflow官方源码)" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/">Transformer</a></p>
                            <p class="item-title"><a href="/2020/04/11/Transformer-%E5%8E%9F%E7%90%86-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93-Tensorflow%E5%AE%98%E6%96%B9%E6%BA%90%E7%A0%81/" class="title">Transformer 原理+源码分析总结(Tensorflow官方源码)</a></p>
                            <p class="item-date"><time datetime="2020-04-11T09:49:11.000Z" itemprop="datePublished">2020-04-11</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/04/10/%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4%E5%8F%8AColab%E7%99%BD%E5%AB%96%E5%BF%85%E5%A4%87/" class="thumbnail">
    
    
        <span style="background-image:url(https://img-blog.csdnimg.cn/20200410192110723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70)" alt="终端命令及Colab白嫖必备" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%8A%80%E5%B7%A7/">技巧</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E6%8A%80%E5%B7%A7/Linux/">Linux</a></p>
                            <p class="item-title"><a href="/2020/04/10/%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4%E5%8F%8AColab%E7%99%BD%E5%AB%96%E5%BF%85%E5%A4%87/" class="title">终端命令及Colab白嫖必备</a></p>
                            <p class="item-date"><time datetime="2020-04-10T15:24:35.000Z" itemprop="datePublished">2020-04-10</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">4</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">链接</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    </li>
                
                    <li>
                        <a href="https://blog.csdn.net/popofzk" target="_blank" rel="noopener">CSDN</a>
                    </li>
                
                    <li>
                        <a href="https://www.zhihu.com/people/feng-kun-33-65" target="_blank" rel="noopener">知乎</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 Eric kun</p>
                
                <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/ppoffice" target="_blank">PPOffice</a></p>
                
            </div>
            <div class="parent">
              <div class="child">
                <span id="busuanzi_container_site_pv">
                  访问量<span id="busuanzi_value_site_pv"></span>次
                </span>
                <span class="post-meta-divider">|</span>
                <span id="busuanzi_container_site_uv" style='display:none'>
                  访客数<span id="busuanzi_value_site_uv"></span>人
                </span>
                <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
              </div>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
    <style>
      .parent {
        position: relative;
      }
      .child {
        position: absolute;
        left: 50%;
        top: 50%;
        transform: translate(-50%, -50%);
      }
    </style>  
</footer>

        
    
    <script src="//unpkg.com/valine"></script>
    <script>
        var GUEST = ['nick','mail','link'];
        var meta = '';
        meta = meta.split(',').filter(function (item) {
            return GUEST.indexOf(item)>-1;
        });
        var avatarcdn = 'https://gravatar.loli.net/avatar/' == true;
        new Valine({
            el: '.vcomments',
            notify: "",
            verify: "",
            appId: "tx6zs0UB1yRovubWAD3heyoM-gzGzoHsz",
            appKey: "8SJzl4MBSSjcdEESUaALKRXk",
            placeholder: "Just Do It",
            avatar:"identicon",
            recordIP:"",
            visitor: ""
        });
    </script>





    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>

    

    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>
