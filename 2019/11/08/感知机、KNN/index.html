<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>感知机、KNN | EricKun</title><meta name="keywords" content="感知机,KNN"><meta name="author" content="Eric kun"><meta name="copyright" content="Eric kun"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="前言：参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！代码来自：https:&#x2F;&#x2F;www.pkudodo.com （一）感知机定义：感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础。 个人理解：结合看过的《深度学习入门基于python的">
<meta property="og:type" content="article">
<meta property="og:title" content="感知机、KNN">
<meta property="og:url" content="https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/index.html">
<meta property="og:site_name" content="EricKun">
<meta property="og:description" content="前言：参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！代码来自：https:&#x2F;&#x2F;www.pkudodo.com （一）感知机定义：感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础。 个人理解：结合看过的《深度学习入门基于python的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center">
<meta property="article:published_time" content="2019-11-08T02:12:13.000Z">
<meta property="article:modified_time" content="2020-10-01T11:50:44.010Z">
<meta property="article:author" content="Eric kun">
<meta property="article:tag" content="感知机">
<meta property="article:tag" content="KNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center"><link rel="shortcut icon" href="https://pic2.zhimg.com/80/v2-f19e0e0add10a40489cdb8df576a0f7e_qhd.jpg"><link rel="canonical" href="https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?0d6b4455953d3e1c0917234dfebaa739";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '4.2.1',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-01 19:50:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="EricKun" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">32</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">63</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">27</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#（一）感知机"><span class="toc-text">（一）感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义："><span class="toc-text">定义：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码："><span class="toc-text">代码：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#（二）K-邻近"><span class="toc-text">（二）K-邻近</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义：-1"><span class="toc-text">定义：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码：-1"><span class="toc-text">代码：</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">EricKun</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">感知机、KNN</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-11-08T02:12:13.000Z" title="发表于 2019-11-08 10:12:13">2019-11-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-10-01T11:50:44.010Z" title="更新于 2020-10-01 19:50:44">2020-10-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA-KNN/">感知机&amp;KNN</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p><strong>前言：</strong><br>参考了一位NLP学长的博客，受益颇多，跟着学长学习李航老师的《统计学习方法》，希望整理一些重点，便于翻阅，日积月累，为三年后的面试打下基础！<br>代码来自：<br><a href="https://www.pkudodo.com" target="_blank" rel="noopener">https://www.pkudodo.com</a></p>
<h1 id="（一）感知机"><a href="#（一）感知机" class="headerlink" title="（一）感知机"></a>（一）感知机</h1><h2 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h2><p>感知机是二分类的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行<strong>线性划分</strong>的分离超平面.是神经网络和支持向量机的基础。</p>
<p>个人理解：结合看过的《深度学习入门基于python的理论与实现》，感知机说白了就是接受一些信号，输出信号的模型（就像理工科电工科中讲到的逻辑电路一个道理），多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用，也就是说，权重越大，对应该权重的信号的重要性就越高。<br><br><img src="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" style="zoom:70%;" /><br>那么，有同学就疑问了，为什么是线性呢，非线性不能吗，这里可以看看两张图：<br><img src="https://img-blog.csdnimg.cn/20191127192443953.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" style="zoom:70%;" /><br><img src="https://img-blog.csdnimg.cn/20191127192528864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" style="zoom:80%;" /></p>
<p>用一条直线是可以将图1正常分割开，而无法将第二张图分割，第一张图在编程实现时用到的是简单的逻辑电路（与门、与非门、或门），但是第二张图这种异或门只能通过多层感知机，也就是神经网络才能够实现。</p>
<p><strong>感知机的几何解释:</strong><br>模型公式:$f(x)=sign(w\cdot x+b)$<br>$w$叫作权值向量,$b$叫做偏置,$sign$是符号函数.<br>$w\cdot x+b$对应于特征空间中的一个分离超平面$S$,其中$w$是$S$的法向量,$b$是$S$的截距.$S$将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.<br><br>策略:<br>假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面$S$的总距离。因为误分类点到超平面S的距离是$\frac{1}{||w||}|w\cdot{x_0}+b|$.且对于误分类的数据来说,总有:$-y_i(w\cdot{x_i}+b)&gt;0$成立,因此不考虑$\frac{1}{||w||}$,就得到感知机的<strong>损失函数</strong>:<br>$L(w,b)=-\sum_{x_i\in{M}} y_i(w\cdot{x_i}+b)$,其中$M$是误分类点的集合.感知机学习的策略就是选取使<strong>损失函数最小的模型参数</strong>.<br></p>
<p>算法:感知机的最优化方法采用随机梯度下降法.首先任意选取一个超平面$w_0$,$b_0$,然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新$w,b$,直到损失函数为0:<br>$$w\longleftarrow w+\eta y_ix_i$$<br>$$b\longleftarrow b+\eta y_i$$<br>其中$η$表示步长.该算法的直观解释是:当一个点被误分类,就调整$w,b$使分离超平面向该误分类点接近.感知机的解可以不同.</p>
<p><strong>对偶形式:</strong> 假设原始形式中的$w_0$和$b_0$均为0,设逐步修改$w$和$b$共$n$次,令$a=nη$,最后学习到的$w,b$可以表示为$w=\sum_{i=1}^{N}\alpha y_i x_i,$.那么对偶算法就变为设初始a和b均为0,每次选取数据更新a和b直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><p><a href="https://www.pkudodo.com" target="_blank" rel="noopener">参考代码:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#Author:Dodo</span></span><br><span class="line"><span class="comment">#Date:2018-11-15</span></span><br><span class="line"><span class="comment">#Email:lvtengchao@pku.edu.cn</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000</span></span><br><span class="line"><span class="string">测试集数量：10000</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：</span></span><br><span class="line"><span class="string">正确率：81.72%（二分类）</span></span><br><span class="line"><span class="string">运行时长：78.6s</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载Mnist数据集</span></span><br><span class="line"><span class="string">    :param fileName:要加载的数据集路径</span></span><br><span class="line"><span class="string">    :return: list形式的数据集及标记</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start to read data'</span>)</span><br><span class="line">    <span class="comment"># 存放数据及标记的list</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment"># 打开文件</span></span><br><span class="line">    fr = open(fileName, <span class="string">'r'</span>)</span><br><span class="line">    <span class="comment"># 将文件按行读取</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 对每一行数据按切割福','进行切割，返回字段列表</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">','</span>)</span><br><span class="line">        <span class="comment"># Mnsit有0-9是个标记，由于是二分类任务，所以将&gt;=5的作为1，&lt;5为-1</span></span><br><span class="line">        <span class="keyword">if</span> int(curLine[<span class="number">0</span>]) &gt;= <span class="number">5</span>:</span><br><span class="line">            labelArr.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labelArr.append(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment">#存放标记</span></span><br><span class="line">        <span class="comment">#[int(num) for num in curLine[1:]] -&gt; 遍历每一行中除了以第一哥元素（标记）外将所有元素转换成int类型</span></span><br><span class="line">        <span class="comment">#[int(num)/255 for num in curLine[1:]] -&gt; 将所有数据除255归一化(非必须步骤，可以不归一化)</span></span><br><span class="line">        dataArr.append([int(num)/<span class="number">255</span> <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line">    <span class="comment">#返回data和label</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptron</span><span class="params">(dataArr, labelArr, iter=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    感知器训练过程</span></span><br><span class="line"><span class="string">    :param dataArr:训练集的数据 (list)</span></span><br><span class="line"><span class="string">    :param labelArr: 训练集的标签(list)</span></span><br><span class="line"><span class="string">    :param iter: 迭代次数，默认50</span></span><br><span class="line"><span class="string">    :return: 训练好的w和b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start to trans'</span>)</span><br><span class="line">    <span class="comment">#将数据转换成矩阵形式（在机器学习中因为通常都是向量的运算，转换成矩阵形式方便运算）</span></span><br><span class="line">    <span class="comment">#转换后的数据中每一个样本的向量都是横向的</span></span><br><span class="line">    dataMat = np.mat(dataArr)</span><br><span class="line">    <span class="comment">#将标签转换成矩阵，之后转置(.T为转置)。</span></span><br><span class="line">    <span class="comment">#转置是因为在运算中需要单独取label中的某一个元素，如果是1xN的矩阵的话，无法用label[i]的方式读取</span></span><br><span class="line">    <span class="comment">#对于只有1xN的label可以不转换成矩阵，直接label[i]即可，这里转换是为了格式上的统一</span></span><br><span class="line">    labelMat = np.mat(labelArr).T</span><br><span class="line">    <span class="comment">#获取数据矩阵的大小，为m*n</span></span><br><span class="line">    m, n = np.shape(dataMat)</span><br><span class="line">    <span class="comment">#创建初始权重w，初始值全为0。</span></span><br><span class="line">    <span class="comment">#np.shape(dataMat)的返回值为m，n -&gt; np.shape(dataMat)[1])的值即为n，与</span></span><br><span class="line">    <span class="comment">#样本长度保持一致</span></span><br><span class="line">    w = np.zeros((<span class="number">1</span>, np.shape(dataMat)[<span class="number">1</span>]))<span class="comment"># 初始化权重w为1*N的0矩阵</span></span><br><span class="line">    <span class="comment">#初始化偏置b为0</span></span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">#初始化步长，也就是梯度下降过程中的n，控制梯度下降速率</span></span><br><span class="line">    h = <span class="number">0.0001</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#进行iter次迭代计算</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(iter):</span><br><span class="line">        <span class="comment">#对于每一个样本进行梯度下降</span></span><br><span class="line">        <span class="comment">#李航书中在2.3.1开头部分使用的梯度下降，是全部样本都算一遍以后，统一</span></span><br><span class="line">        <span class="comment">#进行一次梯度下降</span></span><br><span class="line">        <span class="comment">#在2.3.1的后半部分可以看到（例如公式2.6 2.7），求和符号没有了，此时用</span></span><br><span class="line">        <span class="comment">#的是随机梯度下降，即计算一个样本就针对该样本进行一次梯度下降。</span></span><br><span class="line">        <span class="comment">#两者的差异各有千秋，但较为常用的是随机梯度下降。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment">#获取当前样本的向量</span></span><br><span class="line">            xi = dataMat[i]</span><br><span class="line">            <span class="comment">#获取当前样本所对应的标签</span></span><br><span class="line">            yi = labelMat[i]</span><br><span class="line">            <span class="comment">#判断是否是误分类样本</span></span><br><span class="line">            <span class="comment">#误分类样本特征为： -yi(w*xi+b)&gt;=0，详细可参考书中2.2.2小节</span></span><br><span class="line">            <span class="comment">#在书的公式中写的是&gt;0，实际上如果=0，说明改点在超平面上，也是不正确的</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">-1</span> * yi * (w * xi.T + b) &gt;= <span class="number">0</span>:</span><br><span class="line">                <span class="comment">#对于误分类样本，进行梯度下降，更新w和b</span></span><br><span class="line">                w = w + h *  yi * xi</span><br><span class="line">                b = b + h * yi</span><br><span class="line">        <span class="comment">#打印训练进度</span></span><br><span class="line">        print(<span class="string">'Round %d:%d training'</span> % (k, iter))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回训练完的w、b</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(dataArr, labelArr, w, b)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试准确率</span></span><br><span class="line"><span class="string">    :param dataArr:测试集</span></span><br><span class="line"><span class="string">    :param labelArr: 测试集标签</span></span><br><span class="line"><span class="string">    :param w: 训练获得的权重w</span></span><br><span class="line"><span class="string">    :param b: 训练获得的偏置b</span></span><br><span class="line"><span class="string">    :return: 正确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start to test'</span>)</span><br><span class="line">    <span class="comment">#将数据集转换为矩阵形式方便运算</span></span><br><span class="line">    dataMat = np.mat(dataArr)</span><br><span class="line">    <span class="comment">#将label转换为矩阵并转置，详细信息参考上文perceptron中</span></span><br><span class="line">    <span class="comment">#对于这部分的解说</span></span><br><span class="line">    labelMat = np.mat(labelArr).T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取测试数据集矩阵的大小</span></span><br><span class="line">    m, n = np.shape(dataMat)</span><br><span class="line">    <span class="comment">#错误样本数计数</span></span><br><span class="line">    errorCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">#遍历所有测试样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment">#获得单个样本向量</span></span><br><span class="line">        xi = dataMat[i]</span><br><span class="line">        <span class="comment">#获得该样本标记</span></span><br><span class="line">        yi = labelMat[i]</span><br><span class="line">        <span class="comment">#获得运算结果</span></span><br><span class="line">        result = <span class="number">-1</span> * yi * (w * xi.T + b)</span><br><span class="line">        <span class="comment">#如果-yi(w*xi+b)&gt;=0，说明该样本被误分类，错误样本数加一</span></span><br><span class="line">        <span class="keyword">if</span> result &gt;= <span class="number">0</span>: errorCnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">#正确率 = 1 - （样本分类错误数 / 样本总数）</span></span><br><span class="line">    accruRate = <span class="number">1</span> - (errorCnt / m)</span><br><span class="line">    <span class="comment">#返回正确率</span></span><br><span class="line">    <span class="keyword">return</span> accruRate</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#获取当前时间</span></span><br><span class="line">    <span class="comment">#在文末同样获取当前时间，两时间差即为程序运行时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取训练集及标签</span></span><br><span class="line">    trainData, trainLabel = loadData(<span class="string">'./mnist_train.csv'</span>)</span><br><span class="line">    <span class="comment">#获取测试集及标签</span></span><br><span class="line">    testData, testLabel = loadData(<span class="string">'./mnist_test.csv'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练获得权重</span></span><br><span class="line">    w, b = perceptron(trainData, trainLabel, iter = <span class="number">30</span>)</span><br><span class="line">    <span class="comment">#进行测试，获得正确率</span></span><br><span class="line">    accruRate = test(testData, testLabel, w, b)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取当前时间，作为结束时间</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="comment">#显示正确率</span></span><br><span class="line">    print(<span class="string">'accuracy rate is:'</span>, accruRate)</span><br><span class="line">    <span class="comment">#显示用时时长</span></span><br><span class="line">    print(<span class="string">'time span:'</span>, end - start)</span><br></pre></td></tr></table></figure>

<h1 id="（二）K-邻近"><a href="#（二）K-邻近" class="headerlink" title="（二）K-邻近"></a>（二）K-邻近</h1><h2 id="定义：-1"><a href="#定义：-1" class="headerlink" title="定义："></a>定义：</h2><p>$k$近邻法根据其$k$个最邻的训练实例的类别,通过<strong>多数表决</strong>等方式进行预测.<br><br>什么是多数表决？我们为了对样本$x$进行归类，通过它周围最近的$k$个点来“投票”，这$k$个点大多数是哪个类型的，则定样本$x$为这个类型，故称为多数表决</p>
<p>模型说明:<br>(1)<strong>训练集</strong>（样本$x$以及样本$x$对应的label:$y$）<br>(2)<strong>距离度量</strong>(欧氏距离or曼哈顿距离) 特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用<strong>欧氏距离</strong>,也可以使用曼哈顿距离.<br>欧式距离：<br>曼哈顿距离：<br>(3)<strong>k值</strong> k值较小时,整体模型变得复杂,容易发生过拟合;k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k.</p>
<p>但是K邻近算法也有其局限性：</p>
<ol>
<li><p>在预测样本类别时，待预测样本需要与训练集中所有样本计算距离，当训练集数量过高时（例如Mnsit训练集有60000个样本），每预测一个样本都要计算60000个距离，计算代价过高，尤其当测试集数目也较大时（Mnist测试集有10000个）。</p>
</li>
<li><p>K近邻在高维情况下时（高维在机器学习中并不少见），待预测样本需要与依次与所有样本求距离。向量维度过高时使得欧式距离的计算变得不太迅速了。本文在60000训练集的情况下，将10000个测试集缩减为200个，整个过程仍然需要308秒（曼哈顿距离为246秒，但准确度大幅下降）。</p>
</li>
<li><p>使用欧氏距离还是曼哈顿距离，性能上的差别相对来说不是很大，说明欧式距离并不是制约计算速度的主要方式。最主要的是训练集的大小，每次预测都需要与60000个样本进行比对，同时选出距离最近的$k$项</p>
<br>

</li>
</ol>
<h2 id="代码：-1"><a href="#代码：-1" class="headerlink" title="代码："></a>代码：</h2><p><a href="https://www.pkudodo.com" target="_blank" rel="noopener">参考代码:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#Author:Dodo</span></span><br><span class="line"><span class="comment">#Date:2018-11-16</span></span><br><span class="line"><span class="comment">#Email:lvtengchao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000</span></span><br><span class="line"><span class="string">测试集数量：10000（实际使用：200）</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：（邻近k数量：25）</span></span><br><span class="line"><span class="string">向量距离使用算法——欧式距离</span></span><br><span class="line"><span class="string">    正确率：97%</span></span><br><span class="line"><span class="string">    运行时长：308s</span></span><br><span class="line"><span class="string">向量距离使用算法——曼哈顿距离</span></span><br><span class="line"><span class="string">    正确率：14%</span></span><br><span class="line"><span class="string">    运行时长：246s</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载文件</span></span><br><span class="line"><span class="string">    :param fileName:要加载的文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集和标签集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start read file'</span>)</span><br><span class="line">    <span class="comment">#存放数据及标记</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment">#读取文件</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="comment">#遍历文件中的每一行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment">#获取当前行，并按“，”切割成字段放入列表中</span></span><br><span class="line">        <span class="comment">#strip：去掉每行字符串首尾指定的字符（默认空格或换行符）</span></span><br><span class="line">        <span class="comment">#split：按照指定的字符将字符串切割成每个字段，返回列表形式</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">','</span>)</span><br><span class="line">        <span class="comment">#将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）</span></span><br><span class="line">        <span class="comment">#在放入的同时将原先字符串形式的数据转换为整型</span></span><br><span class="line">        dataArr.append([int(num) <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line">        <span class="comment">#将标记信息放入标记集中</span></span><br><span class="line">        <span class="comment">#放入的同时将标记转换为整型</span></span><br><span class="line">        labelArr.append(int(curLine[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#返回数据集和标记</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcDist</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算两个样本点向量之间的距离</span></span><br><span class="line"><span class="string">    使用的是欧氏距离，即 样本点每个元素相减的平方  再求和  再开方</span></span><br><span class="line"><span class="string">    欧式举例公式这里不方便写，可以百度或谷歌欧式距离（也称欧几里得距离）</span></span><br><span class="line"><span class="string">    :param x1:向量1</span></span><br><span class="line"><span class="string">    :param x2:向量2</span></span><br><span class="line"><span class="string">    :return:向量之间的欧式距离</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.square(x1 - x2)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#马哈顿距离计算公式</span></span><br><span class="line">    <span class="comment"># return np.sum(x1 - x2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getClosest</span><span class="params">(trainDataMat, trainLabelMat, x, topK)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测样本x的标记。</span></span><br><span class="line"><span class="string">    获取方式通过找到与样本x最近的topK个点，并查看它们的标签。</span></span><br><span class="line"><span class="string">    查找里面占某类标签最多的那类标签</span></span><br><span class="line"><span class="string">    （书中3.1 3.2节）</span></span><br><span class="line"><span class="string">    :param trainDataMat:训练集数据集</span></span><br><span class="line"><span class="string">    :param trainLabelMat:训练集标签集</span></span><br><span class="line"><span class="string">    :param x:要预测的样本x</span></span><br><span class="line"><span class="string">    :param topK:选择参考最邻近样本的数目（样本数目的选择关系到正确率，详看3.2.3 K值的选择）</span></span><br><span class="line"><span class="string">    :return:预测的标记</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#建立一个存放向量x与每个训练集中样本距离的列表</span></span><br><span class="line">    <span class="comment">#列表的长度为训练集的长度，distList[i]表示x与训练集中第</span></span><br><span class="line">    <span class="comment">## i个样本的距离</span></span><br><span class="line">    distList = [<span class="number">0</span>] * len(trainLabelMat)</span><br><span class="line">    <span class="comment">#遍历训练集中所有的样本点，计算与x的距离</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(trainDataMat)):</span><br><span class="line">        <span class="comment">#获取训练集中当前样本的向量</span></span><br><span class="line">        x1 = trainDataMat[i]</span><br><span class="line">        <span class="comment">#计算向量x与训练集样本x的距离</span></span><br><span class="line">        curDist = calcDist(x1, x)</span><br><span class="line">        <span class="comment">#将距离放入对应的列表位置中</span></span><br><span class="line">        distList[i] = curDist</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对距离列表进行排序</span></span><br><span class="line">    <span class="comment">#argsort：函数将数组的值从小到大排序后，并按照其相对应的索引值输出</span></span><br><span class="line">    <span class="comment">#例如：</span></span><br><span class="line">    <span class="comment">#   &gt;&gt;&gt; x = np.array([3, 1, 2])</span></span><br><span class="line">    <span class="comment">#   &gt;&gt;&gt; np.argsort(x)</span></span><br><span class="line">    <span class="comment">#   array([1, 2, 0])</span></span><br><span class="line">    <span class="comment">#返回的是列表中从小到大的元素索引值，对于我们这种需要查找最小距离的情况来说很合适</span></span><br><span class="line">    <span class="comment">#array返回的是整个索引值列表，我们通过[:topK]取列表中前topL个放入list中。</span></span><br><span class="line">    <span class="comment">#----------------优化点-------------------</span></span><br><span class="line">    <span class="comment">#由于我们只取topK小的元素索引值，所以其实不需要对整个列表进行排序，而argsort是对整个</span></span><br><span class="line">    <span class="comment">#列表进行排序的，存在时间上的浪费。字典有现成的方法可以只排序top大或top小，可以自行查阅</span></span><br><span class="line">    <span class="comment">#对代码进行稍稍修改即可</span></span><br><span class="line">    <span class="comment">#这里没有对其进行优化主要原因是KNN的时间耗费大头在计算向量与向量之间的距离上，由于向量高维</span></span><br><span class="line">    <span class="comment">#所以计算时间需要很长，所以如果要提升时间，在这里优化的意义不大。（当然不是说就可以不优化了，</span></span><br><span class="line">    <span class="comment">#主要是我太懒了）</span></span><br><span class="line">    topKList = np.argsort(np.array(distList))[:topK]        <span class="comment">#升序排序</span></span><br><span class="line">    <span class="comment">#建立一个长度时的列表，用于选择数量最多的标记</span></span><br><span class="line">    <span class="comment">#3.2.4提到了分类决策使用的是投票表决，topK个标记每人有一票，在数组中每个标记代表的位置中投入</span></span><br><span class="line">    <span class="comment">#自己对应的地方，随后进行唱票选择最高票的标记</span></span><br><span class="line">    labelList = [<span class="number">0</span>] * <span class="number">10</span></span><br><span class="line">    <span class="comment">#对topK个索引进行遍历</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> topKList:</span><br><span class="line">        <span class="comment">#trainLabelMat[index]：在训练集标签中寻找topK元素索引对应的标记</span></span><br><span class="line">        <span class="comment">#int(trainLabelMat[index])：将标记转换为int（实际上已经是int了，但是不int的话，报错）</span></span><br><span class="line">        <span class="comment">#labelList[int(trainLabelMat[index])]：找到标记在labelList中对应的位置</span></span><br><span class="line">        <span class="comment">#最后加1，表示投了一票</span></span><br><span class="line">        labelList[int(trainLabelMat[index])] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#max(labelList)：找到选票箱中票数最多的票数值</span></span><br><span class="line">    <span class="comment">#labelList.index(max(labelList))：再根据最大值在列表中找到该值对应的索引，等同于预测的标记</span></span><br><span class="line">    <span class="keyword">return</span> labelList.index(max(labelList))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(trainDataArr, trainLabelArr, testDataArr, testLabelArr, topK)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试正确率</span></span><br><span class="line"><span class="string">    :param trainDataArr:训练集数据集</span></span><br><span class="line"><span class="string">    :param trainLabelArr: 训练集标记</span></span><br><span class="line"><span class="string">    :param testDataArr: 测试集数据集</span></span><br><span class="line"><span class="string">    :param testLabelArr: 测试集标记</span></span><br><span class="line"><span class="string">    :param topK: 选择多少个邻近点参考</span></span><br><span class="line"><span class="string">    :return: 正确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">'start test'</span>)</span><br><span class="line">    <span class="comment">#将所有列表转换为矩阵形式，方便运算</span></span><br><span class="line">    trainDataMat = np.mat(trainDataArr); trainLabelMat = np.mat(trainLabelArr).T</span><br><span class="line">    testDataMat = np.mat(testDataArr); testLabelMat = np.mat(testLabelArr).T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#错误值技术</span></span><br><span class="line">    errorCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">#遍历测试集，对每个测试集样本进行测试</span></span><br><span class="line">    <span class="comment">#由于计算向量与向量之间的时间耗费太大，测试集有6000个样本，所以这里人为改成了</span></span><br><span class="line">    <span class="comment">#测试200个样本点，如果要全跑，将行注释取消，再下一行for注释即可，同时下面的print</span></span><br><span class="line">    <span class="comment">#和return也要相应的更换注释行</span></span><br><span class="line">    <span class="comment"># for i in range(len(testDataMat)):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">        <span class="comment"># print('test %d:%d'%(i, len(trainDataArr)))</span></span><br><span class="line">        print(<span class="string">'test %d:%d'</span> % (i, <span class="number">200</span>))</span><br><span class="line">        <span class="comment">#读取测试集当前测试样本的向量</span></span><br><span class="line">        x = testDataMat[i]</span><br><span class="line">        <span class="comment">#获取预测的标记</span></span><br><span class="line">        y = getClosest(trainDataMat, trainLabelMat, x, topK)</span><br><span class="line">        <span class="comment">#如果预测标记与实际标记不符，错误值计数加1</span></span><br><span class="line">        <span class="keyword">if</span> y != testLabelMat[i]: errorCnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回正确率</span></span><br><span class="line">    <span class="comment"># return 1 - (errorCnt / len(testDataMat))</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (errorCnt / <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取训练集</span></span><br><span class="line">    trainDataArr, trainLabelArr = loadData(<span class="string">'./mnist_train.csv'</span>)</span><br><span class="line">    <span class="comment">#获取测试集</span></span><br><span class="line">    testDataArr, testLabelArr = loadData(<span class="string">'./mnist_test.csv'</span>)</span><br><span class="line">    <span class="comment">#计算测试集正确率</span></span><br><span class="line">    accur = test(trainDataArr, trainLabelArr, testDataArr, testLabelArr, <span class="number">25</span>)</span><br><span class="line">    <span class="comment">#打印正确率</span></span><br><span class="line">    print(<span class="string">'accur is:%d'</span>%(accur * <span class="number">100</span>), <span class="string">'%'</span>)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="comment">#显示花费时间</span></span><br><span class="line">print(<span class="string">'time span:'</span>, end - start)</span><br></pre></td></tr></table></figure></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Eric kun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/">https://ericzikun.github.io/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ericzikun.github.io" target="_blank">EricKun</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/">感知机</a><a class="post-meta__tags" href="/tags/KNN/">KNN</a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png"/></a><div class="post-qr-code-desc"></div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/"><img class="prev-cover" src="https://img-blog.csdnimg.cn/20191209103004602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">决策树</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/07/NLP%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E4%B9%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E8%AF%8D%E9%A2%91%E5%92%8CTF-IDF%EF%BC%8C%E5%88%A9%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB/"><img class="next-cover" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=1474209398,3737326875&amp;fm=26&amp;gp=0.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP入门实战之——基于词频和TF-IDF，利用朴素贝叶斯机器学习方法新闻分类</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer" style="background: #000000"><div id="footer-wrap"><div class="copyright">&copy;2020 By Eric kun</div><div class="framework-info"><span>框架 </span><a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'tx6zs0UB1yRovubWAD3heyoM-gzGzoHsz',
      appKey: '8SJzl4MBSSjcdEESUaALKRXk',
      placeholder: 'Just do it!',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>