<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>决策树 | EricKun</title><meta name="keywords" content="统计学习方法,机器学习,决策树"><meta name="author" content="Eric kun"><meta name="copyright" content="Eric kun"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="（四）决策树定义：书中实例：贷款申请样本，通过一个人的年龄、是否有工作、是否有自己的房子、信贷情况这四个特征判定，最终构建模型来判别是否给予贷款，如图：希望通过所给的训练数据学习一个贷款申请的决策树，用来对未来贷款申请进行分类（二分类），决策树可以理解成：有一个根节点开始，往下进行分支，越重要的节点应该离根越近，我们将重要的、影响度大的特征作为根节点，依次向下，其次重要的往下面街接，如图： 熵与条">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="https://ericzikun.github.io/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/index.html">
<meta property="og:site_name" content="EricKun">
<meta property="og:description" content="（四）决策树定义：书中实例：贷款申请样本，通过一个人的年龄、是否有工作、是否有自己的房子、信贷情况这四个特征判定，最终构建模型来判别是否给予贷款，如图：希望通过所给的训练数据学习一个贷款申请的决策树，用来对未来贷款申请进行分类（二分类），决策树可以理解成：有一个根节点开始，往下进行分支，越重要的节点应该离根越近，我们将重要的、影响度大的特征作为根节点，依次向下，其次重要的往下面街接，如图： 熵与条">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191209103004602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center">
<meta property="article:published_time" content="2019-11-13T02:17:04.000Z">
<meta property="article:modified_time" content="2020-10-01T11:50:53.664Z">
<meta property="article:author" content="Eric kun">
<meta property="article:tag" content="统计学习方法">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="决策树">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20191209103004602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center"><link rel="shortcut icon" href="https://pic2.zhimg.com/80/v2-f19e0e0add10a40489cdb8df576a0f7e_qhd.jpg"><link rel="canonical" href="https://ericzikun.github.io/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?0d6b4455953d3e1c0917234dfebaa739";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '4.2.1',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-01 19:50:53'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="EricKun" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">46</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">86</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">33</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#（四）决策树"><span class="toc-text">（四）决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义："><span class="toc-text">定义：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码："><span class="toc-text">代码：</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://img-blog.csdnimg.cn/20191209103004602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">EricKun</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">决策树</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-11-13T02:17:04.000Z" title="发表于 2019-11-13 10:17:04">2019-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-10-01T11:50:53.664Z" title="更新于 2020-10-01 19:50:53">2020-10-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="（四）决策树"><a href="#（四）决策树" class="headerlink" title="（四）决策树"></a>（四）决策树</h1><h2 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h2><p>书中实例：贷款申请样本，通过一个人的年龄、是否有工作、是否有自己的房子、信贷情况这四个特征判定，最终构建模型来判别是否给予贷款，如图：<br><img src="https://img-blog.csdnimg.cn/20191208162829471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>希望通过所给的训练数据学习一个贷款申请的决策树，用来对未来贷款申请进行分类（二分类），决策树可以理解成：有一个根节点开始，往下进行分支，越重要的节点应该离根越近，我们将重要的、影响度大的特征作为根节点，依次向下，其次重要的往下面街接，如图：<br><img src="https://img-blog.csdnimg.cn/20191209103004602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<p><strong>熵与条件熵的定义：</strong><br><strong>熵:</strong> 表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：$$P(X=x_i)=p_i,  i=1,2,…,n$$<br>则随机变量$X$的熵定义为：<br>$$H(X)=-\sum_{i=1}^{n}p_i\log{p_i}$$<br>越大的概率，得到的熵值越小，也就是说概率大的确定性大，不确定不就小了嘛，反之亦然；<br>举例：$A$集合：[1,1,1,1,1,1,1,2,2]<br>   &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$B$集合：[1,2,3,4,5,6,7,8,9]<br>   显然$A$集合的熵值要低，因为A里面只有两种类别，相对稳定一些，而B中类别太多，熵值就会大很多，而在分类问题中我们当然是希望分支后的数据类别的熵值小，确定性就大嘛，熵值越低，分类效果越好撒<br>同理<strong>条件熵</strong> 就是表示在已知随机变量X的条件下随机变量Y的不确定性$H(Y|X)$,定义为$X$给定条件下Y的条件概率分布的熵对X的数学期望：<br>$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$</p>
<p><strong>信息增益：</strong><br>做决策树目的就是在过程中将熵值不断减小，增益呢，就是熵值下降了多少，通过信息增益来遍历计算所有特征，哪个特征使得我们的信息增益最大，最大的哪个特征就拿过来当做根节点，接着同理把剩下的特征也这么来排序，排出第二个节点，第三个节点。。。<br>信息增益表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度。<br> 特征$A$对训练数据集$D$的信息增益$g(D,A)$,定义为集合$D$的经验熵，经验熵就是不考虑特征，只考虑整个样本label的熵，附上书中实例：<br> <img src="https://img-blog.csdnimg.cn/20191209110159625.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" alt=""><br> $H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：<br>$$g(D,A)=H(D)-H(D|A)$$<br>熵$H(Y)$与条件熵$H(Y|X)$之差成为互信息，此时信息增益等于互信息。<br><strong>信息增益算法：</strong><br>输入：训练数据集$D$和特征$A$；<br>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$。<br>(1)计算数据集$D$的经验熵$H(D)$<br>$$H(D)=-\sum_{k=1}^{k}\frac{|C_k|}{|D|}\log{\frac{|C_k|}{|D|}}$$<br>(2)计算特征$A$对数据集$D$的经验条件熵$H(D|A)$<br>$$H(D|A)=\sum_{i=1}^{n}\frac{|D_I|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{D}\sum_{k=1}^{k}\frac{|D_{ik}|}{D_i}\log_{2}\frac{|D_{ik}|}{|D_i|}$$<br>(3)计算信息增益：<br>$$g(D,A)=H(D)-H(D,A)$$</p>
<p><strong>信息增益比：</strong><br>以信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比对其校正：特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即：<br>$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$<br>以上讨论都是离散值，如果是连续值呢？</p>
<p><strong>ID3算法</strong>:<br>核心是在决策树各个结点上应用信息增益准则选择信息增益最大且大于阈值的特征，递归地构建决策树.ID3相当于用极大似然法进行概率模型的选择.甶于算法只有树的生成，所以容易产生过拟合。<br><img src="https://img-blog.csdnimg.cn/2019120817050284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191208170537141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>决策树剪枝策略：</strong><br>为什么要剪枝：决策树过拟合风险很大，<br>预剪枝：边建立决策树边进行剪枝的操作<br>后剪枝：当建立完决策树后进行剪枝操作</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><p><a href="https://www.pkudodo.com" target="_blank" rel="noopener">参考代码:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#Author:Dodo</span></span><br><span class="line"><span class="comment">#Date:2018-11-21</span></span><br><span class="line"><span class="comment">#Email:lvtengchao@pku.edu.cn</span></span><br><span class="line"><span class="comment">#Blog:www.pkudodo.com</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000</span></span><br><span class="line"><span class="string">测试集数量：10000</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：ID3(未剪枝)</span></span><br><span class="line"><span class="string">    正确率：85.9%</span></span><br><span class="line"><span class="string">    运行时长：356s</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载文件</span></span><br><span class="line"><span class="string">    :param fileName:要加载的文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集和标签集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#存放数据及标记</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment">#读取文件</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="comment">#遍历文件中的每一行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment">#获取当前行，并按“，”切割成字段放入列表中</span></span><br><span class="line">        <span class="comment">#strip：去掉每行字符串首尾指定的字符（默认空格或换行符）</span></span><br><span class="line">        <span class="comment">#split：按照指定的字符将字符串切割成每个字段，返回列表形式</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">','</span>)</span><br><span class="line">        <span class="comment">#将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）</span></span><br><span class="line">        <span class="comment">#在放入的同时将原先字符串形式的数据转换为整型</span></span><br><span class="line">        <span class="comment">#此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算</span></span><br><span class="line">        dataArr.append([int(int(num) &gt; <span class="number">128</span>) <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line">        <span class="comment">#将标记信息放入标记集中</span></span><br><span class="line">        <span class="comment">#放入的同时将标记转换为整型</span></span><br><span class="line">        labelArr.append(int(curLine[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#返回数据集和标记</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorClass</span><span class="params">(labelArr)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    找到当前标签集中占数目最大的标签</span></span><br><span class="line"><span class="string">    :param labelArr: 标签集</span></span><br><span class="line"><span class="string">    :return: 最大的标签</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#建立字典，用于不同类别的标签技术</span></span><br><span class="line">    classDict = &#123;&#125;</span><br><span class="line">    <span class="comment">#遍历所有标签</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labelArr)):</span><br><span class="line">        <span class="comment">#当第一次遇到A标签时，字典内还没有A标签，这时候直接幅值加1是错误的，</span></span><br><span class="line">        <span class="comment">#所以需要判断字典中是否有该键，没有则创建，有就直接自增</span></span><br><span class="line">        <span class="keyword">if</span> labelArr[i] <span class="keyword">in</span> classDict.keys():</span><br><span class="line">            <span class="comment"># 若在字典中存在该标签，则直接加1</span></span><br><span class="line">            classDict[labelArr[i]] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#若无该标签，设初值为1，表示出现了1次了</span></span><br><span class="line">            classDict[labelArr[i]] = <span class="number">1</span></span><br><span class="line">    <span class="comment">#对字典依据值进行降序排序</span></span><br><span class="line">    classSort = sorted(classDict.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#返回最大一项的标签，即占数目最多的标签</span></span><br><span class="line">    <span class="keyword">return</span> classSort[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_H_D</span><span class="params">(trainLabelArr)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算数据集D的经验熵，参考公式5.7 经验熵的计算</span></span><br><span class="line"><span class="string">    :param trainLabelArr:当前数据集的标签集</span></span><br><span class="line"><span class="string">    :return: 经验熵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#初始化为0</span></span><br><span class="line">    H_D = <span class="number">0</span></span><br><span class="line">    <span class="comment">#将当前所有标签放入集合中，这样只要有的标签都会在集合中出现，且出现一次。</span></span><br><span class="line">    <span class="comment">#遍历该集合就可以遍历所有出现过的标记并计算其Ck</span></span><br><span class="line">    <span class="comment">#这么做有一个很重要的原因：首先假设一个背景，当前标签集中有一些标记已经没有了，比如说标签集中</span></span><br><span class="line">    <span class="comment">#没有0（这是很正常的，说明当前分支不存在这个标签）。 式5.7中有一项Ck，那按照式中的针对不同标签k</span></span><br><span class="line">    <span class="comment">#计算Cl和D并求和时，由于没有0，那么C0=0，此时C0/D0=0,log2(C0/D0) = log2(0)，事实上0并不在log的</span></span><br><span class="line">    <span class="comment">#定义区间内，出现了问题</span></span><br><span class="line">    <span class="comment">#所以使用集合的方式先知道当前标签中都出现了那些标签，随后对每个标签进行计算，如果没出现的标签那一项就</span></span><br><span class="line">    <span class="comment">#不在经验熵中出现（未参与，对经验熵无影响），保证log的计算能一直有定义</span></span><br><span class="line">    trainLabelSet = set([label <span class="keyword">for</span> label <span class="keyword">in</span> trainLabelArr])</span><br><span class="line">    <span class="comment">#遍历每一个出现过的标签</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> trainLabelSet:</span><br><span class="line">        <span class="comment">#计算|Ck|/|D|</span></span><br><span class="line">        <span class="comment">#trainLabelArr == i：当前标签集中为该标签的的位置</span></span><br><span class="line">        <span class="comment">#例如a = [1, 0, 0, 1], c = (a == 1): c == [True, false, false, True]</span></span><br><span class="line">        <span class="comment">#trainLabelArr[trainLabelArr == i]：获得为指定标签的样本</span></span><br><span class="line">        <span class="comment">#trainLabelArr[trainLabelArr == i].size：获得为指定标签的样本的大小，即标签为i的样本</span></span><br><span class="line">        <span class="comment">#数量，就是|Ck|</span></span><br><span class="line">        <span class="comment">#trainLabelArr.size：整个标签集的数量（也就是样本集的数量），即|D|</span></span><br><span class="line">        p = trainLabelArr[trainLabelArr == i].size / trainLabelArr.size</span><br><span class="line">        <span class="comment">#对经验熵的每一项累加求和</span></span><br><span class="line">        H_D += <span class="number">-1</span> * p * np.log2(p)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回经验熵</span></span><br><span class="line">    <span class="keyword">return</span> H_D</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcH_D_A</span><span class="params">(trainDataArr_DevFeature, trainLabelArr)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算经验条件熵</span></span><br><span class="line"><span class="string">    :param trainDataArr_DevFeature:切割后只有feature那列数据的数组</span></span><br><span class="line"><span class="string">    :param trainLabelArr: 标签集数组</span></span><br><span class="line"><span class="string">    :return: 经验条件熵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#初始为0</span></span><br><span class="line">    H_D_A = <span class="number">0</span></span><br><span class="line">    <span class="comment">#在featue那列放入集合中，是为了根据集合中的数目知道该feature目前可取值数目是多少</span></span><br><span class="line">    trainDataSet = set([label <span class="keyword">for</span> label <span class="keyword">in</span> trainDataArr_DevFeature])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对于每一个特征取值遍历计算条件经验熵的每一项</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> trainDataSet:</span><br><span class="line">        <span class="comment">#计算H(D|A)</span></span><br><span class="line">        <span class="comment">#trainDataArr_DevFeature[trainDataArr_DevFeature == i].size / trainDataArr_DevFeature.size:|Di| / |D|</span></span><br><span class="line">        <span class="comment">#calc_H_D(trainLabelArr[trainDataArr_DevFeature == i]):H(Di)</span></span><br><span class="line">        H_D_A += trainDataArr_DevFeature[trainDataArr_DevFeature == i].size / trainDataArr_DevFeature.size \</span><br><span class="line">                * calc_H_D(trainLabelArr[trainDataArr_DevFeature == i])</span><br><span class="line">    <span class="comment">#返回得出的条件经验熵</span></span><br><span class="line">    <span class="keyword">return</span> H_D_A</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcBestFeature</span><span class="params">(trainDataList, trainLabelList)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算信息增益最大的特征</span></span><br><span class="line"><span class="string">    :param trainDataList: 当前数据集</span></span><br><span class="line"><span class="string">    :param trainLabelList: 当前标签集</span></span><br><span class="line"><span class="string">    :return: 信息增益最大的特征及最大信息增益值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#将数据集和标签集转换为数组形式</span></span><br><span class="line">    <span class="comment">#trainLabelArr转换后需要转置，这样在取数时方便</span></span><br><span class="line">    <span class="comment">#例如a = np.array([1, 2, 3]); b = np.array([1, 2, 3]).T</span></span><br><span class="line">    <span class="comment">#若不转置，a[0] = [1, 2, 3]，转置后b[0] = 1, b[1] = 2</span></span><br><span class="line">    <span class="comment">#对于标签集来说，能够很方便地取到每一位是很重要的</span></span><br><span class="line">    trainDataArr = np.array(trainDataList)</span><br><span class="line">    trainLabelArr = np.array(trainLabelList).T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取当前特征数目，也就是数据集的横轴大小</span></span><br><span class="line">    featureNum = trainDataArr.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化最大信息增益</span></span><br><span class="line">    maxG_D_A = <span class="number">-1</span></span><br><span class="line">    <span class="comment">#初始化最大信息增益的特征</span></span><br><span class="line">    maxFeature = <span class="number">-1</span></span><br><span class="line">    <span class="comment">#对每一个特征进行遍历计算</span></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> range(featureNum):</span><br><span class="line">        <span class="comment">#“5.2.2 信息增益”中“算法5.1（信息增益的算法）”第一步：</span></span><br><span class="line">        <span class="comment">#1.计算数据集D的经验熵H(D)</span></span><br><span class="line">        H_D = calc_H_D(trainLabelArr)</span><br><span class="line">        <span class="comment">#2.计算条件经验熵H(D|A)</span></span><br><span class="line">        <span class="comment">#由于条件经验熵的计算过程中只涉及到标签以及当前特征，为了提高运算速度（全部样本</span></span><br><span class="line">        <span class="comment">#做成的矩阵运算速度太慢，需要剔除不需要的部分），将数据集矩阵进行切割</span></span><br><span class="line">        <span class="comment">#数据集在初始时刻是一个Arr = 60000*784的矩阵，针对当前要计算的feature，在训练集中切割下</span></span><br><span class="line">        <span class="comment">#Arr[:, feature]这么一条来，因为后续计算中数据集中只用到这个（没明白的跟着算一遍例5.2）</span></span><br><span class="line">        <span class="comment">#trainDataArr[:, feature]:在数据集中切割下这么一条</span></span><br><span class="line">        <span class="comment">#trainDataArr[:, feature].flat：将这么一条转换成竖着的列表</span></span><br><span class="line">        <span class="comment">#np.array(trainDataArr[:, feature].flat)：再转换成一条竖着的矩阵，大小为60000*1（只是初始是</span></span><br><span class="line">        <span class="comment">#这么大，运行过程中是依据当前数据集大小动态变的）</span></span><br><span class="line">        trainDataArr_DevideByFeature = np.array(trainDataArr[:, feature].flat)</span><br><span class="line">        <span class="comment">#3.计算信息增益G(D|A)    G(D|A) = H(D) - H(D | A)</span></span><br><span class="line">        G_D_A = H_D - calcH_D_A(trainDataArr_DevideByFeature, trainLabelArr)</span><br><span class="line">        <span class="comment">#不断更新最大的信息增益以及对应的feature</span></span><br><span class="line">        <span class="keyword">if</span> G_D_A &gt; maxG_D_A:</span><br><span class="line">            maxG_D_A = G_D_A</span><br><span class="line">            maxFeature = feature</span><br><span class="line">    <span class="keyword">return</span> maxFeature, maxG_D_A</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSubDataArr</span><span class="params">(trainDataArr, trainLabelArr, A, a)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    更新数据集和标签集</span></span><br><span class="line"><span class="string">    :param trainDataArr:要更新的数据集</span></span><br><span class="line"><span class="string">    :param trainLabelArr: 要更新的标签集</span></span><br><span class="line"><span class="string">    :param A: 要去除的特征索引</span></span><br><span class="line"><span class="string">    :param a: 当data[A]== a时，说明该行样本时要保留的</span></span><br><span class="line"><span class="string">    :return: 新的数据集和标签集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#返回的数据集</span></span><br><span class="line">    retDataArr = []</span><br><span class="line">    <span class="comment">#返回的标签集</span></span><br><span class="line">    retLabelArr = []</span><br><span class="line">    <span class="comment">#对当前数据的每一个样本进行遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(trainDataArr)):</span><br><span class="line">        <span class="comment">#如果当前样本的特征为指定特征值a</span></span><br><span class="line">        <span class="keyword">if</span> trainDataArr[i][A] == a:</span><br><span class="line">            <span class="comment">#那么将该样本的第A个特征切割掉，放入返回的数据集中</span></span><br><span class="line">            retDataArr.append(trainDataArr[i][<span class="number">0</span>:A] + trainDataArr[i][A+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment">#将该样本的标签放入返回标签集中</span></span><br><span class="line">            retLabelArr.append(trainLabelArr[i])</span><br><span class="line">    <span class="comment">#返回新的数据集和标签集</span></span><br><span class="line">    <span class="keyword">return</span> retDataArr, retLabelArr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(*dataSet)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    递归创建决策树</span></span><br><span class="line"><span class="string">    :param dataSet:(trainDataList， trainLabelList) &lt;&lt;-- 元祖形式</span></span><br><span class="line"><span class="string">    :return:新的子节点或该叶子节点的值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#设置Epsilon，“5.3.1 ID3算法”第4步提到需要将信息增益与阈值Epsilon比较，若小于则</span></span><br><span class="line">    <span class="comment">#直接处理后返回T</span></span><br><span class="line">    <span class="comment">#该值的大小在设置上并未考虑太多，观察到信息增益前期在运行中为0.3左右，所以设置了0.1</span></span><br><span class="line">    Epsilon = <span class="number">0.1</span></span><br><span class="line">    <span class="comment">#从参数中获取trainDataList和trainLabelList</span></span><br><span class="line">    <span class="comment">#之所以使用元祖作为参数，是由于后续递归调用时直数据集需要对某个特征进行切割，在函数递归</span></span><br><span class="line">    <span class="comment">#调用上直接将切割函数的返回值放入递归调用中，而函数的返回值形式是元祖的，等看到这个函数</span></span><br><span class="line">    <span class="comment">#的底部就会明白了，这样子的用处就是写程序的时候简洁一点，方便一点</span></span><br><span class="line">    trainDataList = dataSet[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    trainLabelList = dataSet[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    <span class="comment">#打印信息：开始一个子节点创建，打印当前特征向量数目及当前剩余样本数目</span></span><br><span class="line">    print(<span class="string">'start a node'</span>, len(trainDataList[<span class="number">0</span>]), len(trainLabelList))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将标签放入一个字典中，当前样本有多少类，在字典中就会有多少项</span></span><br><span class="line">    <span class="comment">#也相当于去重，多次出现的标签就留一次。举个例子，假如处理结束后字典的长度为1，那说明所有的样本</span></span><br><span class="line">    <span class="comment">#都是同一个标签，那就可以直接返回该标签了，不需要再生成子节点了。</span></span><br><span class="line">    classDict = &#123;i <span class="keyword">for</span> i <span class="keyword">in</span> trainLabelList&#125;</span><br><span class="line">    <span class="comment">#如果D中所有实例属于同一类Ck，则置T为单节点数，并将Ck作为该节点的类，返回T</span></span><br><span class="line">    <span class="comment">#即若所有样本的标签一致，也就不需要再分化，返回标记作为该节点的值，返回后这就是一个叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> len(classDict) == <span class="number">1</span>:</span><br><span class="line">        <span class="comment">#因为所有样本都是一致的，在标签集中随便拿一个标签返回都行，这里用的第0个（因为你并不知道</span></span><br><span class="line">        <span class="comment">#当前标签集的长度是多少，但运行中所有标签只要有长度都会有第0位。</span></span><br><span class="line">        <span class="keyword">return</span> trainLabelList[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果A为空集，则置T为单节点数，并将D中实例数最大的类Ck作为该节点的类，返回T</span></span><br><span class="line">    <span class="comment">#即如果已经没有特征可以用来再分化了，就返回占大多数的类别</span></span><br><span class="line">    <span class="keyword">if</span> len(trainDataList[<span class="number">0</span>]) == <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#返回当前标签集中占数目最大的标签</span></span><br><span class="line">        <span class="keyword">return</span> majorClass(trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#否则，按式5.10计算A中个特征值的信息增益，选择信息增益最大的特征Ag</span></span><br><span class="line">    Ag, EpsilonGet = calcBestFeature(trainDataList, trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果Ag的信息增益比小于阈值Epsilon，则置T为单节点树，并将D中实例数最大的类Ck</span></span><br><span class="line">    <span class="comment">#作为该节点的类，返回T</span></span><br><span class="line">    <span class="keyword">if</span> EpsilonGet &lt; Epsilon:</span><br><span class="line">        <span class="keyword">return</span> majorClass(trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的</span></span><br><span class="line">    <span class="comment"># 类作为标记，构建子节点，由节点及其子节点构成树T，返回T</span></span><br><span class="line">    treeDict = &#123;Ag:&#123;&#125;&#125;</span><br><span class="line">    <span class="comment">#特征值为0时，进入0分支</span></span><br><span class="line">    <span class="comment">#getSubDataArr(trainDataList, trainLabelList, Ag, 0)：在当前数据集中切割当前feature，返回新的数据集和标签集</span></span><br><span class="line">    treeDict[Ag][<span class="number">0</span>] = createTree(getSubDataArr(trainDataList, trainLabelList, Ag, <span class="number">0</span>))</span><br><span class="line">    treeDict[Ag][<span class="number">1</span>] = createTree(getSubDataArr(trainDataList, trainLabelList, Ag, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> treeDict</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(testDataList, tree)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测标签</span></span><br><span class="line"><span class="string">    :param testDataList:样本</span></span><br><span class="line"><span class="string">    :param tree: 决策树</span></span><br><span class="line"><span class="string">    :return: 预测结果</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># treeDict = copy.deepcopy(tree)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#死循环，直到找到一个有效地分类</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment">#因为有时候当前字典只有一个节点</span></span><br><span class="line">        <span class="comment">#例如&#123;73: &#123;0: &#123;74:6&#125;&#125;&#125;看起来节点很多，但是对于字典的最顶层来说，只有73一个key，其余都是value</span></span><br><span class="line">        <span class="comment">#若还是采用for来读取的话不太合适，所以使用下行这种方式读取key和value</span></span><br><span class="line">        (key, value), = tree.items()</span><br><span class="line">        <span class="comment">#如果当前的value是字典，说明还需要遍历下去</span></span><br><span class="line">        <span class="keyword">if</span> type(tree[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            <span class="comment">#获取目前所在节点的feature值，需要在样本中删除该feature</span></span><br><span class="line">            <span class="comment">#因为在创建树的过程中，feature的索引值永远是对于当时剩余的feature来设置的</span></span><br><span class="line">            <span class="comment">#所以需要不断地删除已经用掉的特征，保证索引相对位置的一致性</span></span><br><span class="line">            dataVal = testDataList[key]</span><br><span class="line">            <span class="keyword">del</span> testDataList[key]</span><br><span class="line">            <span class="comment">#将tree更新为其子节点的字典</span></span><br><span class="line">            tree = value[dataVal]</span><br><span class="line">            <span class="comment">#如果当前节点的子节点的值是int，就直接返回该int值</span></span><br><span class="line">            <span class="comment">#例如&#123;403: &#123;0: 7, 1: &#123;297:7&#125;&#125;，dataVal=0</span></span><br><span class="line">            <span class="comment">#此时上一行tree = value[dataVal]，将tree定位到了7，而7不再是一个字典了，</span></span><br><span class="line">            <span class="comment">#这里就可以直接返回7了，如果tree = value[1]，那就是一个新的子节点，需要继续遍历下去</span></span><br><span class="line">            <span class="keyword">if</span> type(tree).__name__ == <span class="string">'int'</span>:</span><br><span class="line">                <span class="comment">#返回该节点值，也就是分类值</span></span><br><span class="line">                <span class="keyword">return</span> tree</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#如果当前value不是字典，那就返回分类值</span></span><br><span class="line">            <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(testDataList, testLabelList, tree)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试准确率</span></span><br><span class="line"><span class="string">    :param testDataList:待测试数据集</span></span><br><span class="line"><span class="string">    :param testLabelList: 待测试标签集</span></span><br><span class="line"><span class="string">    :param tree: 训练集生成的树</span></span><br><span class="line"><span class="string">    :return: 准确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#错误次数计数</span></span><br><span class="line">    errorCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">#遍历测试集中每一个测试样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testDataList)):</span><br><span class="line">        <span class="comment">#判断预测与标签中结果是否一致</span></span><br><span class="line">        <span class="keyword">if</span> testLabelList[i] != predict(testDataList[i], tree):</span><br><span class="line">            errorCnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">#返回准确率</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - errorCnt / len(testDataList)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#开始时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取训练集</span></span><br><span class="line">    trainDataList, trainLabelList = loadData(<span class="string">'../Mnist/mnist_train.csv'</span>)</span><br><span class="line">    <span class="comment"># 获取测试集</span></span><br><span class="line">    testDataList, testLabelList = loadData(<span class="string">'../Mnist/mnist_test.csv'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#创建决策树</span></span><br><span class="line">    print(<span class="string">'start create tree'</span>)</span><br><span class="line">    tree = createTree((trainDataList, trainLabelList))</span><br><span class="line">    print(<span class="string">'tree is:'</span>, tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#测试准确率</span></span><br><span class="line">    print(<span class="string">'start test'</span>)</span><br><span class="line">    accur = test(testDataList, testLabelList, tree)</span><br><span class="line">    print(<span class="string">'the accur is:'</span>, accur)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#结束时间</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'time span:'</span>, end - start)</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Eric kun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ericzikun.github.io/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/">https://ericzikun.github.io/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ericzikun.github.io" target="_blank">EricKun</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/20191209103004602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png"/></a><div class="post-qr-code-desc"></div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/11/15/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><img class="prev-cover" src="https://img-blog.csdnimg.cn/20200114131425132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">逻辑回归</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/08/%E6%84%9F%E7%9F%A5%E6%9C%BA%E3%80%81KNN/"><img class="next-cover" src="https://img-blog.csdnimg.cn/20191127190937753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">感知机、KNN</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/04/08/朴素贝叶斯理论/" title="朴素贝叶斯理论"><img class="cover" src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=1474209398,3737326875&fm=26&gp=0.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-08</div><div class="title">朴素贝叶斯理论</div></div></a></div><div><a href="/2019/11/15/逻辑回归/" title="逻辑回归"><img class="cover" src="https://img-blog.csdnimg.cn/20200114131425132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-11-15</div><div class="title">逻辑回归</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer" style="background: #000000"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Eric kun</div><div class="framework-info"><span>框架 </span><a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'tx6zs0UB1yRovubWAD3heyoM-gzGzoHsz',
      appKey: '8SJzl4MBSSjcdEESUaALKRXk',
      placeholder: 'Just do it!',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>